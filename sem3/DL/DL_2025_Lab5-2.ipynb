{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janbanot/msc-cs-code/blob/main/sem3/DL/DL_2025_Lab5-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "270dbf72",
      "metadata": {
        "id": "270dbf72"
      },
      "source": [
        "# Sieci typu transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7622fdf1",
      "metadata": {
        "id": "7622fdf1"
      },
      "source": [
        "Mechanizm samouwagi (self-attention) zaproponowany w pracy [1] stanowi istotny krok w rozwoju współczesnych sieci neuronowych,\n",
        "w tym dużych modeli językowych. W bieżącym notatniku rozpatrzymy sieć typu GPT (Generative Pre-trained Transformer)\n",
        "odpowiadającą wariantowi GPT-2.\n",
        "\n",
        "[1] Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in neural information processing systems 30 (2017)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3128e919",
      "metadata": {
        "cellView": "form",
        "id": "3128e919"
      },
      "outputs": [],
      "source": [
        "# @title Zwięzła implementacja modelu GPT\n",
        "# Żródło: https://github.com/karpathy/nanoGPT/blob/master/model.py\n",
        "\n",
        "!uv pip install torchinfo\n",
        "\n",
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torchinfo import summary\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Przyczynowa samouwaga (Causal Self-Attention).\n",
        "    Token może \"patrzeć\" tylko na tokeny z przeszłości, tj. za nim, a nie po nim.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "\n",
        "        # Jedna warstwa liniowa liczy naraz Q, K i V (potem rozdzielamy na 3 części).\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "\n",
        "        # Projekcja wyjściowa po scaleniu głowic.\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "\n",
        "        # Dropout na wyjściu bloku uwagi\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.attn_dropout_p = float(config.dropout)\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, C) -> batch, długość sekwencji, wymiar embeddingu\n",
        "        B, T, C = x.size()\n",
        "        head_size = C // self.n_head\n",
        "\n",
        "        # Liczymy Q, K, V i rozdzielamy wynik na trzy tensory.\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "\n",
        "        # (B, T, C) -> (B, n_head, T, head_size)\n",
        "        # .transpose(1, 2) przenosi wymiar głowic przed czas.\n",
        "        q = q.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
        "\n",
        "        # PyTorch 2.0+: scaled_dot_product_attention może użyć Flash Attention.\n",
        "        # is_causal=True wymusza maskę trójkątną (brak wglądu w przyszłość).\n",
        "        y = F.scaled_dot_product_attention(\n",
        "            q, k, v,\n",
        "            attn_mask=None,\n",
        "            dropout_p=self.attn_dropout_p if self.training else 0.0,\n",
        "            is_causal=True,\n",
        "        )\n",
        "\n",
        "        # Scal głowice: (B, n_head, T, head_size) -> (B, T, C)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        # Projekcja + dropout rezydualny\n",
        "        return self.resid_dropout(self.c_proj(y))\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Prosty MLP działający niezależnie na każdym tokenie.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias),\n",
        "            nn.Dropout(config.dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "    Pojedynczy blok Transformera (pre-norm):\n",
        "    LN -> Attention -> resid, LN -> MLP -> resid\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd, elementwise_affine=True, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd, elementwise_affine=True, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pre-norm: najpierw normalizacja, potem operacja, potem dodanie rezydualne.\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config: GPTConfig):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte=nn.Embedding(config.vocab_size, config.n_embd),    # embedding tokenów\n",
        "            wpe=nn.Embedding(config.block_size, config.n_embd),    # embedding pozycji\n",
        "            drop=nn.Dropout(config.dropout),\n",
        "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f=nn.LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "\n",
        "        # Głowa językowa: projekcja na rozmiar słownika.\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # Weight tying: wte.weight i lm_head.weight to ten sam parametr.\n",
        "        # Zmniejsza liczbę parametrów i zwykle poprawia jakość.\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # Bufor z indeksami pozycji: unikamy torch.arange w każdym forward().\n",
        "        # persistent=False => nie zapisuje się do checkpointów (bo można odtworzyć).\n",
        "        self.register_buffer(\n",
        "            \"pos_idx\",\n",
        "            torch.arange(config.block_size, dtype=torch.long),\n",
        "            persistent=False,\n",
        "        )\n",
        "\n",
        "        # Inicjalizacja wag\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        # Specjalna inicjalizacja dla projekcji rezydualnych (jak w GPT-2),\n",
        "        # aby stabilizować wariancję w głębokiej sieci na starcie treningu.\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith(\"c_proj.weight\"):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        print(\"Liczba parametrów: %.2fM\" % (self.get_num_params() / 1e6,))\n",
        "\n",
        "    def get_num_params(self) -> int:\n",
        "        \"\"\"Zwraca liczbę parametrów (bez embeddingów pozycyjnych).\"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module: nn.Module) -> None:\n",
        "        \"\"\"Domyślna inicjalizacja wag (rozkład normalny).\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.01)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.01)\n",
        "\n",
        "    def forward(self, idx: torch.Tensor, targets: torch.Tensor | None = None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, (\n",
        "            f\"Sekwencja {t} jest dłuższa niż block_size={self.config.block_size}\"\n",
        "        )\n",
        "\n",
        "        # (b, t, n_embd) + (t, n_embd) => broadcast po batchu\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "\n",
        "        pos = self.pos_idx[:t].to(device)   # Pozycje kolejnych tokenów\n",
        "        pos_emb = self.transformer.wpe(pos) # Model uczy się kodowania pozycji\n",
        "\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)  # Opcjonalny dropout\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # Trening: logity dla wszystkich pozycji + loss.\n",
        "            logits = self.lm_head(x)  # (b, t, vocab)\n",
        "\n",
        "            # cross_entropy oczekuje (N, C, ...) dla logits oraz (N, ...) dla targetów,\n",
        "            # więc przestawiamy osie na (b, vocab, t).\n",
        "            loss = F.cross_entropy(\n",
        "                logits.transpose(1, 2),\n",
        "                targets,\n",
        "                ignore_index=-1,  # Ignoruj tokeny o tej wartości\n",
        "            )\n",
        "        else:\n",
        "            # Inferencja: interesuje nas predykcja następnego tokena (ostatnia pozycja).\n",
        "            logits = self.lm_head(x[:, [-1], :])  # (b, 1, vocab)\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        \"\"\"\n",
        "        AdamW z podziałem parametrów na:\n",
        "        - decay: wagi warstw liniowych (nn.Linear.weight)\n",
        "        - no_decay: biasy, LayerNorm, Embeddingi\n",
        "        \"\"\"\n",
        "\n",
        "        whitelist = (nn.Linear,)\n",
        "        blacklist = (nn.LayerNorm, nn.Embedding)\n",
        "\n",
        "        # mapowanie: id(param) -> (param, czy_decay)\n",
        "        # czy_decay = True oznacza, że parametr trafi do grupy z weight_decay\n",
        "        param_to_decay = {}\n",
        "        id_to_param = {}\n",
        "\n",
        "        for module in self.modules():\n",
        "            for name, param in module.named_parameters(recurse=False):\n",
        "                if not param.requires_grad:\n",
        "                    continue\n",
        "\n",
        "                pid = id(param)\n",
        "                id_to_param[pid] = param\n",
        "\n",
        "                # Reguły klasyfikacji:\n",
        "                is_bias = name.endswith(\"bias\")\n",
        "                is_linear_weight = isinstance(module, whitelist) and name.endswith(\"weight\")\n",
        "                is_blacklisted = isinstance(module, blacklist)\n",
        "\n",
        "                # Priorytet: NO_DECAY wygrywa zawsze (szczególnie ważne przy weight tying).\n",
        "                if is_bias or is_blacklisted:\n",
        "                    param_to_decay[pid] = False\n",
        "                elif is_linear_weight:\n",
        "                    # decay tylko jeśli parametr nie został wcześniej oznaczony jako no_decay\n",
        "                    param_to_decay.setdefault(pid, True)\n",
        "                else:\n",
        "                    # bezpieczny domyślny wybór: brak decay\n",
        "                    param_to_decay.setdefault(pid, False)\n",
        "\n",
        "        decay_params = [id_to_param[pid] for pid, dec in param_to_decay.items() if dec]\n",
        "        nodecay_params = [id_to_param[pid] for pid, dec in param_to_decay.items() if not dec]\n",
        "\n",
        "        optim_groups = [\n",
        "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
        "            {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
        "        ]\n",
        "\n",
        "        # device_type: jeśli przekazujesz torch.device, użyj device_type = device.type\n",
        "        if isinstance(device_type, torch.device):\n",
        "            device_type = device_type.type\n",
        "\n",
        "        use_fused = (device_type == \"cuda\") and (\"fused\" in inspect.signature(torch.optim.AdamW).parameters)\n",
        "        print(f\"Używanie fused AdamW: {use_fused}\")\n",
        "\n",
        "        return torch.optim.AdamW(\n",
        "            optim_groups,\n",
        "            lr=learning_rate,\n",
        "            betas=betas,\n",
        "            fused=use_fused,\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx: torch.Tensor, max_new_tokens: int, temperature: float = 1.0):\n",
        "        \"\"\"\n",
        "        Generuje kolejne tokeny autoregresyjnie.\n",
        "\n",
        "        Ważne:\n",
        "        - Zapamiętujemy poprzedni tryb (train/eval) i przywracamy go na końcu,\n",
        "          żeby generate() nie psuło treningu, jeśli zostanie wywołane w trakcie.\n",
        "        - @torch.no_grad() wyłącza gradienty (szybciej i mniej pamięci).\n",
        "        \"\"\"\n",
        "        was_training = self.training\n",
        "        self.eval()\n",
        "        try:\n",
        "            for _ in range(max_new_tokens):\n",
        "                # Ograniczamy kontekst do block_size (model nie widzi dalej).\n",
        "                idx_cond = idx[:, -self.config.block_size:]\n",
        "\n",
        "                # forward() w trybie inferencji zwraca logity tylko dla ostatniej pozycji (b, 1, vocab)\n",
        "                logits, _ = self(idx_cond)\n",
        "                logits = logits[:, -1, :]  # (b, vocab)\n",
        "\n",
        "                # temperature=0 -> deterministycznie (argmax)\n",
        "                if temperature == 0.0:\n",
        "                    idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "                else:\n",
        "                    logits = logits / temperature\n",
        "                    probs = F.softmax(logits, dim=-1)\n",
        "                    idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                idx = torch.cat([idx, idx_next], dim=1)\n",
        "\n",
        "            return idx\n",
        "        finally:\n",
        "            # Przywróć poprzedni tryb modelu.\n",
        "            self.train(was_training)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08bf941a",
      "metadata": {
        "id": "08bf941a"
      },
      "source": [
        "## Przykład 1.\n",
        "\n",
        "Spróbujmy wytrenować model GPT rozwiązujący problem odwracania słowa\n",
        "dla ciągów bitowych, np.\n",
        "\n",
        "    0101 -> 1010\n",
        "    110 -> 011\n",
        "\n",
        "Dane kodowane będą za pomocą dodatkowych symboli, tj. separatora `#`, końca ciągu `<EOS>` wypełnienia (padding) `<PAD>`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "957097ae",
      "metadata": {
        "id": "957097ae"
      },
      "outputs": [],
      "source": [
        "# @title Generowanie danych treningowych\n",
        "\n",
        "# Słownik tokenów: 0/1/# oraz znaczniki specjalne PAD (do wypełniania) i EOS (koniec sekwencji)\n",
        "VOCAB = {'0': 0, '1': 1, '#': 2, '<PAD>': 3, '<EOS>': 4}\n",
        "INV_VOCAB = {v: k for k, v in VOCAB.items()}  # przydatne do debugowania/drukowania\n",
        "PAD_IDX = VOCAB['<PAD>']\n",
        "EOS_IDX = VOCAB['<EOS>']\n",
        "\n",
        "def bin_rev_encode(tokens):\n",
        "    \"\"\"Zamienia listę tokenów (np. ['0','1','#','1','0','<EOS>']) na listę id.\"\"\"\n",
        "    return [VOCAB[t] for t in tokens]\n",
        "\n",
        "def bin_rev_decode(tokens):\n",
        "    return ''.join(INV_VOCAB[t] for t in tokens)\n",
        "\n",
        "def bin_rev_make_example(n_bits):\n",
        "    \"\"\"\n",
        "    Generuje pojedynczy przykład:\n",
        "      bits  = losowy ciąg bitów (np. '01')\n",
        "      rev   = odwrócony ciąg (np. '10')\n",
        "      tokens= ['0','1','#','1','0','<EOS>']\n",
        "    Zwraca tensor z identyfikatorami tokenów,\n",
        "        bits + '#' oraz rev do ewentualnego podglądu/debugowania.\n",
        "    \"\"\"\n",
        "    bits = ''.join(random.choice('01') for _ in range(n_bits))\n",
        "    rev = bits[::-1]\n",
        "    tokens = list(bits) + ['#'] + list(rev) + ['<EOS>']\n",
        "    return torch.tensor(bin_rev_encode(tokens), dtype=torch.long), bits + '#', rev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e602884",
      "metadata": {
        "id": "8e602884"
      },
      "outputs": [],
      "source": [
        "# @title Funkcje evaluacji oraz treningu\n",
        "\n",
        "def eval_accuracy(model, max_bits, make_example, decode, eval_examples=100, n_show=1):\n",
        "    \"\"\"\n",
        "    Oblicza i zwraca:\n",
        "    - sequence accuracy -- dokładność (ile przykładów poprawnych)\n",
        "    - token-level accuracy (ile tokenów w sufiksie po '#' się zgadza)\n",
        "    \"\"\"\n",
        "    seq_correct = 0\n",
        "    seq_total = 0\n",
        "\n",
        "    tok_correct = 0\n",
        "    tok_total = 0\n",
        "\n",
        "    shown = 0\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    for _ in range(eval_examples):\n",
        "        ids, prefix, suffix = make_example(random.randint(1, max_bits))\n",
        "\n",
        "        cut = len(prefix)\n",
        "        prompt = ids[:cut].unsqueeze(0).to(device)\n",
        "        max_new = int(ids.numel() - cut) # ile tokenów trzeba dogenerować\n",
        "\n",
        "        out = model.generate(prompt, max_new_tokens=max_new, temperature=0.0)\n",
        "\n",
        "        generated_suffix = out[0, cut:].detach().cpu()\n",
        "        target_suffix = ids[cut:].detach().cpu()\n",
        "\n",
        "        ok = (generated_suffix.numel() == target_suffix.numel()\n",
        "              and torch.equal(generated_suffix, target_suffix))\n",
        "        if ok:\n",
        "            seq_correct += 1\n",
        "        seq_total += 1\n",
        "\n",
        "        # Ile tokenów OK\n",
        "        L = min(generated_suffix.numel(), target_suffix.numel())\n",
        "        if L > 0:\n",
        "            tok_correct += (generated_suffix[:L] == target_suffix[:L]).sum().item()\n",
        "            tok_total += L\n",
        "\n",
        "        if shown < n_show:  # Opcjonalny podgląd przykładu\n",
        "            prompt_str = decode(prompt[0].detach().cpu().tolist())\n",
        "            gen_str    = decode(out[0].detach().cpu().tolist())\n",
        "            tgt_str    = decode(ids.detach().cpu().tolist())\n",
        "\n",
        "            print(f\"  [{shown+1}] prompt: {prompt_str}\")\n",
        "            print(f\"      gen:   {gen_str}\")\n",
        "            print(f\"      tgt:   {tgt_str}\")\n",
        "            print(f\"      seq_ok:{ok}    (inp={prefix}, out={out})\")\n",
        "            shown += 1\n",
        "\n",
        "    seq_acc = (seq_correct / seq_total) if seq_total > 0 else float('nan')\n",
        "    tok_acc = (tok_correct / tok_total) if tok_total > 0 else float('nan')\n",
        "\n",
        "    return seq_acc, tok_acc\n",
        "\n",
        "\n",
        "def train_model(model, make_example, decode, max_bits=15, batch_size=128, total_steps=10_000, best_model_path='best_model.pt'):\n",
        "    MAX_LR = 1e-3\n",
        "    eval_every = 500\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    optimizer = model.configure_optimizers(\n",
        "        weight_decay=0.01,\n",
        "        learning_rate=5e-4,\n",
        "        betas=(0.9, 0.99),\n",
        "        device_type=device\n",
        "    )\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=MAX_LR,\n",
        "        total_steps=total_steps,\n",
        "        pct_start=0.1\n",
        "    )\n",
        "\n",
        "    loss_history = []\n",
        "    print(\"--- Rozpoczynanie Treningu ---\")\n",
        "    model.train()\n",
        "\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for step in range(total_steps):\n",
        "        # Dynamiczne generowanie batcha\n",
        "        batch = [make_example(random.randint(1, max_bits)) for _ in range(batch_size)]\n",
        "\n",
        "        examples = [tok_ids for tok_ids, _, _ in batch]\n",
        "\n",
        "        inputs = [s[:-1] for s in examples]\n",
        "        labels = [s[1:]  for s in examples]\n",
        "\n",
        "        # xs: 0, 1, #, 1, 0,     <EOS>, <PAD>, ..., <PAD>\n",
        "        xs = pad_sequence(inputs, batch_first=True, padding_value=PAD_IDX).to(device)\n",
        "        # ys: 1, #, 1, 0, <EOS>, <PAD>, ..., <PAD>, -1\n",
        "        ys = pad_sequence(labels, batch_first=True, padding_value=-1).to(device)\n",
        "\n",
        "        _, loss = model(xs, targets=ys)\n",
        "\n",
        "        current_loss = float(loss.item())\n",
        "        loss_history.append(current_loss)\n",
        "\n",
        "        if current_loss < best_loss:\n",
        "            best_loss = current_loss\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "\n",
        "        if step % eval_every == 0:\n",
        "            current_lr = scheduler.get_last_lr()[0]\n",
        "            acc, tok_acc = eval_accuracy(model, max_bits, make_example=make_example, decode=decode)\n",
        "            print(f\"Krok {step}, Loss: {current_loss:.4f}, Acc: {acc*100:.1f}%, Tok. acc: {tok_acc*100:.1f}% LR: {current_lr:.6f}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    print(f\"--- Koniec treningu. Najlepsza strata: {best_loss:.4f} ---\")\n",
        "\n",
        "    # Przywracanie najlepszego modelu\n",
        "    print(f\"Ładowanie najlepszego modelu z {best_model_path}...\")\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "\n",
        "    acc, tok_acc = eval_accuracy(model, max_bits, make_example=make_example, decode=decode, eval_examples=1000)\n",
        "    print(f\"Final acc: {acc*100:.1f}%, Tok. acc: {tok_acc*100:.1f}%\")\n",
        "\n",
        "    # Wykres\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(loss_history)\n",
        "    plt.xlabel('Krok')\n",
        "    plt.ylabel('Wartość straty')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Dodanie wygładzonej średniej (dla czytelności przy dużych wahaniach)\n",
        "    if len(loss_history) > 50:\n",
        "        # Prosta średnia krocząca\n",
        "        window = 50\n",
        "        smoothed = [sum(loss_history[i:i+window])/window for i in range(len(loss_history)-window)]\n",
        "        plt.plot(range(window, len(loss_history)), smoothed, color='red', linewidth=1, label='Średnia krocząca')\n",
        "        plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4348bfe5",
      "metadata": {
        "id": "4348bfe5"
      },
      "outputs": [],
      "source": [
        "# Ustawienie seed dla reprodukowalności\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    # Dla pełnej deterministyczności (może spowolnić trening na GPU, ale zapewnia reprodukowalność)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# Konfiguracja naszego modelu\n",
        "@dataclass\n",
        "class BinRevConfig:\n",
        "    block_size: int = 48   # Maksymalna długość kontekstu (musi pomieścić \"bity + # + rewers\")\n",
        "    vocab_size: int = len(VOCAB)    # Rozmiar słownika (0, 1, #, PAD, EOS)\n",
        "    n_layer: int = 1       # Tylko 1 warstwa (zadanie jest łatwe)\n",
        "    n_head: int = 4        # Tylko 1 głowica uwagi\n",
        "    n_embd: int = 64       # Mały wymiar osadzenia\n",
        "    dropout: float = 0.0   # Brak dropoutu (mały model, dużo danych syntetycznych)\n",
        "    bias: bool = False     # Wyłączamy bias dla szybkości\n",
        "\n",
        "\n",
        "set_seed(12383249)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# device = 'cpu'\n",
        "print(f\"Running on device: {device}\")\n",
        "config = BinRevConfig()\n",
        "model = GPT(config=config)\n",
        "model = model.to(device)\n",
        "\n",
        "# Wyświetl podsumowanie struktury modelu i liczby parametrów\n",
        "print(summary(model, device=device))\n",
        "\n",
        "model = train_model(model, make_example=bin_rev_make_example,\n",
        "                    decode=bin_rev_decode, total_steps=4000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fdce19b",
      "metadata": {
        "id": "0fdce19b"
      },
      "outputs": [],
      "source": [
        "prompt = \"11111101010#\"  # możesz też dać np. \"0110#\" itd.\n",
        "\n",
        "idx = torch.tensor(bin_rev_encode(prompt), dtype=torch.long).unsqueeze(0).to(device)   # (1, T)\n",
        "\n",
        "# Ile tokenów dogenerować? Najprościej: tyle co liczba bitów (tu 2) + ewentualnie 1 na <EOS>\n",
        "max_new = len(prompt)\n",
        "\n",
        "out = model.generate(idx, max_new_tokens=max_new, temperature=0.0)  # 0.0 = deterministycznie\n",
        "completed = bin_rev_decode(out[0].tolist())\n",
        "\n",
        "print(\"Wynik:\", completed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21a5c92a",
      "metadata": {
        "id": "21a5c92a"
      },
      "source": [
        "## Zad. 1\n",
        "\n",
        "Na podstawie poprzedniego przykładu wytrenuj model rozwiązujący problem\n",
        "inkrementacji bitowej dla ciągów o długości do 15 bitów.\n",
        "\n",
        "Przykładowo, \"1011=1100\" (11+1=12)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca068c92",
      "metadata": {
        "id": "ca068c92"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}