{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOoHphukSmZJE1IOqQcQhO4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janbanot/msc-cs-code/blob/main/sem3/DL/DL_2025_Task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UGZSN - Sprawozdanie 2 - Jan Banot"
      ],
      "metadata": {
        "id": "-npfNhaxrYT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install torchinfo"
      ],
      "metadata": {
        "id": "SH_u4WyNrnB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuJF7RwFrRQi"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import inspect\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from dataclasses import dataclass\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- ARCHITEKTURA GPT ---\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Przyczynowa samouwaga (Causal Self-Attention).\n",
        "    Token może \"patrzeć\" tylko na tokeny z przeszłości, tj. za nim, a nie po nim.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "\n",
        "        # Jedna warstwa liniowa liczy naraz Q, K i V (potem rozdzielamy na 3 części).\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "\n",
        "        # Projekcja wyjściowa po scaleniu głowic\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "\n",
        "        # Dropout na wyjściu bloku uwagi\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.attn_dropout_p = float(config.dropout)\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, C) -> batch, długość sekwencji, wymiar embeddingu\n",
        "        B, T, C = x.size()\n",
        "        head_size = C // self.n_head\n",
        "\n",
        "        # Liczymy Q, K, V i rozdzielamy wynik na trzy tensory\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "\n",
        "        # (B, T, C) -> (B, n_head, T, head_size)\n",
        "        # .transpose(1, 2) przenosi wymiar głowic przed czas\n",
        "        q = q.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
        "\n",
        "        # PyTorch 2.0+: scaled_dot_product_attention może użyć Flash Attention.\n",
        "        # is_causal=True wymusza maskę trójkątną (brak wglądu w przyszłość)\n",
        "        y = F.scaled_dot_product_attention(\n",
        "            q, k, v,\n",
        "            attn_mask=None,\n",
        "            dropout_p=self.attn_dropout_p if self.training else 0.0,\n",
        "            is_causal=True,\n",
        "        )\n",
        "\n",
        "        # Scal głowice: (B, n_head, T, head_size) -> (B, T, C)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        # Projekcja + dropout rezydualny\n",
        "        return self.resid_dropout(self.c_proj(y))\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Prosty MLP działający niezależnie na każdym tokenie\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias),\n",
        "            nn.Dropout(config.dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "    Pojedynczy blok Transformera (pre-norm):\n",
        "    LN -> Attention -> resid, LN -> MLP -> resid\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd, elementwise_affine=True, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd, elementwise_affine=True, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pre-norm: najpierw normalizacja, potem operacja, potem dodanie rezydualne\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 128  # Zapas dla 15+15+16 bitów + znaki specjalne\n",
        "    vocab_size: int = 7    # 0, 1, +, =, PAD, EOS, ewentualnie separator\n",
        "    n_layer: int = 4\n",
        "    n_head: int = 8\n",
        "    n_embd: int = 128      # To da nam ok. 800k - 1M parametrów\n",
        "    dropout: float = 0.1\n",
        "    bias: bool = True\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config: GPTConfig):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte=nn.Embedding(config.vocab_size, config.n_embd),    # embedding tokenów\n",
        "            wpe=nn.Embedding(config.block_size, config.n_embd),    # embedding pozycji\n",
        "            drop=nn.Dropout(config.dropout),\n",
        "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f=nn.LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "\n",
        "        # Głowa językowa: projekcja na rozmiar słownika\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # Weight tying: wte.weight i lm_head.weight to ten sam parametr\n",
        "        # Zmniejsza liczbę parametrów i zwykle poprawia jakość.\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # Bufor z indeksami pozycji: unikamy torch.arange w każdym forward()\n",
        "        # persistent=False => nie zapisuje się do checkpointów (bo można odtworzyć)\n",
        "        self.register_buffer(\n",
        "            \"pos_idx\",\n",
        "            torch.arange(config.block_size, dtype=torch.long),\n",
        "            persistent=False,\n",
        "        )\n",
        "\n",
        "        # Inicjalizacja wag\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        # Specjalna inicjalizacja dla projekcji rezydualnych (jak w GPT-2),\n",
        "        # aby stabilizować wariancję w głębokiej sieci na starcie treningu\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith(\"c_proj.weight\"):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        print(\"Liczba parametrów: %.2fM\" % (self.get_num_params() / 1e6,))\n",
        "\n",
        "    def get_num_params(self) -> int:\n",
        "        \"\"\"Zwraca liczbę parametrów (bez embeddingów pozycyjnych).\"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module: nn.Module) -> None:\n",
        "        \"\"\"Domyślna inicjalizacja wag (rozkład normalny).\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.01)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.01)\n",
        "\n",
        "    def forward(self, idx: torch.Tensor, targets: torch.Tensor | None = None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, (\n",
        "            f\"Sekwencja {t} jest dłuższa niż block_size={self.config.block_size}\"\n",
        "        )\n",
        "\n",
        "        # (b, t, n_embd) + (t, n_embd) => broadcast po batchu\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "\n",
        "        pos = self.pos_idx[:t].to(device)   # Pozycje kolejnych tokenów\n",
        "        pos_emb = self.transformer.wpe(pos) # Model uczy się kodowania pozycji\n",
        "\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)  # Opcjonalny dropout\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # Trening: logity dla wszystkich pozycji + loss.\n",
        "            logits = self.lm_head(x)  # (b, t, vocab)\n",
        "\n",
        "            # cross_entropy oczekuje (N, C, ...) dla logits oraz (N, ...) dla targetów,\n",
        "            # więc przestawiamy osie na (b, vocab, t)\n",
        "            loss = F.cross_entropy(\n",
        "                logits.transpose(1, 2),\n",
        "                targets,\n",
        "                ignore_index=-1,  # Ignoruj tokeny o tej wartości\n",
        "            )\n",
        "        else:\n",
        "            # Inferencja: interesuje nas predykcja następnego tokena (ostatnia pozycja)\n",
        "            logits = self.lm_head(x[:, [-1], :])  # (b, 1, vocab)\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        \"\"\"\n",
        "        AdamW z podziałem parametrów na:\n",
        "        - decay: wagi warstw liniowych (nn.Linear.weight)\n",
        "        - no_decay: biasy, LayerNorm, Embeddingi\n",
        "        \"\"\"\n",
        "\n",
        "        whitelist = (nn.Linear,)\n",
        "        blacklist = (nn.LayerNorm, nn.Embedding)\n",
        "\n",
        "        # mapowanie: id(param) -> (param, czy_decay)\n",
        "        # czy_decay = True oznacza, że parametr trafi do grupy z weight_decay\n",
        "        param_to_decay = {}\n",
        "        id_to_param = {}\n",
        "\n",
        "        for module in self.modules():\n",
        "            for name, param in module.named_parameters(recurse=False):\n",
        "                if not param.requires_grad:\n",
        "                    continue\n",
        "\n",
        "                pid = id(param)\n",
        "                id_to_param[pid] = param\n",
        "\n",
        "                # Reguły klasyfikacji:\n",
        "                is_bias = name.endswith(\"bias\")\n",
        "                is_linear_weight = isinstance(module, whitelist) and name.endswith(\"weight\")\n",
        "                is_blacklisted = isinstance(module, blacklist)\n",
        "\n",
        "                # Priorytet: NO_DECAY wygrywa zawsze (szczególnie ważne przy weight tying)\n",
        "                if is_bias or is_blacklisted:\n",
        "                    param_to_decay[pid] = False\n",
        "                elif is_linear_weight:\n",
        "                    # decay tylko jeśli parametr nie został wcześniej oznaczony jako no_decay\n",
        "                    param_to_decay.setdefault(pid, True)\n",
        "                else:\n",
        "                    # bezpieczny domyślny wybór: brak decay\n",
        "                    param_to_decay.setdefault(pid, False)\n",
        "\n",
        "        decay_params = [id_to_param[pid] for pid, dec in param_to_decay.items() if dec]\n",
        "        nodecay_params = [id_to_param[pid] for pid, dec in param_to_decay.items() if not dec]\n",
        "\n",
        "        optim_groups = [\n",
        "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
        "            {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
        "        ]\n",
        "\n",
        "        # device_type: jeśli przekazujesz torch.device, użyj device_type = device.type\n",
        "        if isinstance(device_type, torch.device):\n",
        "            device_type = device_type.type\n",
        "\n",
        "        use_fused = (device_type == \"cuda\") and (\"fused\" in inspect.signature(torch.optim.AdamW).parameters)\n",
        "        print(f\"Używanie fused AdamW: {use_fused}\")\n",
        "\n",
        "        return torch.optim.AdamW(\n",
        "            optim_groups,\n",
        "            lr=learning_rate,\n",
        "            betas=betas,\n",
        "            fused=use_fused,\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx: torch.Tensor, max_new_tokens: int, temperature: float = 1.0):\n",
        "        \"\"\"\n",
        "        Generuje kolejne tokeny autoregresyjnie\n",
        "\n",
        "        Ważne:\n",
        "        - Zapamiętujemy poprzedni tryb (train/eval) i przywracamy go na końcu,\n",
        "          żeby generate() nie psuło treningu, jeśli zostanie wywołane w trakcie.\n",
        "        - @torch.no_grad() wyłącza gradienty (szybciej i mniej pamięci)\n",
        "        \"\"\"\n",
        "        was_training = self.training\n",
        "        self.eval()\n",
        "        try:\n",
        "            for _ in range(max_new_tokens):\n",
        "                # Ograniczamy kontekst do block_size (model nie widzi dalej).\n",
        "                idx_cond = idx[:, -self.config.block_size:]\n",
        "\n",
        "                # forward() w trybie inferencji zwraca logity tylko dla ostatniej pozycji (b, 1, vocab)\n",
        "                logits, _ = self(idx_cond)\n",
        "                logits = logits[:, -1, :]  # (b, vocab)\n",
        "\n",
        "                # temperature=0 -> deterministycznie (argmax)\n",
        "                if temperature == 0.0:\n",
        "                    idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "                else:\n",
        "                    logits = logits / temperature\n",
        "                    probs = F.softmax(logits, dim=-1)\n",
        "                    idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                idx = torch.cat([idx, idx_next], dim=1)\n",
        "\n",
        "            return idx\n",
        "        finally:\n",
        "            # Przywróć poprzedni tryb modelu\n",
        "            self.train(was_training)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB = {'0': 0, '1': 1, '+': 2, '=': 3, '<PAD>': 4, '<EOS>': 5}\n",
        "INV_VOCAB = {v: k for k, v in VOCAB.items()}\n",
        "PAD_IDX = VOCAB['<PAD>']\n",
        "EOS_IDX = VOCAB['<EOS>']\n",
        "\n",
        "def encode(tokens):\n",
        "    return [VOCAB[t] for t in tokens]\n",
        "\n",
        "def decode(tokens):\n",
        "    return ''.join(INV_VOCAB[t] for t in tokens if t in INV_VOCAB)\n",
        "\n",
        "def make_addition_example(variant, max_bits=15):\n",
        "    # 1. Losujemy dwie liczby\n",
        "    a_int = random.randint(0, 2**max_bits - 1)\n",
        "    b_int = random.randint(0, 2**max_bits - 1)\n",
        "    c_int = a_int + b_int\n",
        "\n",
        "    a_bin = bin(a_int)[2:]\n",
        "    b_bin = bin(b_int)[2:]\n",
        "    c_bin = bin(c_int)[2:]\n",
        "\n",
        "    if variant == 1:\n",
        "        # Stała liczba bitów (dopełnienie zerami do 15/16)\n",
        "        a_str = a_bin.zfill(max_bits)\n",
        "        b_str = b_bin.zfill(max_bits)\n",
        "        c_str = c_bin.zfill(max_bits + 1)\n",
        "    elif variant == 2:\n",
        "        # Minimalna liczba bitów\n",
        "        a_str, b_str, c_str = a_bin, b_bin, c_bin\n",
        "    elif variant == 3:\n",
        "        # Odwrócona kolejność bitów (Najmniej znaczący bit pierwszy)\n",
        "        a_str, b_str, c_str = a_bin[::-1], b_bin[::-1], c_bin[::-1]\n",
        "\n",
        "    # Konstrukcja ciągu: A + B = C <EOS>\n",
        "    input_str = a_str + '+' + b_str + '='\n",
        "    full_tokens = list(input_str) + list(c_str) + ['<EOS>']\n",
        "\n",
        "    return torch.tensor(encode(full_tokens), dtype=torch.long), input_str, c_str"
      ],
      "metadata": {
        "id": "QKG2CFi5sTqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, variant, n_examples=1000, max_bits=15):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    correct = 0\n",
        "\n",
        "    for _ in range(n_examples):\n",
        "        ids, prefix, target_suffix = make_addition_example(variant, max_bits)\n",
        "        prompt_ids = encode(list(prefix))\n",
        "        prompt_tensor = torch.tensor(prompt_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Generujemy tyle tokenów, ile ma wynik + EOS\n",
        "        max_gen = len(target_suffix) + 1\n",
        "        out = model.generate(prompt_tensor, max_new_tokens=max_gen, temperature=0.0)\n",
        "\n",
        "        gen_str = decode(out[0, len(prompt_ids):].tolist())\n",
        "        if target_suffix + '<EOS>' in gen_str or target_suffix == gen_str.replace('<EOS>',''):\n",
        "            correct += 1\n",
        "\n",
        "    return correct / n_examples\n",
        "\n",
        "def train_variant(variant_idx, total_steps=15000):\n",
        "    print(f\"\\n--- Trenowanie Wariantu {variant_idx} ---\")\n",
        "    config = GPTConfig()\n",
        "    model = GPT(config).to(device)\n",
        "\n",
        "    optimizer = model.configure_optimizers(weight_decay=0.01, learning_rate=5e-4, betas=(0.9, 0.95), device_type=device)\n",
        "    loss_history = []\n",
        "\n",
        "    for step in range(total_steps):\n",
        "        # Generowanie batcha\n",
        "        batch = [make_addition_example(variant_idx) for _ in range(256)]\n",
        "        xs_list = [item[0][:-1] for item in batch]\n",
        "        ys_list = [item[0][1:] for item in batch]\n",
        "\n",
        "        xs = pad_sequence(xs_list, batch_first=True, padding_value=PAD_IDX).to(device)\n",
        "        ys = pad_sequence(ys_list, batch_first=True, padding_value=-1).to(device)\n",
        "\n",
        "        logits, loss = model(xs, ys)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_history.append(loss.item())\n",
        "        if step % 500 == 0:\n",
        "            print(f\"Krok {step}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    acc = evaluate_model(model, variant_idx)\n",
        "    print(f\"Finalna dokładność (Wariant {variant_idx}): {acc*100:.2f}%\")\n",
        "    return loss_history, acc"
      ],
      "metadata": {
        "id": "3w2D2wYCshwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "results = {}\n",
        "\n",
        "for v in [1, 2, 3]:\n",
        "    loss_h, final_acc = train_variant(v)\n",
        "    results[v] = {'loss': loss_h, 'acc': final_acc}\n",
        "\n",
        "# Wykresy\n",
        "plt.figure(figsize=(12, 6))\n",
        "for v in [1, 2, 3]:\n",
        "    plt.plot(results[v]['loss'], label=f'Wariant {v} (Acc: {results[v][\"acc\"]*100:.1f}%)')\n",
        "plt.xlabel('Krok')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Porównanie procesów uczenia dla 3 wariantów dodawania')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "05-g9dAzslzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analiza wyników i odpowiedzi na pytania\n",
        "1. Który z wariantów problemu jest najłatwiejszy do rozwiązania i dlaczego?\n",
        "Najłatwiejszym wariantem okazał się Wariant 3 (minimalna liczba bitów, kolejność odwrócona), który osiągnął 100.0% dokładności.\n",
        "\n",
        "Dlaczego? Wynika to z mechaniki dodawania pisemnego. Dodając dwie liczby, zaczynamy od najmniej znaczącego bitu (LSB), ponieważ musimy wiedzieć, czy występuje \"przeniesienie\" (carry) do następnej kolumny. W wariancie 3 model dostaje bity i generuje wynik dokładnie w tej samej kolejności, w jakiej działają prawa arytmetyki. Dzięki temu sieć może łatwo \"pamiętać\" bit przeniesienia z poprzedniego kroku i wykorzystać go w bieżącym.\n",
        "\n",
        "2. Czy wariant 3. ułatwia trening w stosunku do wariantu 2., jeżeli tak to dlaczego?\n",
        "Tak, wariant 3 znacząco ułatwia trening (skok dokładności z 89.4% na 100.0% oraz szybszy spadek funkcji straty).\n",
        "\n",
        "Uzasadnienie: W wariancie 2 (kolejność standardowa, od MSB) model musi przewidzieć pierwszy (najbardziej znaczący) bit wyniku, nie wiedząc jeszcze, co stanie się na samym końcu liczby. Jednak pierwszy bit wyniku może zależeć od przeniesienia, które powstało na samym końcu i \"przeszło\" przez całą długość liczby (tzw. carry propagation). Jest to problem zależności dalekosiężnej, który jest trudny dla sieci. W wariancie 3 problem ten znika. Przeniesienie jest zawsze dostępne \"pod ręką\", z poprzedniego wygenerowanego tokena.\n",
        "\n",
        "3. Komentarz do otrzymanych wyników\n",
        "Wariant 3 (100%): Model perfekcyjnie opanował algorytm dodawania. Stabilizacja straty nastąpiła bardzo szybko (już około 2500 kroku strata spadła poniżej 0.5 i pozostała stabilna).\n",
        "\n",
        "Wariant 2 (89.4%): Model radzi sobie dobrze, ale popełnia błędy, prawdopodobnie przy bardzo długich przeniesieniach, które muszą być przewidziane \"z góry\".\n",
        "\n",
        "Wariant 1 (32.7%): To najciekawszy wynik - stała liczba 15 bitów okazała się najtrudniejsza. Prawdopodobnie wynika to z faktu, że przy stałej długości (dopełnianie zerami) sekwencje są zawsze długie, a istotne dane są \"rozmyte\" w dużej ilości zer. Atencja modelu musi przeszukiwać znacznie dłuższy kontekst, co przy ograniczonej liczbie parametrów (0.79M) i kroków treningowych utrudniło znalezienie poprawnej reguły."
      ],
      "metadata": {
        "id": "r2AZPebY0qgB"
      }
    }
  ]
}