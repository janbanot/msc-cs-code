{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janbanot/msc-cs-code/blob/main/sem3/DL/DL_2025_Lab5-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJgoLJHUJ0Wo"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<style>\n",
        ".cell-output-ipywidget-background{background-color:transparent!important}\n",
        ":root{--jp-widgets-color:var(--vscode-editor-foreground);--jp-widgets-font-size:var(--vscode-editor-font-size)}\n",
        "</style>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEKd0WKyJ0Wq"
      },
      "source": [
        "# Przykład 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKNPsEjxi2Bc"
      },
      "source": [
        "Wytrenowany model językowy typu BERT można zastosować, np. do przewidywania **brakujących słów w tekście** -- tutaj oznaczanych za pomocą specjalnego tokena [MASK]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVymo4DPfGF9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "# Załaduj wstępnie wytrenowany model i tokenizator\n",
        "# To inicjalizuje tokenizator BERT i model języka maskowanego z wersji wstępnie wytrenowanej\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "model.eval()  # Ustawia model w tryb ewaluacji, wyłączając dropout i normalizację wsadową\n",
        "\n",
        "print(f\"L. param. modelu: {sum(p.numel() for p in model.parameters()) / 1_000_000:.1f} mln\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCWpXaopfKK0"
      },
      "outputs": [],
      "source": [
        "# Przykładowe zdanie z zamaskowanym tokenem\n",
        "sentence = \"The quick brown [MASK] jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenizuj wejście\n",
        "# Przekształca zdanie w identyfikatory tokenów, dodając specjalne tokeny i zwracając tensor PyTorch\n",
        "input_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
        "print(f'{input_ids = }')\n",
        "\n",
        "# Znajduje indeks tokenu [MASK] w stokenizowanym wejściu\n",
        "mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
        "print(f'{tokenizer.mask_token_id = } {mask_token_index = }')\n",
        "\n",
        "# Uzyskaj przewidywania dla zamaskowanego tokenu\n",
        "with torch.no_grad():  # wyłącz obliczanie gradientów, aby zaoszczędzić pamięć podczas wnioskowania\n",
        "    output = model(input_ids)\n",
        "logits = output.logits  # Surowe wyniki predykcji dla każdego tokenu w słowniku\n",
        "\n",
        "# Wyodrębnij logity odpowiadające zamaskowanemu tokenowi i uzyskaj 5 najlepszych przewidywań\n",
        "mask_token_logits = logits[0, mask_token_index, :]\n",
        "# Pobiera indeksy 5 tokenów o najwyższych wynikach\n",
        "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
        "print(f'{top_5_tokens = }')\n",
        "\n",
        "# Dekoduj przewidywane tokeny\n",
        "# Przekształca identyfikatory tokenów z powrotem w czytelne dla człowieka słowa, usuwając białe znaki\n",
        "predicted_tokens = [tokenizer.decode([token]).strip() for token in top_5_tokens]\n",
        "print(\"Przewidywane tokeny:\", predicted_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59CIZQqcArGr"
      },
      "outputs": [],
      "source": [
        "# Możemy również zamienić logity na prawdop. za pomocą softmax\n",
        "probs = torch.softmax(mask_token_logits, dim=1)\n",
        "top_k = torch.topk(probs, k=5)\n",
        "top_confidence, top_idx = top_k.values, top_k.indices\n",
        "\n",
        "for token, prob in zip(top_idx[0], top_confidence[0]):\n",
        "    print(f\"{tokenizer.decode([token]).strip()} - {prob.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d28ZtKhJJ0Wu"
      },
      "source": [
        "# Zadanie 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_oksqRXjb3D"
      },
      "source": [
        "Na podstawie poprzedniego przykładu sprawdź jak model BERT radzi sobie z przewidywaniem różnych części mowy w podanych zdaniach.\n",
        "\n",
        "Użyj modelu `dkleczek/bert-base-polish-uncased-v1` dla języka polskiego.\n",
        "\n",
        "Zdania:\n",
        "\n",
        "    sentences = {\n",
        "        \"noun\": \"Uwielbiam jeść [MASK].\",\n",
        "        \"verb\": \"Ja [MASK] pizzę.\",\n",
        "        \"adjective\": \"Ta pizza jest [MASK].\"\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXYho5FCoS7P"
      },
      "source": [
        "## Rozwiązanie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcQm1nFoz3jF"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('dkleczek/bert-base-polish-uncased-v1')\n",
        "model = BertForMaskedLM.from_pretrained('dkleczek/bert-base-polish-uncased-v1')\n",
        "\n",
        "\n",
        "print(f\"L. param. modelu: {sum(p.numel() for p in model.parameters()) / 1_000_000:.1f} mln\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = {\n",
        "    \"noun\": \"Uwielbiam jeść małe [MASK].\",\n",
        "    \"verb\": \"Ja [MASK] małą pizzę.\",\n",
        "    \"adjective\": \"Ten chłopak jest [MASK].\"\n",
        "}\n",
        "\n",
        "for part_of_speech, sentence in sentences.items():\n",
        "    print(f\"\\nPrzewidywania dla '{part_of_speech}':\")\n",
        "    # Tokenizuj wejście\n",
        "    input_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
        "\n",
        "    # Znajduje indeks tokenu [MASK] w stokenizowanym wejściu\n",
        "    mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "    # Uzyskaj przewidywania dla zamaskowanego tokenu\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids)\n",
        "    logits = output.logits\n",
        "\n",
        "    # Wyodrębnij logity odpowiadające zamaskowanemu tokenowi i uzyskaj 5 najlepszych przewidywań\n",
        "    mask_token_logits = logits[0, mask_token_index, :]\n",
        "    top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
        "\n",
        "    # Dekoduj przewidywane tokeny\n",
        "    predicted_tokens = [tokenizer.decode([token]).strip() for token in top_5_tokens]\n",
        "    print(\"Przewidywane tokeny:\", predicted_tokens)\n"
      ],
      "metadata": {
        "id": "bt7-TeOH0KEr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}