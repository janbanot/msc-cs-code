{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janbanot/msc-cs-code/blob/main/sem3/DL/DL_2025_Lab7-b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb89db57",
      "metadata": {
        "id": "eb89db57"
      },
      "outputs": [],
      "source": [
        "# Opcjonalna instalacja gymnasium\n",
        "\n",
        "!uv pip install gymnasium\n",
        "!uv pip install \"gymnasium[toy-text]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d53a73d5",
      "metadata": {
        "id": "d53a73d5"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "def enable_video(env, interval=200):\n",
        "    \"\"\"\n",
        "    Monkey-patch `env` so that:\n",
        "      - env.frames will accumulate all rgb_array frames whenever you call env.render()\n",
        "      - env.get_video() returns an IPython.display.HTML object of the animation.\n",
        "    \"\"\"\n",
        "    if env.render_mode != 'rgb_array':\n",
        "        env.get_video = lambda : \"Render mode should be 'rgb_array' to get a video\"\n",
        "        return env  # No recording\n",
        "\n",
        "    env.frames = []\n",
        "    orig_render = env.render\n",
        "\n",
        "    def _render_and_capture(*args, **kwargs):\n",
        "        frame = orig_render(*args, **kwargs)  # get the RGB array\n",
        "        env.frames.append(frame)\n",
        "        return frame\n",
        "\n",
        "    env.render = _render_and_capture\n",
        "\n",
        "    def get_video():\n",
        "        if not env.frames:\n",
        "            return \"No frames in env.frames; make sure you called env.render() at least once.\"\n",
        "        fig, ax = plt.subplots(figsize=(3, 3), tight_layout=True)\n",
        "        ax.axis(\"off\")\n",
        "        img = ax.imshow(env.frames[0], interpolation='none', animated=True)\n",
        "\n",
        "        def _update(i):\n",
        "            img.set_array(env.frames[i])\n",
        "            return (img,)\n",
        "\n",
        "        ani = FuncAnimation(fig, _update,\n",
        "                            frames=len(env.frames), interval=interval,\n",
        "                            blit=True, repeat=False)\n",
        "        plt.close(fig)\n",
        "        return HTML(ani.to_html5_video())\n",
        "\n",
        "    env.get_video = get_video\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f34b37cc",
      "metadata": {
        "id": "f34b37cc"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False,\n",
        "               render_mode='rgb_array')\n",
        "\n",
        "enable_video(env)\n",
        "\n",
        "state, info = env.reset()\n",
        "terminated = False\n",
        "truncated = False\n",
        "while not (terminated or truncated):\n",
        "    action = env.action_space.sample()  # Losowa akcja\n",
        "    # Przejście do kolejnego stanu\n",
        "    state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    if env.render_mode:\n",
        "        env.render()   # Dodaje kolejną klatkę animacji\n",
        "\n",
        "env.close()\n",
        "\n",
        "env.get_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf43d44a",
      "metadata": {
        "id": "cf43d44a"
      },
      "source": [
        "# Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6389a588",
      "metadata": {
        "id": "6389a588"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def train(episodes):\n",
        "    env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False,\n",
        "                render_mode=None)\n",
        "\n",
        "    q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    print(f'{q.shape = }')\n",
        "\n",
        "    learning_rate = 0.9\n",
        "    discount_factor = 0.9\n",
        "\n",
        "    epsilon = 1\n",
        "    epsilon_decay_rate = 1 / episodes\n",
        "    rng = np.random.default_rng()\n",
        "    rewards = np.zeros(episodes)\n",
        "\n",
        "    for i in range(episodes):\n",
        "        state = env.reset()[0]\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        while not (terminated or truncated):\n",
        "            if rng.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = np.argmax(q[state, :])\n",
        "\n",
        "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            # Aktualizacja funkcji (tablicy) stanu-akcji\n",
        "            q[state, action] = q[state, action] + learning_rate * (\n",
        "                reward + discount_factor * np.max(q[new_state, :]) - q[state, action]\n",
        "            )\n",
        "            rewards[i] += reward\n",
        "\n",
        "            state = new_state\n",
        "\n",
        "        epsilon = max(epsilon - epsilon_decay_rate, 0)\n",
        "        learning_rate = max(learning_rate - 1 / episodes, 1e-4)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    sum_rewards = np.zeros(episodes)\n",
        "    for t in range(episodes):\n",
        "        sum_rewards[t] = np.sum(rewards[max(0, t-100) : t+1])\n",
        "\n",
        "    plt.plot(sum_rewards)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward (past 100 episodes)')\n",
        "    plt.show()\n",
        "    return q\n",
        "\n",
        "q_table = train(15_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c0c308f",
      "metadata": {
        "id": "3c0c308f"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False,\n",
        "               render_mode='rgb_array')\n",
        "\n",
        "enable_video(env, interval=300)\n",
        "\n",
        "state, info = env.reset()\n",
        "terminated = False\n",
        "truncated = False\n",
        "while not (terminated or truncated):\n",
        "    action = np.argmax(q_table[state, :])\n",
        "    state, reward, terminated, truncated, info = env.step(action)\n",
        "    env.render()\n",
        "\n",
        "env.close()\n",
        "\n",
        "env.get_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db211e8f",
      "metadata": {
        "id": "db211e8f"
      },
      "source": [
        "# Zad. 1.\n",
        "\n",
        "Sprawdź działanie algorytmu Q-learning dla problemu FrozenLake-v1 i mapy \"8x8\".\n",
        "* Rozważ wariant `is_slippery = False` oraz `is_slippery = True`\n",
        "* Uruchom algorytm kilka razy, czy zawsze otrzymujemy zbieżność?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee495ac0",
      "metadata": {
        "id": "ee495ac0"
      },
      "source": [
        "# Deep Q-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51b1cfe1",
      "metadata": {
        "id": "51b1cfe1"
      },
      "source": [
        "W poniższym przykładzie funkcja stanu--wartości Q przybliżana jest za pomocą sieci neuronowej.\n",
        "W celu stabilizacji procesu nauki stosowana jest druga sieć, tzw. \"docelowa\", która\n",
        "jest aktualizowana rzadziej niż sieć podstawowa, tj. \"online\"."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebf94f26",
      "metadata": {
        "id": "ebf94f26"
      },
      "source": [
        "## Cart Pole\n",
        "\n",
        "Rozwiązywanym problemem jest [Cart Pole](https://gymnasium.farama.org/environments/classic_control/cart_pole/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "474e87f9",
      "metadata": {
        "id": "474e87f9"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "enable_video(env, interval=100)\n",
        "\n",
        "state, info = env.reset()\n",
        "terminated = False\n",
        "truncated = False\n",
        "episode_reward = 0\n",
        "while not (terminated or truncated):\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, terminated, truncated, info = env.step(action)\n",
        "    # cart_pos, cart_v, pole_angle, pole_angular_v = state\n",
        "    episode_reward += 1\n",
        "    env.render()\n",
        "\n",
        "env.close()\n",
        "\n",
        "print(f'{episode_reward = }')\n",
        "env.get_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e5d32ab",
      "metadata": {
        "id": "2e5d32ab"
      },
      "source": [
        "### Rozwiązanie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bf4788b",
      "metadata": {
        "id": "4bf4788b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "from copy import deepcopy\n",
        "\n",
        "# Hiperparametry\n",
        "n_inputs = 4\n",
        "n_outputs = 2\n",
        "episodes = 200                # liczba epizodów treningu\n",
        "epsilon_start = 1.0           # początkowe ε dla polityki ε‐greedy\n",
        "epsilon_end = 0.01            # końcowe ε\n",
        "epsilon_decay = 200           # liczba epizodów, w ciągu których ε maleje\n",
        "gamma = 0.99                  # współczynnik dyskontujący\n",
        "batch_size = 32\n",
        "lr = 1e-3\n",
        "target_update_freq = 100      # co ile kroków zaktualizować sieć docelową\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "rng = np.random.default_rng()\n",
        "\n",
        "# 1) Definicja sieci online i docelowej\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(n_inputs, 32),\n",
        "    torch.nn.Tanh(),\n",
        "    torch.nn.Linear(32, n_outputs)\n",
        ").to(device)\n",
        "\n",
        "target_model = deepcopy(model).to(device)  # sieć docelowa\n",
        "target_model.eval()                        # tryb ewaluacji w sieci docelowej\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "replay_buffer = deque(maxlen=2000)\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=None)\n",
        "\n",
        "# Liniowe wygaszanie ε z epsilon_start do epsilon_end w ciągu epsilon_decay epizodów\n",
        "def get_epsilon(ep):\n",
        "    fraction = max(0, (epsilon_decay - ep) / epsilon_decay)\n",
        "    return epsilon_end + (epsilon_start - epsilon_end) * fraction\n",
        "\n",
        "step_count = 0\n",
        "episodes_reward = []\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state, _ = env.reset()\n",
        "    state = np.array(state, dtype=np.float32)\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        epsilon = get_epsilon(episode)\n",
        "        # 1) Wybór akcji zgodnie z ε‐greedy\n",
        "        if rng.random() < epsilon:  # Losowo\n",
        "            action = env.action_space.sample()\n",
        "        else:  # Zachłannie\n",
        "            with torch.no_grad():\n",
        "                state_t = torch.from_numpy(state).unsqueeze(0).to(device)  # kształt: [1, 4]\n",
        "                action = int(model(state_t).argmax(dim=1).item())\n",
        "\n",
        "        # 2) Wykonanie kroku w środowisku\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        next_state = np.array(next_state, dtype=np.float32)\n",
        "        episode_reward += reward\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # 3) Zapis przejścia do bufora doświadczeń\n",
        "        replay_buffer.append((state, np.int64(action), np.float32(reward), next_state, np.float32(done)))\n",
        "        state = next_state\n",
        "\n",
        "        # 4) Trening, gdy bufor ma wystarczająco wiele próbek\n",
        "        if len(replay_buffer) >= batch_size:\n",
        "            # Wybierz losowo próbkę doświadczeń\n",
        "            batch_indices = rng.choice(len(replay_buffer), batch_size, replace=False)\n",
        "            batch = [replay_buffer[i] for i in batch_indices]\n",
        "            states, actions, rewards, next_states, dones = [\n",
        "                torch.from_numpy(\n",
        "                    np.array([experience[field] for experience in batch])\n",
        "                ).to(device)\n",
        "                for field in range(5)\n",
        "            ]\n",
        "\n",
        "            # 4.4) Obliczanie Q(s, a) z sieci online\n",
        "            idx = torch.arange(states.size(0))\n",
        "            state_action_values = model(states)[idx, actions]\n",
        "\n",
        "            # 4.5) Obliczanie target Q: r + γ * max_a' Q_target(s', a') * (1 − done)\n",
        "            with torch.no_grad():\n",
        "                max_next_Q = target_model(next_states).max(dim=1)[0]    # [32]\n",
        "                target_q = rewards + gamma * max_next_Q * (1.0 - dones) # [32]\n",
        "\n",
        "            # 4.6) Obliczanie straty (MSE między Q(s,a) a target_q)\n",
        "            loss = F.mse_loss(state_action_values, target_q)\n",
        "\n",
        "            # 4.7) Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # 5) Okresowa aktualizacja sieci docelowej\n",
        "        step_count += 1\n",
        "        if step_count % target_update_freq == 0:\n",
        "            target_model.load_state_dict(model.state_dict())\n",
        "            target_model.eval()\n",
        "\n",
        "    print(f\"Epizod {episode+1}/{episodes} Nagroda: {episode_reward:.1f} ε: {epsilon:.3f}\")\n",
        "    episodes_reward.append(episode_reward)\n",
        "\n",
        "env.close()\n",
        "\n",
        "plt.plot(episodes_reward);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74925329",
      "metadata": {
        "id": "74925329"
      },
      "outputs": [],
      "source": [
        "# Podgląd działania wytrenowanego modelu\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "enable_video(env, interval=100)\n",
        "\n",
        "state, info = env.reset()\n",
        "terminated = False\n",
        "truncated = False\n",
        "episode_reward = 0\n",
        "while not (terminated or truncated):\n",
        "    with torch.no_grad():\n",
        "        state_t = torch.from_numpy(state).unsqueeze(0).to(device)\n",
        "        action = int(model(state_t).argmax(dim=1).item())\n",
        "\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)\n",
        "    episode_reward += 1\n",
        "    env.render()\n",
        "    state = next_state\n",
        "\n",
        "env.close()\n",
        "\n",
        "print(f'{episode_reward = }')\n",
        "env.get_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d44806d1",
      "metadata": {
        "id": "d44806d1"
      },
      "source": [
        "# Zad 2."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c68928",
      "metadata": {
        "id": "a1c68928"
      },
      "source": [
        "Rozwiąż problem [MountainCar-v0](https://gymnasium.farama.org/environments/classic_control/mountain_car/)\n",
        "\n",
        "Celem agenta jest doprowadzenie małego samochodu na szczyt wzgórza po prawej stronie. Problem polega na tym, że silnik samochodu jest zbyt słaby, aby osiągnąć cel bez odpowiedniego rozpędu - agent musi nauczyć się kołysać samochodem tam i z powrotem, wykorzystując energię kinetyczną.\n",
        "\n",
        "**Stan (obserwacje)**\n",
        "\n",
        "Agent otrzymuje wektor stanu zawierający:\n",
        "\n",
        "* `position` ∈ \\[−1.2, 0.6] - pozycja samochodu wzdłuż osi X (metry),\n",
        "* `velocity` ∈ \\[−0.07, 0.07] - prędkość samochodu (metry/sek).\n",
        "\n",
        "**Przestrzeń akcji**\n",
        "\n",
        "Dyskretna przestrzeń z 3 deterministycznymi akcjami:\n",
        "\n",
        "* `0` – przyspiesz w lewo,\n",
        "* `1` – nie przyspieszaj,\n",
        "* `2` – przyspiesz w prawo.\n",
        "\n",
        "**Nagroda i cel**\n",
        "\n",
        "* Agent otrzymuje **−1 za każdy krok**, dopóki nie osiągnie celu (pozycja ≥ 0.5).\n",
        "* Epizod kończy się po maks. 200 krokach lub osiągnięciu celu.\n",
        "\n",
        "**Wskazówki**\n",
        "\n",
        "Problem ten jest *trudniejszy* od `CartPole`, dlatego rozważ:\n",
        "- zwiększenie liczby epizodów, np. >= 2000 (pamiętaj o zmianie epislon_decay)\n",
        "- zwiększeniu rozmiariu bufora powtórek, np. do 10000\n",
        "- rzadszej aktualizacji sieci docelowej (target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8775fc05",
      "metadata": {
        "id": "8775fc05"
      },
      "outputs": [],
      "source": [
        "# Podgląd\n",
        "\n",
        "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
        "\n",
        "enable_video(env, interval=1000 / 25)\n",
        "\n",
        "state, info = env.reset()\n",
        "terminated = False\n",
        "truncated = False\n",
        "while not (terminated or truncated):\n",
        "    action = env.action_space.sample()  # Losowa akcja\n",
        "    state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    if env.render_mode:\n",
        "        env.render()\n",
        "\n",
        "env.close()\n",
        "\n",
        "env.get_video()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "822d0b46",
      "metadata": {
        "id": "822d0b46"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Hiperparametry dostosowane do MountainCar ---\n",
        "n_inputs = 2   # Pozycja, Prędkość\n",
        "n_outputs = 3  # Lewo, Nic, Prawo\n",
        "episodes = 3000                 # Znacznie więcej epizodów\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.01\n",
        "epsilon_decay = 2500            # Bardzo wolne wygaszanie eksploracji\n",
        "gamma = 0.99\n",
        "batch_size = 64                 # Nieco większy batch\n",
        "lr = 0.001\n",
        "target_update_freq = 200        # Rzadsza aktualizacja sieci docelowej\n",
        "buffer_size = 20000             # Większy bufor pamięci\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "rng = np.random.default_rng()\n",
        "\n",
        "# 1) Definicja sieci - Nieco większa niż dla CartPole\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(n_inputs, 64),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(64, 64),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(64, n_outputs)\n",
        ").to(device)\n",
        "\n",
        "target_model = deepcopy(model).to(device)\n",
        "target_model.eval()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "replay_buffer = deque(maxlen=buffer_size)\n",
        "\n",
        "# Zwykły tryb renderowania, aby przyspieszyć trening (bez wideo)\n",
        "env = gym.make(\"MountainCar-v0\", render_mode=None)\n",
        "\n",
        "def get_epsilon(ep):\n",
        "    fraction = max(0, (epsilon_decay - ep) / epsilon_decay)\n",
        "    return epsilon_end + (epsilon_start - epsilon_end) * fraction\n",
        "\n",
        "step_count = 0\n",
        "episodes_reward = []\n",
        "success_count = 0 # Licznik sukcesów (dotarcia do flagi)\n",
        "\n",
        "print(f\"Rozpoczynam trening na urządzeniu: {device}\")\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state, _ = env.reset()\n",
        "    state = np.array(state, dtype=np.float32)\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        epsilon = get_epsilon(episode)\n",
        "\n",
        "        # 1) Wybór akcji (Epsilon-Greedy)\n",
        "        if rng.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state_t = torch.from_numpy(state).unsqueeze(0).to(device)\n",
        "                action = int(model(state_t).argmax(dim=1).item())\n",
        "\n",
        "        # 2) Krok środowiska\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        next_state = np.array(next_state, dtype=np.float32)\n",
        "\n",
        "        # Modyfikacja: W MountainCar standardowa nagroda to zawsze -1.\n",
        "        # Sukces to terminated == True (dotarcie do flagi, pozycja 0.5)\n",
        "        # Truncated to koniec czasu (200 kroków)\n",
        "        episode_reward += reward\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # 3) Zapis do bufora\n",
        "        replay_buffer.append((state, np.int64(action), np.float32(reward), next_state, np.float32(done)))\n",
        "        state = next_state\n",
        "\n",
        "        # 4) Trening\n",
        "        if len(replay_buffer) >= batch_size:\n",
        "            batch_indices = rng.choice(len(replay_buffer), batch_size, replace=False)\n",
        "            batch = [replay_buffer[i] for i in batch_indices]\n",
        "\n",
        "            states, actions, rewards, next_states, dones = [\n",
        "                torch.from_numpy(\n",
        "                    np.array([experience[field] for experience in batch])\n",
        "                ).to(device)\n",
        "                for field in range(5)\n",
        "            ]\n",
        "\n",
        "            # Q(s, a)\n",
        "            idx = torch.arange(states.size(0))\n",
        "            state_action_values = model(states)[idx, actions]\n",
        "\n",
        "            # Target Q\n",
        "            with torch.no_grad():\n",
        "                max_next_Q = target_model(next_states).max(dim=1)[0]\n",
        "                target_q = rewards + gamma * max_next_Q * (1.0 - dones)\n",
        "\n",
        "            loss = F.mse_loss(state_action_values, target_q)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # 5) Aktualizacja sieci docelowej\n",
        "        step_count += 1\n",
        "        if step_count % target_update_freq == 0:\n",
        "            target_model.load_state_dict(model.state_dict())\n",
        "            target_model.eval()\n",
        "\n",
        "    if terminated: # Sukces!\n",
        "        success_count += 1\n",
        "\n",
        "    # Logowanie co 100 epizodów\n",
        "    if (episode + 1) % 100 == 0:\n",
        "        avg_reward = np.mean(episodes_reward[-100:]) if episodes_reward else -200\n",
        "        print(f\"Epizod {episode+1}/{episodes} | Śr. nagroda (ost. 100): {avg_reward:.1f} | Sukcesy: {success_count} | ε: {epsilon:.3f}\")\n",
        "\n",
        "    episodes_reward.append(episode_reward)\n",
        "\n",
        "env.close()\n",
        "\n",
        "# Wykres nagród\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(episodes_reward)\n",
        "plt.title(\"MountainCar-v0: Nagroda w czasie\")\n",
        "plt.xlabel(\"Epizod\")\n",
        "plt.ylabel(\"Nagroda\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Podgląd działania wytrenowanego modelu\n",
        "# enable_video musi być zdefiniowane wcześniej (z pierwszych komórek notebooka)\n",
        "\n",
        "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
        "env = enable_video(env, interval=50) # Szybsze odświeżanie dla płynności\n",
        "\n",
        "state, info = env.reset()\n",
        "terminated = False\n",
        "truncated = False\n",
        "episode_reward = 0\n",
        "\n",
        "while not (terminated or truncated):\n",
        "    with torch.no_grad():\n",
        "        state_t = torch.from_numpy(np.array(state, dtype=np.float32)).unsqueeze(0).to(device)\n",
        "        # Wybór najlepszej akcji (zachłannie)\n",
        "        action = int(model(state_t).argmax(dim=1).item())\n",
        "\n",
        "    state, reward, terminated, truncated, info = env.step(action)\n",
        "    episode_reward += reward\n",
        "    env.render()\n",
        "\n",
        "env.close()\n",
        "\n",
        "print(f'Końcowa nagroda testowa: {episode_reward}')\n",
        "# Jeśli nagroda > -200, oznacza to, że samochód dotarł do flagi przed upływem czasu.\n",
        "env.get_video()"
      ],
      "metadata": {
        "id": "sRf9zneQwp4x"
      },
      "id": "sRf9zneQwp4x",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}