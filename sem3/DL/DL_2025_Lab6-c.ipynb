{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janbanot/msc-cs-code/blob/main/sem3/DL/DL_2025_Lab6-c.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2fec908",
      "metadata": {
        "id": "f2fec908"
      },
      "outputs": [],
      "source": [
        "!pip -q install torchinfo\n",
        "\n",
        "# Import potrzebnych modułów i funkcji\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70207cae",
      "metadata": {
        "id": "70207cae"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<!-- Potrzebne dla poprawnego wyświetlania paska postępu tqdm w VSCode https://stackoverflow.com/a/77566731 -->\n",
        "<style>\n",
        ".cell-output-ipywidget-background {\n",
        "    background-color: transparent !important;\n",
        "}\n",
        ":root {\n",
        "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
        "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
        "}\n",
        "</style>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4dcb274",
      "metadata": {
        "id": "a4dcb274"
      },
      "source": [
        "# Modele generatywne -- część II"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b494e61e",
      "metadata": {
        "id": "b494e61e"
      },
      "source": [
        "## Flow Matching & Stochastic Flow Matching"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce1da578",
      "metadata": {
        "id": "ce1da578"
      },
      "source": [
        "### Przykład 1\n",
        "\n",
        "Poniższy przykład bazuje na [przykładzie](https://github.com/facebookresearch/flow_matching/blob/main/examples/standalone_flow_matching.ipynb) z biblioteki `flow_matching`\n",
        "\n",
        "*Celem* jest trening modelu, który umożliwia przekształcenie próbek z rozkładu początkowego\n",
        "w próbki z rozkładu docelowego (danych). Rozkład początkowy to _rozkład normalny_ (Gaussa),\n",
        "natomiast końcowy to rozkład _dwóch półksiężyców_ próbkowany za pomocą `sklearn.datasets.make_moons`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3421cebb",
      "metadata": {
        "id": "3421cebb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "# Generate 200 samples using make_moons\n",
        "X, _ = make_moons(n_samples=200, noise=0.1, random_state=41)\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(X[:, 0], X[:, 1])\n",
        "plt.title(\"2D samples from sklearn.datasets.make_moons\")\n",
        "plt.xlabel(\"X1\")\n",
        "plt.ylabel(\"X2\")\n",
        "plt.grid(True)\n",
        "plt.axis('equal')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42453516",
      "metadata": {
        "id": "42453516"
      },
      "source": [
        "#### Model\n",
        "\n",
        "Model przybliża *pole wektorowe*, które dla podanego pkt. oraz chwili czasu $t \\in [0, 1]$ wskazuje\n",
        "wektor prędkości wg, którego powinien być przesunięty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aba0da05",
      "metadata": {
        "id": "aba0da05"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch import nn, Tensor\n",
        "\n",
        "\n",
        "class Flow(nn.Module):\n",
        "    \" Perceptron z 2 warstwami ukrytymi \"\n",
        "\n",
        "    def __init__(self, input_dim=2, hidden_dim=64):\n",
        "        super(Flow, self).__init__()\n",
        "        time_dim = 1\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim + time_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        \"\"\"\n",
        "        Wejściem sieci jest aktualne położenie pkt. oraz czas t.\n",
        "        Wynikiem jest wektor prędkości (2 wymiarowy).\n",
        "        \"\"\"\n",
        "        return self.encoder(torch.cat([x, t], dim=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ff7cc84",
      "metadata": {
        "id": "1ff7cc84"
      },
      "source": [
        "### Krok treningowy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "755d4143",
      "metadata": {
        "id": "755d4143"
      },
      "outputs": [],
      "source": [
        "def train_step(model, optimizer, x_1, stochastic=False):\n",
        "    \"\"\"\n",
        "    Wykonuje jeden krok treningowy dla modelu Flow Matching.\n",
        "\n",
        "    Kroki:\n",
        "      1. Próbkuj szum (x_0) z rozkładu normalnego.\n",
        "      2. Wylosuj czas t dla każdego przykładu docelowego x_1.\n",
        "      3. Wygeneruj zaszumiony obraz (x_t) jako interpolację między szumem a \"czystym\" przykładem.\n",
        "      4. Zdefiniuj cel jako różnicę (prędkość) między czystym przykładem a szumem.\n",
        "      5. Naucz model przewidywać tę prędkość.\n",
        "\n",
        "    Zwraca:\n",
        "      float: Obliczona wartość funkcji straty.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    batch_size = x_1.size(0)\n",
        "\n",
        "    # 1. Próbkowanie szumu z rozkładu normalnego.\n",
        "    x_0 = torch.randn_like(x_1, device=x_1.device)\n",
        "\n",
        "    # 2. Próbkowanie czasu t z rozkładu jednostajnego dla każdego przykładu, kształt [B, 1, ...].\n",
        "    t = torch.rand([batch_size] + [1]*(x_0.ndim - 1), device=x_1.device)\n",
        "\n",
        "    # 3. Obliczenie zaszumionego przykładu x_t jako interpolacji między x_0 a x_1.\n",
        "    # Zakładamy, że punkty poruszają się po prostej łączącej x_0 z x_1 (ścieżka liniowa)\n",
        "    x_t = (1 - t) * x_0 + t * x_1\n",
        "\n",
        "    if stochastic:  # Przypadek stochastyczny (SFM): Dodaj szum zgodnie z harmonogramem sigma_t\n",
        "        sigma_t = calculate_sigma(t)\n",
        "        noise = torch.randn_like(x_1) # Próbkowanie szumu z rozkładu normalnego\n",
        "        x_t = x_t + sigma_t * noise\n",
        "\n",
        "    # 4. Obliczenie docelowej prędkości -- dla ścieżki liniowej\n",
        "    target_velocity = x_1 - x_0\n",
        "\n",
        "    # 5. Predykcja prędkości za pomocą modelu\n",
        "    predicted_velocity = model(x_t, t.view(-1, 1))\n",
        "\n",
        "    loss = torch.mean((predicted_velocity - target_velocity)**2)  # MSE\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def calculate_sigma(t):\n",
        "    \"\"\"\n",
        "    Funkcja definiująca harmonogram szumu dla SFM.\n",
        "    Prosty przebieg sinusoidalny – maksimum przy t=0.5, zero przy t=0 i t=1.\n",
        "    \"\"\"\n",
        "    return 0.01 * torch.sin(torch.pi * t) + 1e-4 # Dodaj małe epsilon dla stabilności"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c81d3174",
      "metadata": {
        "id": "c81d3174"
      },
      "source": [
        "### Trening"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af81e3e1",
      "metadata": {
        "id": "af81e3e1"
      },
      "outputs": [],
      "source": [
        "model = Flow()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.005)\n",
        "\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    num_batches = 1000\n",
        "    for _ in range(num_batches):\n",
        "        batch_size = 256\n",
        "        x1 = Tensor(make_moons(batch_size, noise=0.05)[0])\n",
        "        loss_val = train_step(model, optimizer, x1, stochastic=False)\n",
        "        total_loss += loss_val\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    print(f'[{epoch}] {avg_loss = :.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e10971e",
      "metadata": {
        "id": "5e10971e"
      },
      "source": [
        "### Próbkowanie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7f2dc0b",
      "metadata": {
        "id": "b7f2dc0b"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def sample_trajectory(model, shape, device='cpu', steps=100, samples=1, checkpoint_every=10):\n",
        "    \"\"\"\n",
        "    Generuje trajektorie próbek z modelu flow-matching metodą Eulera.\n",
        "\n",
        "    Rozpoczyna przy t=0 od szumu gaussowskiego, a następnie iteracyjnie aktualizuje:\n",
        "        x ← x + v(x, t) * Δt\n",
        "    aż do t=1, zapisując stan co checkpoint_every kroków.\n",
        "\n",
        "    Args:\n",
        "        model:       wytrenowany model predykcji prędkości v(x, t).\n",
        "        shape:       kształt jednej próbki (np. (1, 28, 28)).\n",
        "        steps:       liczba kroków Eulera od t=0 do t=1.\n",
        "        samples:     liczba próbek w batchu.\n",
        "        device:      'cpu' lub 'cuda'.\n",
        "        checkpoint_every: odstęp kroków między zapisem stanu.\n",
        "\n",
        "    Returns:\n",
        "        Tensor o kształcie [num_checkpoints, samples, *shape]\n",
        "        zawierający zapisane stany x (przeniesione na CPU).\n",
        "    \"\"\"\n",
        "    device = torch.device(device)\n",
        "    model.eval()\n",
        "    # Inicjalizacja od losowego szumu dla t=0\n",
        "    x = torch.randn(samples, *shape, device=device)\n",
        "\n",
        "    time_steps = torch.linspace(0, 1, steps + 1, device=device)\n",
        "    delta_t = time_steps[1] - time_steps[0]\n",
        "\n",
        "    # Ile stanów będziemy zapisywać\n",
        "    num_ckpt = steps // checkpoint_every\n",
        "    ckpt_idx = 0\n",
        "    traj = torch.empty(num_ckpt, samples, *shape, device=device)\n",
        "    for i in range(steps):\n",
        "        # Tworzymy wektor czasu o rozmiarze batcha\n",
        "        t = time_steps[i].repeat(samples, 1)\n",
        "\n",
        "        v = model(x, t)   # Predykcja prędkości v(x, t)\n",
        "\n",
        "        # Aktualizacja Eulera w miejscu: x ← x + v * Δt\n",
        "        x = x + v * delta_t\n",
        "\n",
        "        if (i+1) % checkpoint_every == 0:\n",
        "            traj[ckpt_idx] = x\n",
        "            ckpt_idx += 1\n",
        "\n",
        "    return traj.reshape(-1, samples, *shape).cpu()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, num_samples, data_shape, device, num_steps=100):\n",
        "    \" Próbkowanie z modelu FM/SFM (ostatni pkt. trajektorii) \"\n",
        "\n",
        "    model.eval()\n",
        "    device = torch.device(device)\n",
        "    # Inicjalizacja: szum w chwili t=0\n",
        "    x = torch.randn(num_samples, *data_shape, device=device)\n",
        "\n",
        "    time_steps = torch.linspace(0, 1, num_steps + 1, device=device)\n",
        "    delta_t = time_steps[1] - time_steps[0]\n",
        "\n",
        "    for i in range(num_steps):\n",
        "        t = time_steps[i].repeat(num_samples, 1)\n",
        "        v = model(x, t)\n",
        "        x = x + v * delta_t  # Krok Eulera\n",
        "    return x.cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9dfbb05",
      "metadata": {
        "id": "b9dfbb05"
      },
      "outputs": [],
      "source": [
        "dim = 2\n",
        "trajectory = sample_trajectory(model, (dim, ), samples=200)\n",
        "n_steps = trajectory.shape[0]\n",
        "\n",
        "fig, axes = plt.subplots(1, n_steps + 1, figsize=(30, 4), sharex=True, sharey=True)\n",
        "time_steps = torch.linspace(0, 1.0, n_steps + 1)\n",
        "\n",
        "x = trajectory[0].detach()\n",
        "axes[0].scatter(x[:, 0], x[:, 1], s=10)\n",
        "axes[0].set_title(f't = {time_steps[0]:.2f}')\n",
        "axes[0].set_xlim(-3.0, 3.0)\n",
        "axes[0].set_ylim(-3.0, 3.0)\n",
        "\n",
        "for i in range(n_steps):\n",
        "    x = trajectory[i].detach()\n",
        "    axes[i + 1].scatter(x[:, 0], x[:, 1], s=10)\n",
        "    axes[i + 1].set_title(f't = {time_steps[i + 1]:.2f}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b830c19",
      "metadata": {
        "id": "2b830c19"
      },
      "source": [
        "### Przypadek stochastyczny"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44e00503",
      "metadata": {
        "id": "44e00503"
      },
      "outputs": [],
      "source": [
        "model = Flow()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.005)\n",
        "\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    num_batches = 1000\n",
        "    for _ in range(num_batches):\n",
        "        batch_size = 256\n",
        "        x1 = Tensor(make_moons(batch_size, noise=0.05)[0])\n",
        "        loss_val = train_step(model, optimizer, x1, stochastic=True)\n",
        "        total_loss += loss_val\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    print(f'[{epoch}] {avg_loss = :.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad9e8fbf",
      "metadata": {
        "id": "ad9e8fbf"
      },
      "outputs": [],
      "source": [
        "dim = 2\n",
        "trajectory = sample_trajectory(model, (dim, ), samples=200)\n",
        "n_steps = trajectory.shape[0]\n",
        "\n",
        "fig, axes = plt.subplots(1, n_steps + 1, figsize=(30, 4), sharex=True, sharey=True)\n",
        "time_steps = torch.linspace(0, 1.0, n_steps + 1)\n",
        "\n",
        "x = trajectory[0].detach()\n",
        "axes[0].scatter(x[:, 0], x[:, 1], s=10)\n",
        "axes[0].set_title(f't = {time_steps[0]:.2f}')\n",
        "axes[0].set_xlim(-3.0, 3.0)\n",
        "axes[0].set_ylim(-3.0, 3.0)\n",
        "\n",
        "for i in range(n_steps):\n",
        "    x = trajectory[i].detach()\n",
        "    axes[i + 1].scatter(x[:, 0], x[:, 1], s=10)\n",
        "    axes[i + 1].set_title(f't = {time_steps[i + 1]:.2f}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77d9944b",
      "metadata": {
        "id": "77d9944b"
      },
      "source": [
        "## Przykład 2. MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f69bc2e5",
      "metadata": {
        "id": "f69bc2e5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# Ogranicz do wybranych cyfr\n",
        "def filter_indices(dataset, target_digits=[4, 7]):\n",
        "    indices = []\n",
        "    for idx in range(len(dataset)):\n",
        "        if dataset[idx][1] in target_digits:\n",
        "            indices.append(idx)\n",
        "    return indices\n",
        "\n",
        "# Indeksy dla obrazów wybranych cyfr\n",
        "train_indices = filter_indices(train_dataset)\n",
        "test_indices = filter_indices(test_dataset)\n",
        "\n",
        "# Podzbiory\n",
        "train_subset = Subset(train_dataset, train_indices)\n",
        "test_subset = Subset(test_dataset, test_indices)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(dataset=train_subset, batch_size=batch_size, shuffle=True,\n",
        "                          pin_memory=True, num_workers=2, persistent_workers=True)\n",
        "test_loader = DataLoader(dataset=test_subset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5911a7b",
      "metadata": {
        "id": "d5911a7b"
      },
      "source": [
        "### Model\n",
        "\n",
        "Spróbujemy z prostą siecią typu MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0edb935f",
      "metadata": {
        "id": "0edb935f"
      },
      "outputs": [],
      "source": [
        "def get_device():   # Obliczenia wykonamy na GPU, jeżeli jest dostępne, a na CPU w przeciwny razie\n",
        "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class TimeEmbeddingNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Sieć do kodowania czasu do wektora o wymiarach time_emb_dim\n",
        "    Wej: t, shape [B, 1]\n",
        "    Wyj: wektor czasu [B, time_emb_dim]\n",
        "    \"\"\"\n",
        "    def __init__(self, time_emb_dim=128):\n",
        "        super(TimeEmbeddingNet, self).__init__()\n",
        "        self.linear1 = nn.Linear(1, time_emb_dim)\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.linear2 = nn.Linear(time_emb_dim, time_emb_dim)\n",
        "\n",
        "    def forward(self, t):   # t: [B, 1]\n",
        "        t_emb = self.relu(self.linear1(t))\n",
        "        t_emb = self.relu(self.linear2(t_emb))\n",
        "        return t_emb  # [B, time_emb_dim]\n",
        "\n",
        "\n",
        "class FlowMNIST(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=1024, time_dim=128):\n",
        "        super(FlowMNIST, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim + time_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "\n",
        "        self.time_emb = TimeEmbeddingNet(time_emb_dim=time_dim)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        t = self.time_emb(t)\n",
        "        return self.encoder(torch.cat([x, t], dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eca7598",
      "metadata": {
        "id": "4eca7598"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "\n",
        "\n",
        "model = FlowMNIST(28 * 28, time_dim=16)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "model = model.to(get_device())\n",
        "num_epochs = 7\n",
        "scheduler = MultiStepLR(optimizer, milestones=[4,6], gamma=0.1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch_x1, _ in tqdm(train_loader):\n",
        "        x1 = batch_x1.view(-1, 28*28)\n",
        "        loss_val = train_step(model, optimizer, x1.to(get_device()), stochastic=False)\n",
        "        total_loss += loss_val\n",
        "    scheduler.step()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f'{avg_loss = :.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "055c6359",
      "metadata": {
        "id": "055c6359"
      },
      "source": [
        "### Trajektoria i próbkowanie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0db26e9e",
      "metadata": {
        "id": "0db26e9e"
      },
      "outputs": [],
      "source": [
        "import torchvision.utils as vutils\n",
        "\n",
        "\n",
        "def show_images(images):\n",
        "    images = torch.stack([(img - img.min()) / (img.max() - img.min() + 1e-5) for img in images])\n",
        "    nrow = ncol = int(len(images) ** 0.5 + 0.5)\n",
        "    grid = vutils.make_grid(images, nrow=nrow, padding=2)\n",
        "    np_grid = grid.numpy().transpose((1, 2, 0))\n",
        "    plt.figure(figsize=(ncol, nrow))\n",
        "    plt.imshow(np_grid, cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "\n",
        "show_images( sample_trajectory(model, (28 * 28, ), device=get_device()).view(-1, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "289f1b9d",
      "metadata": {
        "id": "289f1b9d"
      },
      "outputs": [],
      "source": [
        "samples = sample(model, 9, (28 * 28, ), get_device())\n",
        "show_images(samples.view(9, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59873390",
      "metadata": {
        "id": "59873390"
      },
      "source": [
        "### U-Net"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aec52526",
      "metadata": {
        "id": "aec52526"
      },
      "source": [
        "Uzyskanie wyników lepszej jakości wymaga modelu o architekturze lepiej dopasowanej do zadania, np. sieci U-Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b19056",
      "metadata": {
        "id": "11b19056"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Podstawowy blok rezydualny z warunkowaniem czasowym wykonujący:\n",
        "      - Dwie warstwy konwolucyjne.\n",
        "      - Warunkowanie czasowe przez wyuczoną projekcję liniową wspólnego osadzenia czasowego.\n",
        "      - Dopasowanie połączenia rezydualnego, jeśli liczba kanałów wejścia i wyjścia się różni.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dim):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, groups=out_channels // 16)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, groups=out_channels // 32)\n",
        "        # Projekcja wspólnego osadzenia czasowego na wektor cech do transmisji.\n",
        "        self.time_mlp = nn.Linear(time_emb_dim, out_channels)\n",
        "        self.relu = nn.LeakyReLU(inplace=True)\n",
        "        # Dopasowanie połączenia rezydualnego, jeśli wymiary kanałów są różne.\n",
        "        self.res_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else None\n",
        "        self.norm = nn.GroupNorm(num_groups=in_channels // 8, num_channels=in_channels)\n",
        "\n",
        "    def forward(self, x, t_emb):\n",
        "        h = self.relu(self.conv1(self.norm(x)))\n",
        "        # Projekcja osadzenia czasowego i dodanie go do mapy cech (broadcasting po wymiarach przestrzennych).\n",
        "        time_feature = self.time_mlp(t_emb).unsqueeze(-1).unsqueeze(-1)\n",
        "        h = h + time_feature\n",
        "        h = self.relu(self.conv2(h))\n",
        "        # Zastosowanie połączenia rezydualnego.\n",
        "        residual = self.res_conv(x) if self.res_conv is not None else x\n",
        "        return h + residual\n",
        "\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Blok zmniejszający rozdzielczość (koder), stosujący blok rezydualny, a następnie konwolucję ze skokiem.\n",
        "    Zwraca zarówno cechy z bloku rezydualnego (do połączenia typu skip), jak i wyjście o zmniejszonej rozdzielczości.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dim):\n",
        "        super().__init__()\n",
        "        self.resblock = ResidualBlock(in_channels, out_channels, time_emb_dim)\n",
        "        # Downsampling o czynnik 2.\n",
        "        self.downsample = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1, groups=out_channels//16)\n",
        "\n",
        "    def forward(self, x, t_emb):\n",
        "        skip = self.resblock(x, t_emb)\n",
        "        x_down = self.downsample(skip)\n",
        "        return skip, x_down\n",
        "\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Blok zwiększający rozdzielczość (dekoder), który:\n",
        "      - Zwiększa rozdzielczość za pomocą transponowanej konwolucji.\n",
        "      - Konkatenuje wynik upsamplingu z odpowiadającym połączeniem typu skip.\n",
        "      - Przetwarza połączone cechy przy pomocy bloku rezydualnego.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, skip_channels, out_channels, time_emb_dim):\n",
        "        super().__init__()\n",
        "        self.upsample = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, groups=out_channels//16)\n",
        "        self.resblock = ResidualBlock(out_channels + skip_channels, out_channels, time_emb_dim)\n",
        "\n",
        "    def forward(self, x, skip, t_emb):\n",
        "        x_up = self.upsample(x)\n",
        "        # Konkatenuj po wymiarze kanałów (dodanie skip jako dodatkowych kanałów).\n",
        "        x_cat = torch.cat([x_up, skip], dim=1)\n",
        "        return self.resblock(x_cat, t_emb)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Model U-Net z warunkowaniem czasowym przystosowany do generowania obrazów w stylu modeli dyfuzyjnych.\n",
        "    Model:\n",
        "      1. Oblicza wspólne osadzenie czasowe.\n",
        "      2. Przetwarza wejściowy obraz za pomocą ścieżki kodującej (zmniejszającej rozdzielczość).\n",
        "      3. Stosuje blok typu „wąskie gardło”.\n",
        "      4. Odtwarza wynik przy użyciu ścieżki dekodującej (zwiększającej rozdzielczość) z połączeniami typu skip.\n",
        "    \"\"\"\n",
        "    def __init__(self, time_emb_dim=128, mult=1):\n",
        "        super().__init__()\n",
        "        # Wspólna sieć osadzająca czas (zdefiniowana w innym miejscu).\n",
        "        self.time_embed = TimeEmbeddingNet(time_emb_dim=time_emb_dim)\n",
        "        self.relu = nn.LeakyReLU(inplace=True)\n",
        "        # Początkowa konwolucja: zmiana z 1 kanału wejściowego na 32 mapy cech.\n",
        "        self.initial_conv = nn.Conv2d(1, 32*mult, kernel_size=3, padding=1)\n",
        "\n",
        "        # Koder / ścieżka zmniejszająca rozdzielczość.\n",
        "        self.down1 = DownBlock(32*mult, 64*mult, time_emb_dim)\n",
        "        self.down2 = DownBlock(64*mult, 128*mult, time_emb_dim)\n",
        "\n",
        "        # Blok wąskiego gardła o najmniejszej rozdzielczości.\n",
        "        self.bottleneck = ResidualBlock(128*mult, 128*mult, time_emb_dim)\n",
        "\n",
        "        # Dekoder / ścieżka zwiększająca rozdzielczość.\n",
        "        self.up1 = UpBlock(128*mult, skip_channels=128*mult, out_channels=64*mult, time_emb_dim=time_emb_dim)\n",
        "        self.up2 = UpBlock(64*mult, skip_channels=64*mult, out_channels=32*mult, time_emb_dim=time_emb_dim)\n",
        "\n",
        "        # Ostatnia konwolucja: zmiana na 1 kanał wyjściowy.\n",
        "        self.final_conv = nn.Conv2d(32*mult, 1, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        \"\"\"\n",
        "        Przepływ danych przez model U-Net.\n",
        "        x: obrazy wejściowe o kształcie [B, 1, 28, 28].\n",
        "        t: skalary czasowe o kształcie [B, 1].\n",
        "        \"\"\"\n",
        "        t_emb = self.time_embed(t)\n",
        "        x = self.relu(self.initial_conv(x))\n",
        "        skip1, x = self.down1(x, t_emb)\n",
        "        skip2, x = self.down2(x, t_emb)\n",
        "        x = self.bottleneck(x, t_emb)\n",
        "        x = self.up1(x, skip2, t_emb)\n",
        "        x = self.up2(x, skip1, t_emb)\n",
        "        return self.final_conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dab35c61",
      "metadata": {
        "id": "dab35c61"
      },
      "outputs": [],
      "source": [
        "model = UNet(time_emb_dim=64).to(get_device())\n",
        "# display(summary(model))\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = MultiStepLR(optimizer, milestones=[5,6], gamma=0.1)\n",
        "\n",
        "num_epochs = 7\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    for clean_images, _ in tqdm(train_loader):\n",
        "        clean_images = clean_images.to(get_device())\n",
        "        loss_value = train_step(model, optimizer, clean_images, stochastic=True)\n",
        "        total_loss += loss_value\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}: Loss = {avg_loss:.4f}\")\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d2ef2a7",
      "metadata": {
        "id": "6d2ef2a7"
      },
      "outputs": [],
      "source": [
        "generated_samples = sample(model, num_samples=16, data_shape=(1, 28, 28), device=get_device())\n",
        "show_images(generated_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71c6a00c",
      "metadata": {
        "id": "71c6a00c"
      },
      "outputs": [],
      "source": [
        "show_images( sample_trajectory(model, (1, 28, 28), device=get_device()).view(10, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4444e61b",
      "metadata": {
        "id": "4444e61b"
      },
      "source": [
        "### Alternatywne metody próbkowania\n",
        "\n",
        "Metoda Eulera rozw. równań różniczkowych (czyli generowania trajektorii od szumu\n",
        "do danych) jest jedną z prostszych. Potencjalna poprawa jakości generowanych przykładów\n",
        "jest możliwa, gdy zastosujemy metodę bardziej zaawansowaną, np. dopri5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdiffeq"
      ],
      "metadata": {
        "id": "0s_nOsCUDF4E"
      },
      "id": "0s_nOsCUDF4E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "befd67a1",
      "metadata": {
        "id": "befd67a1"
      },
      "outputs": [],
      "source": [
        "from torchdiffeq import odeint  # Przykładowa biblioteka dla rozw. równań różniczkowych\n",
        "\n",
        "# model: wytrenowana sieć v_theta(x, t)\n",
        "# Musi spełniać wymaganą przez solver sygnaturę, np. f(t, x)\n",
        "class ODEFunc(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        # Solver może przekazać skalarną wartość t, więc trzeba ją rozszerzyć do batcha\n",
        "        t_vec = torch.ones(x.shape[0], 1, device=x.device) * t\n",
        "        # Zapewniamy, że t_vec ma format oczekiwany przez model\n",
        "        # W razie potrzeby dostosować kształt lub wymiary\n",
        "        return self.model(x, t_vec)\n",
        "\n",
        "\n",
        "def sample_dopri(model, data_shape, num_samples, device):\n",
        "    ode_func = ODEFunc(model).to(device)\n",
        "    x0 = torch.randn(num_samples, *data_shape, device=device)  # Próbka szumu\n",
        "    time_points = torch.linspace(0, 1, steps=100, device=device)  # Definicja punktów czasowych integracji\n",
        "\n",
        "    # Rozwiąż równanie różniczkowe: dx/dt = v_theta(x, t)\n",
        "    # 'odeint' wykonuje numeryczne kroki całkowania\n",
        "    # rtol, atol sterują dokładnością solvera (dokładność vs szybkość)\n",
        "    solution = odeint(ode_func, x0, time_points, method='dopri5', rtol=1e-5, atol=1e-5)\n",
        "    final_samples = solution[-1]  # Ostateczne próbki to stan w ostatnim punkcie czasowym (t=1)\n",
        "    return final_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "534f9e92",
      "metadata": {
        "id": "534f9e92"
      },
      "outputs": [],
      "source": [
        "samples = sample_dopri(model, (1, 28, 28), num_samples=16, device=get_device())\n",
        "show_images(samples.cpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c134b686",
      "metadata": {
        "id": "c134b686"
      },
      "source": [
        "# DDPM\n",
        "\n",
        "Odszumiające probabilistyczne modele dyfuzyjne (ang. Denoising Diffusion Probabilistic Models)\n",
        "są alternatywnym podejściem do generowania. W pewnym sensie są równoważne modelom SFM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3be86803",
      "metadata": {
        "id": "3be86803"
      },
      "outputs": [],
      "source": [
        "T = 500\n",
        "device = get_device()\n",
        "betas = torch.linspace(1e-4, 1e-2, steps=T).to(device)\n",
        "alphas = 1 - betas\n",
        "bar_alphas = torch.cumprod(alphas, dim=-1)\n",
        "bar_alphas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d203ef31",
      "metadata": {
        "id": "d203ef31"
      },
      "outputs": [],
      "source": [
        "# Zaszumianie danych\n",
        "\n",
        "for batch, _ in train_loader:\n",
        "    for x0 in batch:\n",
        "        break\n",
        "\n",
        "    for t in (0, 10, 100, 200, 300, 400, 499):\n",
        "        alpha_t = bar_alphas[t]\n",
        "        noise = torch.randn_like(x0).to(device)\n",
        "        x_t = alpha_t**0.5 * x0.to(device) + (1-alpha_t)**0.5 * noise\n",
        "        show_images(x_t.cpu())\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60f0a7ba",
      "metadata": {
        "id": "60f0a7ba"
      },
      "outputs": [],
      "source": [
        "def train_step_ddpm(model, clean_images):\n",
        "    \" Pojedynczy krok treningowy dla DDPM \"\n",
        "\n",
        "    model.train()\n",
        "    batch_size = clean_images.size(0)\n",
        "\n",
        "    x0 = clean_images\n",
        "    t = torch.randint(low=1, high=T+1, size=(batch_size,))\n",
        "\n",
        "    noise = torch.randn_like(x0, device=x0.device)\n",
        "\n",
        "    alpha_t = bar_alphas[t-1].view(-1, 1, 1, 1).to(x0.device)\n",
        "\n",
        "    # *Dodajemy* szum do czystego przykładu\n",
        "    x_t = alpha_t**0.5 * x0 + (1 - alpha_t)**0.5 * noise\n",
        "\n",
        "    # Predykcja *dodanego* szumu\n",
        "    pred_noise = model(x_t, (t / T).view(-1, 1).to(x0.device))\n",
        "\n",
        "    loss = torch.mean((pred_noise - noise)**2)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a8c740b",
      "metadata": {
        "id": "3a8c740b"
      },
      "outputs": [],
      "source": [
        "model = UNet(time_emb_dim=64).to(get_device())\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = MultiStepLR(optimizer, milestones=[8], gamma=0.1)\n",
        "\n",
        "num_epochs = 7\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    for clean_images, _ in tqdm(train_loader):\n",
        "        clean_images = clean_images.to(get_device())\n",
        "        loss_value = train_step_ddpm(model, clean_images)\n",
        "        total_loss += loss_value\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}: Loss = {avg_loss:.4f}\")\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9026a25",
      "metadata": {
        "id": "c9026a25"
      },
      "source": [
        "### Próbkowanie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6aee582",
      "metadata": {
        "id": "e6aee582"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def sample_images(model, num_steps: int, num_samples: int, device):\n",
        "    model.eval()\n",
        "    # Losowy szum początkowy\n",
        "    x = torch.randn(num_samples, 1, 28, 28, device=device)\n",
        "\n",
        "    for step in reversed(range(1, num_steps + 1)):\n",
        "        t_frac = torch.full((num_samples, 1), step / num_steps, device=device)\n",
        "        pred_noise = model(x, t_frac)\n",
        "\n",
        "        # Pobranie skalarów dla bieżącego kroku\n",
        "        a_bar = bar_alphas[step - 1].view(1, 1, 1, 1)\n",
        "        a     = alphas[step - 1].view(1, 1, 1, 1)\n",
        "        b     = betas[step - 1].view(1, 1, 1, 1)\n",
        "\n",
        "        # Obliczenia mu_t i sigma_t\n",
        "        mu    = (x - (b / (1 - a_bar).sqrt()) * pred_noise) / a.sqrt()\n",
        "        sigma = b.sqrt()\n",
        "\n",
        "        # Dodajemy szum, oprócz ostatniego kroku\n",
        "        noise = torch.randn_like(x) if step > 1 else 0.\n",
        "        x = mu + sigma * noise\n",
        "\n",
        "    return x\n",
        "\n",
        "# Generujemy i pokazujemy obrazy\n",
        "samples = sample_images(model, T, num_samples=16, device=device)\n",
        "show_images(samples.cpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "034ae6c5",
      "metadata": {
        "id": "034ae6c5"
      },
      "source": [
        "# Zadanie\n",
        "\n",
        "Proszę dokonać porównania modeli generatywnych, tj. VAE, FM/SM oraz DDPM\n",
        "na zbiorze FashionMNIST lub zbiorze [ludzkich twarzy](https://www.kaggle.com/datasets/badasstechie/celebahq-resized-256x256).\n",
        "W tym drugim przypadku, obrazy należy przekształcić na skalę szarości\n",
        "oraz zmienić rozdzielczość na 28x28 pikseli (lub podobną).\n",
        "\n",
        "Ze względu na \"trudniejszy\" problem należy rozważyć zwiększenie:\n",
        "- rozmiaru modelu\n",
        "- liczby epok treningu.\n",
        "\n",
        "\n",
        "**(Opcjonalnie)**\n",
        "\n",
        "Poprawę jakości oraz skrócenie czasu treningu może przynieść przejście z przestrzeni pikseli do przestrzeni\n",
        "_ukrytej_ za pomocą autokodera, np. 28x28 do 7x7.\n",
        "\n",
        "W takim przypadku, trening modelu generatywnego dokonuje się dla zakodowanej wersji,\n",
        "po czym dekoder pozwala powrócić z przestrzeni ukrytej do przestrzeni pikseli."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13f27248",
      "metadata": {
        "id": "13f27248"
      },
      "source": [
        "Ciekawe prace dot. modeli dyfuzyjnych:\n",
        "\n",
        "- [One Step Diffusion](https://openreview.net/pdf?id=OlzB6LnXcS)\n",
        "- [NeuralSVG](https://arxiv.org/pdf/2501.03992)\n",
        "- [Generative emulation of weather forecast ensembles with diffusion models](https://www.science.org/doi/10.1126/sciadv.adk4489)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}