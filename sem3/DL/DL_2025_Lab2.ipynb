{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AM_PZi-NkR1A",
        "n6z3P0c1kWUL",
        "od7Yoluv-4hT",
        "v0kPmN6RBpJj",
        "FExVPya4HDuY",
        "DC_FsUPLT7_6",
        "abanP1rYT-xY",
        "CeB488R5UUq4",
        "oRTCRkquj9gE",
        "ASVjJq5PU5nX",
        "KkBkUL0tBt18",
        "tVogZU8WsJ9f",
        "fg5wH4TjK67r",
        "t1cckYpXNn3K"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janbanot/msc-cs-code/blob/main/sem3/DL/DL_2025__lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Co potrafi neuron?"
      ],
      "metadata": {
        "id": "qDYxWPfte5gU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wstęp"
      ],
      "metadata": {
        "id": "AM_PZi-NkR1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# torch.nn.Module jest klasą bazową dla sieci i ich modułów\n",
        "class Net1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net1, self).__init__()\n",
        "        self.layer = nn.Linear(1, 1, bias=True)  # 1 wej. 1 wyj.\n",
        "        self.act = nn.Identity()  # f. aktywacji\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.layer(x))\n",
        "\n",
        "    def set_weights_example(self):\n",
        "        # Ustaw wart. parametrów ręcznie:\n",
        "        with torch.no_grad():\n",
        "            self.layer.weight.fill_(2)  # w = 2\n",
        "            self.layer.bias.fill_(0)    # b = 0"
      ],
      "metadata": {
        "id": "eg2YRIjIf4IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net1()\n",
        "\n",
        "pred = model(torch.tensor([1.0]))  # predykcja modelu, czyli w*x + b = 2*x + 0 = 2\n",
        "print(f'{pred = }')\n",
        "\n",
        "# Możemy obliczyć wynik dla wielu przykładów jednocześnie\n",
        "X = torch.tensor([[1.5],\n",
        "                  [-3]])\n",
        "pred = model(X)\n",
        "for x, p in zip(X, pred):\n",
        "    print(f'Dla x = {x.item()} predykcja = {p.item()}')  # item() wyciąga liczbę z tensora"
      ],
      "metadata": {
        "id": "a2WtuXmtgjXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Podgląd wartości wagi i bias\n",
        "model.layer.weight, model.layer.bias"
      ],
      "metadata": {
        "id": "KBOfavprjFds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# state_dict() zwraca informację o wszystkich parametrach oraz tzw. buforach modelu\n",
        "model.state_dict()"
      ],
      "metadata": {
        "id": "O4mKtcbcjSQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zamiast losowych możemy wymusić własne wartości wag\n",
        "model.set_weights_example()\n",
        "\n",
        "pred = model(X)\n",
        "for x, p in zip(X, pred):\n",
        "    print(f'Dla x = {x.item()} predykcja = {p.item()}')  # item() wyciąga liczbę z tensora"
      ],
      "metadata": {
        "id": "TV7bPrNai3fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Klasyfikacja 1D\n",
        "\n",
        "Jeden neuron z nieliniową f. aktywacji, np. sigmoidalną, może dokonać **klasyfikacji binarnej** dla liniowo separowalnych klas."
      ],
      "metadata": {
        "id": "n6z3P0c1kWUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dane treningowe\n",
        "X = torch.tensor([[1.],\n",
        "                  [0.5],\n",
        "                  [1.5],\n",
        "                  [2]])\n",
        "y = torch.tensor([[0],\n",
        "                  [0],\n",
        "                  [1],\n",
        "                  [1]], dtype=torch.int)\n",
        "\n",
        "# Nasz model\n",
        "class Net2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net2, self).__init__()\n",
        "        self.layer = nn.Linear(1, 1, bias=True)  # 1 wej. 1 wyj.\n",
        "        self.act = nn.Sigmoid()  # f. aktywacji\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.layer(x))\n",
        "\n",
        "    def set_weights(self, w, b):\n",
        "        with torch.no_grad():\n",
        "            self.layer.weight.fill_(w)\n",
        "            self.layer.bias.fill_(b)\n",
        "\n",
        "\n",
        "model = Net2()\n",
        "w, b = -2, -1.\n",
        "model.set_weights(w, b)"
      ],
      "metadata": {
        "id": "UEVRB0o5x34X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Narysujmy wykres dla naszego neuronu\n"
      ],
      "metadata": {
        "id": "J9OOKnHg7HWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_net2_a(model, show=True):\n",
        "    plt.figure(figsize=(5, 3))\n",
        "    plt.xlim(-4, 4)\n",
        "    plt.ylim(-2, 2)\n",
        "\n",
        "    # Prosta określona przez parametry naszego neuronu\n",
        "    x_range = torch.linspace(-5, 5, 100).unsqueeze(1)\n",
        "    plt.plot(x_range, model.layer(x_range).detach().numpy(), 'b-', label='y = w*x + b')\n",
        "\n",
        "    # Wynik aktywacji neuronu\n",
        "    y_sigmoid = model(x_range).detach().numpy()  # .detach() odłącza od grafu obliczeń\n",
        "    plt.plot(x_range, y_sigmoid, 'g-', label='Wyj. neuronu (f. sigmoid.)')\n",
        "\n",
        "    plt.xlabel('Wejście (x)')\n",
        "    plt.ylabel('Klasa / prawd.')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    if show:\n",
        "      plt.show()\n",
        "\n",
        "plot_net2_a(model)"
      ],
      "metadata": {
        "id": "SKt19BIn4zGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "F. sigmoidalna zwraca wart. z zakresu [0, 1], które możemy traktować jak prawopodobieństwo przynależności do klasy 1, tj.\n",
        " - dla wart. < 0.5 mamy klasę 0\n",
        " - dla wart. >= 0.5 mamy klasę 1\n",
        "\n",
        "**Granica decyzyjna** leży dokładnie w pkt. dla którego `sigmoid(x) = 0.5`"
      ],
      "metadata": {
        "id": "3TwLcg995I68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_net2_b(model, show=True):\n",
        "    plot_net2_a(model, show=False)\n",
        "\n",
        "    # Granica\n",
        "    w, b = model.layer.weight.item(), model.layer.bias.item()\n",
        "    decision_boundary = -b / w   # w*x + b = 0 (sigmoid = 0.5), czyli x = -b/w\n",
        "    # Wykres\n",
        "    plt.axvline(x=decision_boundary, color='darkred', linestyle='--', label=f'Granica decyzyjna (x = {decision_boundary:.2f})')\n",
        "    plt.legend()\n",
        "    if show:\n",
        "        plt.show()\n",
        "\n",
        "plot_net2_b(model)\n"
      ],
      "metadata": {
        "id": "UKmx2zjWxjc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jakie są **predykcje klas** dokonywane przez nasz model dla danych treningowych?"
      ],
      "metadata": {
        "id": "rkOVuuFy7hYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model(X)\n",
        "\n",
        "klasa = (pred >= 0.5).int()\n",
        "\n",
        "for x, p, k in zip(X, pred, klasa):\n",
        "    print(f'Dla x = {x.item()} predykcja = {p.item():.2f} klasa {k.item()}')  # item() wyciąga liczbę z tensora"
      ],
      "metadata": {
        "id": "c9vJjcHJ7rnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Narysujmy to\n",
        "\n",
        "def plot_net2_c(model, X, y, show=True):\n",
        "    plot_net2_b(model, show=False)\n",
        "\n",
        "    pred = model(X)\n",
        "    klasa = (pred >= 0.5).int()\n",
        "\n",
        "    colors = ('blue', 'red')\n",
        "    # Przykłady z danych wejściowych\n",
        "    plt.scatter(X, y*0.5, c=[colors[k] for k in klasa], marker='o')\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "plot_net2_c(model, X, y)"
      ],
      "metadata": {
        "id": "5b6Qm_zD6xWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zad. 1.\n",
        "\n",
        "Proszę znaleźć parametry modelu, dla którego model poprawnie klasyfikuje przykłady, tj. dwa pierwsze do klasy 0 (niebieski kolor), dwa pozostałe\n",
        "do klasy 1 (czerwony)."
      ],
      "metadata": {
        "id": "od7Yoluv-4hT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rozwiązanie...\n",
        "model = Net2()\n",
        "w, b = 5, -7\n",
        "\n",
        "model.set_weights(w, b)\n",
        "plot_net2_c(model, X, y)"
      ],
      "metadata": {
        "id": "ecaqpFdM_KGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Co potrafi sieć?"
      ],
      "metadata": {
        "id": "v0kPmN6RBpJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Załóżmy, że nasze dane wyglądają tak\n",
        "\n",
        "# Dane treningowe\n",
        "X = torch.tensor([[-2],\n",
        "                  [-1.5],\n",
        "                  [1.],\n",
        "                  [0.5],\n",
        "                  [1.5],\n",
        "                  [2]])\n",
        "\n",
        "y = torch.tensor([[1],\n",
        "                  [1],\n",
        "                  [0],\n",
        "                  [0],\n",
        "                  [1],\n",
        "                  [1]], dtype=torch.int)"
      ],
      "metadata": {
        "id": "5BKrNwSvBwEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pytanie** Czy możemy je poprawnie klasyfikować za pomocą *pojedynczego* neuronu?"
      ],
      "metadata": {
        "id": "eC8MjAO2CbQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_zad2(X, y, show=True):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.xlim(-3, 3)\n",
        "    plt.ylim(-3, 3)\n",
        "\n",
        "    plt.scatter(X, y, c='blue', marker='o')\n",
        "    plt.grid(True)\n",
        "\n",
        "    if show:\n",
        "      plt.show()\n",
        "\n",
        "plot_zad2(X, y)"
      ],
      "metadata": {
        "id": "C5m3uu-yCXrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jak widać, potrzebujemy *większej* liczby neuronów połączonych w **sieć**.\n",
        "\n",
        "          +---Neuron 1 [w1[0],b1[0]]\n",
        "          |                         \\\n",
        "    (x) --+                          *-- Neuron 3 [w2, b2] ->\n",
        "          |                         /\n",
        "          +---Neuron 2 [w1[1],b1[1]]"
      ],
      "metadata": {
        "id": "rfly1LvnDpoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zad 2.\n",
        "\n",
        "Spróbuj dobrać parametry modelu tak aby model poprawnie dokonywał klasyfikacji dla przykładowych danych.\n",
        "\n",
        "Działanie poszczególnych elementów widoczne jest na wykresie."
      ],
      "metadata": {
        "id": "B2jwk35hTfKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact, FloatSlider\n",
        "\n",
        "# Dane treningowe\n",
        "X = torch.tensor([[-2], [-1.5], [1.], [0.5], [1.5], [2]])\n",
        "y = torch.tensor([[1], [1], [0], [0], [1], [1]], dtype=torch.float)\n",
        "\n",
        "def plot_network(w1_0=1.0, w1_1=-1.0, b1_0=0.0, b1_1=-2.0, w2_0=1.0, w2_1=1.0, b2_0=0.0):\n",
        "    # Parametry sieci jako tensory\n",
        "    w1 = torch.tensor([[w1_0], [w1_1]])\n",
        "    b1 = torch.tensor([b1_0, b1_1])\n",
        "    w2 = torch.tensor([[w2_0], [w2_1]])  # Kształt: (2, 1)\n",
        "    b2 = torch.tensor([b2_0])\n",
        "\n",
        "    # Zakres wejść [-5, 5] na potrzeby wykresu\n",
        "    x_range = torch.linspace(-5, 5, 100).unsqueeze(1)\n",
        "\n",
        "    # Oblicz wyjścia modelu (sieci)\n",
        "    layer1_output = F.relu(x_range @ w1.T + b1)  # Kształt: (100, 2)\n",
        "    output = F.sigmoid(layer1_output @ w2 + b2)   # Kształt: (100, 1)\n",
        "\n",
        "    # Konwertuj na numpy\n",
        "    x_range_np = x_range.squeeze().numpy()\n",
        "    layer1_np = layer1_output.detach().numpy()\n",
        "    output_np = output.detach().numpy()\n",
        "\n",
        "    # Oblicz predykcje modelu (sieci) dla danych treningowych\n",
        "    layer1_train = F.relu(X @ w1.T + b1)\n",
        "    pred = F.sigmoid(layer1_train @ w2 + b2).detach().numpy().squeeze()\n",
        "\n",
        "    # Dane treningowe numpy na potrzeby wykresu\n",
        "    X_np = X.squeeze().numpy()\n",
        "    y_np = y.squeeze().numpy()\n",
        "\n",
        "    # Kolory dla punktów danych na podstawie predykcji (pomarańczowy region jeśli >0.5)\n",
        "    point_colors = ['orange' if p > 0.5 else 'blue' for p in pred]\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    for i, (data, title, color, label) in enumerate([\n",
        "        (layer1_np[:, 0], 'Warstwa 1 - Neuron 1', 'blue', f'w1_0={w1_0:.1f}, b1_0={b1_0:.1f}'),\n",
        "        (layer1_np[:, 1], 'Warstwa 1 - Neuron 2', 'green', f'w1_1={w1_1:.1f}, b1_1={b1_1:.1f}'),\n",
        "        (output_np.squeeze(), 'Warstwa wyjściowa', 'red', 'Wynik')\n",
        "    ], 1):\n",
        "        plt.subplot(1, 3, i)\n",
        "        plt.plot(x_range_np, data, color=color, label=label)\n",
        "\n",
        "        if i == 3:  # Dodaj tylko tło i dane tylko do wykresu wyjściowego\n",
        "            plt.scatter(X_np, y_np, c=point_colors, marker='o', label='Dane (pred >0.5: pomarańczowe)')\n",
        "            # Tło\n",
        "            ax = plt.gca()\n",
        "            mask = (output_np.squeeze() > 0.5)\n",
        "            ax.fill_between(x_range_np, -2, 2, where=mask, color='lightgreen', alpha=0.3, label='Wyjście > 0.5')\n",
        "\n",
        "        plt.title(title)\n",
        "        plt.xlabel('Wejście (x)')\n",
        "        plt.ylabel('Wyjście (y)')\n",
        "        plt.xlim([-3, 3])\n",
        "        plt.ylim([-2, 2])\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Utwórz interaktywne suwaki (dostosuj zakresy w razie potrzeby)\n",
        "interact(plot_network,\n",
        "         w1_0=FloatSlider(min=-5, max=5, step=0.1, value=1.0, description='w1_0'),\n",
        "         w1_1=FloatSlider(min=-5, max=5, step=0.1, value=-1.0, description='w1_1'),\n",
        "         b1_0=FloatSlider(min=-5, max=5, step=0.1, value=0.0, description='b1_0'),\n",
        "         b1_1=FloatSlider(min=-5, max=5, step=0.1, value=-2.0, description='b1_1'),\n",
        "         w2_0=FloatSlider(min=-5, max=5, step=0.1, value=1.0, description='w2_0'),\n",
        "         w2_1=FloatSlider(min=-5, max=5, step=0.1, value=1.0, description='w2_1'),\n",
        "         b2_0=FloatSlider(min=-5, max=5, step=0.1, value=0.0, description='b2_0'));"
      ],
      "metadata": {
        "id": "KmT_syns5p3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model PyTorch\n",
        "\n",
        "Zaimplementujmy model sieci obiektowo za pomocą PyTorch"
      ],
      "metadata": {
        "id": "hmA7OJ79TMAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net3, self).__init__()\n",
        "        # Ile wyjść tyle neuronów\n",
        "        self.hidden_layer = nn.Linear(1, 2, bias=True)  # 1 wej. 2 wyj.\n",
        "        self.h_act = nn.ReLU()  # f. aktywacji\n",
        "        self.out_layer = nn.Linear(2, 1, bias=True)  # 2 wej. 1 wyj.\n",
        "        self.act = nn.Sigmoid()  # f. aktywacji\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.h_act(self.hidden_layer(x))\n",
        "        return self.act(self.out_layer(h))\n",
        "\n",
        "    def set_weights(self, w1_1, w1_2, b1_1, b1_2, w2_1, w2_2, b2):\n",
        "        with torch.no_grad():\n",
        "            self.hidden_layer.weight.data = torch.tensor([[w1_1], [w1_2]], dtype=torch.float)\n",
        "            self.hidden_layer.bias.data = torch.tensor([b1_1, b1_2], dtype=torch.float)\n",
        "            self.out_layer.weight.data = torch.tensor([[w2_1, w2_2]], dtype=torch.float)\n",
        "            self.out_layer.bias.data = torch.tensor([b2], dtype=torch.float)\n",
        "\n",
        "\n",
        "model = Net3()\n",
        "\n",
        "model.set_weights(w1_1=0.5, b1_1=0,   # Neuron 1.1 (w. ukryta)\n",
        "                  w1_2=2, b1_2=0,     # Neuron 1.2 (w. ukryta)\n",
        "                  w2_1=1, w2_2=1, b2=-1)   # Neuron wyj.\n",
        "\n",
        "print(f'Parametry modelu:\\n {model.state_dict()}')\n",
        "print(f'Wyjście: {model(X)}\\npredykowana klasa:\\n{(model(X) > 0.5).long()}')"
      ],
      "metadata": {
        "id": "2xz1LlPlDci4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trening automatyczny"
      ],
      "metadata": {
        "id": "L1q-rbbVTnkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "model = Net3()\n",
        "# Definicja funkcji straty i optymalizatora\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy Loss dla klasyfikacji binarnej\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent\n",
        "\n",
        "# Parametry treningu\n",
        "num_epochs = 500\n",
        "\n",
        "# Pętla treningowa\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X)\n",
        "    loss = criterion(outputs, y.float())\n",
        "\n",
        "    # Backward pass i optymalizacja\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Wyświetlanie postępu co 100 epok\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoka [{epoch+1}/{num_epochs}], Strata: {loss.item():.4f}')\n",
        "\n",
        "# Sprawdzenie dokładności po treningu\n",
        "with torch.no_grad():\n",
        "    predicted = model(X)\n",
        "    predicted = (predicted >= 0.5).long()  # Próg 0.5 dla klasyfikacji binarnej\n",
        "    accuracy = (predicted.eq(y.float()).sum() / y.shape[0]).item()\n",
        "    print(f'Dokładność modelu na wszystkich danych: {accuracy:.4f}')\n",
        "\n",
        "print(model.state_dict())"
      ],
      "metadata": {
        "id": "CeezBJu1QNuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sposoby zapisu modeli z użyciem PyTorch\n",
        "\n"
      ],
      "metadata": {
        "id": "FExVPya4HDuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Styl sekwencyjny"
      ],
      "metadata": {
        "id": "DC_FsUPLT7_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(1, 2, bias=True),  # warstwa ukryta: 1 wejście -> 2 neurony\n",
        "            nn.ReLU(),                   # funkcja aktywacji ReLU\n",
        "            nn.Linear(2, 1, bias=True),  # warstwa wyjściowa: 2 wejścia -> 1 neuron\n",
        "            nn.Sigmoid()                 # funkcja aktywacji Sigmoid\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "module = Net3()\n",
        "module.state_dict()"
      ],
      "metadata": {
        "id": "ZG1LWhn1HROW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zalety:\n",
        "- Zwięzły i czytelny zapis.\n",
        "- Idealny dla prostych, liniowych sieci bez rozgałęzień.\n",
        "\n",
        "Wady:\n",
        "- Trudno podejrzeć wartości pośrednie (np. aktywacje po warstwie ukrytej)."
      ],
      "metadata": {
        "id": "KHgW5U1zHcbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Styl funkcyjny"
      ],
      "metadata": {
        "id": "abanP1rYT-xY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden_layer = nn.Linear(1, 2, bias=True)\n",
        "        self.out_layer = nn.Linear(2, 1, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.hidden_layer(x))       # aktywacja ReLU\n",
        "        y = torch.sigmoid(self.out_layer(h))   # aktywacja Sigmoid\n",
        "        return y\n",
        "\n",
        "module = Net3()\n",
        "module.state_dict()"
      ],
      "metadata": {
        "id": "umjq3Xd3UBqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zalety:\n",
        "- Duża elastyczność -- łatwo dodać np. połączenia rezydualne lub własne operacje.\n",
        "- Styl często używany w dokumentacji PyTorcha i w badaniach naukowych.\n",
        "\n",
        "Wady:\n",
        "- Nieco bardziej rozwlekły zapis.\n",
        "- Wymaga jawnego wywoływania funkcji aktywacji."
      ],
      "metadata": {
        "id": "1ZtmlNZ1UH7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Styl modularny\n"
      ],
      "metadata": {
        "id": "CeB488R5UUq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleDict({\n",
        "            \"hidden\": nn.Linear(1, 2),\n",
        "            \"out\": nn.Linear(2, 1)\n",
        "        })\n",
        "        self.h_act = nn.ReLU()\n",
        "        self.out_act = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.h_act(self.layers[\"hidden\"](x))\n",
        "        x = self.out_act(self.layers[\"out\"](x))\n",
        "        return x\n",
        "\n",
        "module = Net3()\n",
        "module.state_dict()"
      ],
      "metadata": {
        "id": "k8wpcyWjUYfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zalety:\n",
        "- Łatwo modyfikować i wymieniać warstwy w czasie eksperymentów.\n",
        "- Warstwy mają nazwy, co ułatwia przeglądanie modelu i zapisywanie konfiguracji.\n",
        "\n",
        "Wady:\n",
        "- Dla prostych sieci wprowadza niepotrzebny narzut."
      ],
      "metadata": {
        "id": "ZgUTW82IUd0B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zad 3. Zb. Iris (2D)\n",
        "\n",
        "Na podstawie rozwiązania poprzedniego zadania zaimplementuj model\n",
        "`NetIris` oraz dokonaj jego treningu z pomocą PyTorch."
      ],
      "metadata": {
        "id": "oRTCRkquj9gE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dane"
      ],
      "metadata": {
        "id": "TEnIGCUcAxDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "# Załaduj zestaw danych Iris\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Wybierz tylko pierwsze dwie cechy: długość płatka i szerokość płatka\n",
        "X = X[:, 0:2]\n",
        "\n",
        "# Połącz klasy 1 (versicolor) i 2 (virginica) w jedną klasę (1)\n",
        "# Klasa 0 pozostaje bez zmian (setosa)\n",
        "y = np.where(y == 2, 1, y)  # Wszystkie \"2\" zastąp \"1\"\n",
        "X[::10], y[::10]  # Pokaż co 10-ty zestaw danych"
      ],
      "metadata": {
        "id": "RcsE0MwqkCxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wykresy pomocnicze"
      ],
      "metadata": {
        "id": "4MRBNDXFUsE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_iris1(X, y):\n",
        "    # Wykres danych\n",
        "    fig = plt.figure(figsize=(7, 5))\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
        "    plt.xlabel('Długość płatka')\n",
        "    plt.ylabel('Szerokość płatka')\n",
        "    plt.title('Zbiór danych Iris')\n",
        "    plt.show()\n",
        "\n",
        "plot_iris1(X, y)"
      ],
      "metadata": {
        "id": "15SmXwkEkP50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_iris_with_decision_boundary(X, y, model):\n",
        "    # Przygotowanie siatki punktów do wyznaczenia granicy decyzyjnej\n",
        "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
        "                         np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "    # Przekształć siatkę na tensor i przewidź klasy\n",
        "    grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        Z = model(grid).numpy()\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Rysowanie wykresu\n",
        "    fig = plt.figure(figsize=(7, 5))\n",
        "    plt.contourf(xx, yy, Z, levels=[0, 0.5, 1], alpha=0.2, cmap='viridis')  # Granica decyzyjna\n",
        "    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linestyles='--')  # Linia granicy\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k')  # Punkty danych\n",
        "    plt.xlabel('Długość płatka')\n",
        "    plt.ylabel('Szerokość płatka')\n",
        "    plt.title('Zbiór danych Iris z granicą decyzyjną')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "zNlLRm69mN4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "ASVjJq5PU5nX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NetIris(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden_layer = nn.Linear(2, 2, bias=True)\n",
        "        self.out_layer = nn.Linear(2, 1, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.hidden_layer(x))       # aktywacja ReLU\n",
        "        y = torch.sigmoid(self.out_layer(h))   # aktywacja Sigmoid\n",
        "        return y\n",
        "\n",
        "model = NetIris()\n",
        "model.state_dict()\n",
        "\n",
        "plot_iris_with_decision_boundary(X, y, model)"
      ],
      "metadata": {
        "id": "iPL07QfXkus9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trening"
      ],
      "metadata": {
        "id": "iPbVcs7wVHPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch\n",
        "\n",
        "model = NetIris()\n",
        "# Definicja funkcji straty i optymalizatora\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy Loss dla klasyfikacji binarnej\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent\n",
        "\n",
        "# Convert NumPy arrays to PyTorch tensors\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "\n",
        "# Parametry treningu\n",
        "num_epochs = 500\n",
        "\n",
        "# Pętla treningowa\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X_tensor)\n",
        "    loss = criterion(outputs, y_tensor)\n",
        "\n",
        "    # Backward pass i optymalizacja\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Wyświetlanie postępu co 100 epok\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoka [{epoch+1}/{num_epochs}], Strata: {loss.item():.4f}')\n",
        "\n",
        "# Sprawdzenie dokładności po treningu\n",
        "with torch.no_grad():\n",
        "    predicted = model(X_tensor)\n",
        "    predicted = (predicted >= 0.5).long()  # Próg 0.5 dla klasyfikacji binarnej\n",
        "    accuracy = (predicted.eq(y_tensor.long()).sum() / y_tensor.shape[0]).item()\n",
        "    print(f'Dokładność modelu na wszystkich danych: {accuracy:.4f}')\n",
        "\n",
        "print(model.state_dict())"
      ],
      "metadata": {
        "id": "c3XvbAaTpDSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_iris_with_decision_boundary(X, y, model)"
      ],
      "metadata": {
        "id": "HiE7NJnxpZ5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Poprawki"
      ],
      "metadata": {
        "id": "KKFDjiOFAYuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Podział na zb. treningowy i walidacyjny\n",
        "\n"
      ],
      "metadata": {
        "id": "OI_cE7lOBnSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "W przypadku treningu sieci neuronowych (i nie tylko), ważny jest podział zbioru danych na część *treningową i testową*.\n",
        "\n",
        "W przypadku, gdy nasz zbiór jest odpowiednio duży i planujemy dostrajać **hiperparametry** modelu, np. liczbę warstw, neuronów, współczynnik szybkości uczenia, to dobrym pomysłem jest podział na 3 części: *treningową, walidacyjną oraz testową*.\n",
        "\n",
        "Część walidacyjna zastępuje testową w trakcie naszego procesu doboru hiperparametrów, a część testowa używana jest do oceny finalnego modelu."
      ],
      "metadata": {
        "id": "xJvpjBkiH61V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Załaduj zestaw danych Iris\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Wybierz tylko pierwsze dwie cechy: długość płatka i szerokość płatka\n",
        "X = X[:, 0:2]\n",
        "# Połącz klasy 1 (versicolor) i 2 (virginica) w jedną klasę (1)\n",
        "y = np.where(y == 2, 1, y)\n",
        "\n",
        "# Podział na część treningową oraz walidacyjną w proporcji 80 / 20\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "# PyTorch operuje na tensorach, więc dokonujemy konwersji z np.array\n",
        "# na torch.Tensor:\n",
        "x_train = torch.FloatTensor(x_train)\n",
        "y_train = torch.FloatTensor(y_train).view(-1, 1)\n",
        "\n",
        "x_test = torch.FloatTensor(x_test)\n",
        "y_test = torch.FloatTensor(y_test).view(-1, 1)\n",
        "\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "H7q3Sb_5AdxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trening"
      ],
      "metadata": {
        "id": "KkBkUL0tBt18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Przyda się możliwość obliczania dokładności predykcji modelu\n",
        "def compute_accuracy(model_output, y_true):\n",
        "    with torch.no_grad():  # Nie twórz grafu obliczeniowego\n",
        "        predicted = (model_output >= 0.5).float()  # Próg 0.5 dla klasyfikacji binarnej\n",
        "        accuracy = (predicted.eq(y_true).sum() / y_true.shape[0]).item()\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "model = NetIris()\n",
        "\n",
        "compute_loss = nn.BCELoss()  # Binary Cross Entropy Loss dla klasyfikacji binarnej\n",
        "\n",
        "# Hiperparametry dla treningu:\n",
        "n_iterations = 500\n",
        "eval_every = 1\n",
        "learning_rate = 0.05\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "history = defaultdict(list)\n",
        "\n",
        "# Trening modelu\n",
        "for i in range(n_iterations):\n",
        "    model.train()  # Przełącz w tryb treningu\n",
        "\n",
        "    out = model(x_train)  # Inferencja (faza w przód)\n",
        "    loss = compute_loss(out, y_train)\n",
        "\n",
        "    history['train_loss'].append(loss.item())\n",
        "    history['train_accuracy'].append(compute_accuracy(out, y_train))\n",
        "\n",
        "    if i % eval_every == 0:  # Policz stratę i dokł. na zb. testowym\n",
        "        model.eval()   # Przełącz w tryb ewaluacji\n",
        "        with torch.no_grad():  # Nie licz gradientów\n",
        "            out = model(x_test)\n",
        "\n",
        "            history['test_loss'].append(compute_loss(out, y_test).item())\n",
        "            history['test_accuracy'].append(compute_accuracy(out, y_test))\n",
        "\n",
        "    loss.backward()   # Policz gradienty (faza wstecz)\n",
        "    optimizer.step()  # Aktualizuj parametry modelu\n",
        "    optimizer.zero_grad()  # Zerowanie gradientów przed kolejną iteracją\n",
        "\n",
        "\n",
        "# Wizualizacja historii treningu:\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))  # 1 wiersz, 2 kolumny\n",
        "\n",
        "# Strata\n",
        "ax1.set_ylabel('Strata')\n",
        "ax1.set_xlabel('Epoka')\n",
        "ax1.plot(range(n_iterations), history['train_loss'], color='green')\n",
        "ax1.plot(range(0, n_iterations, eval_every), history['test_loss'], color='orange')\n",
        "ax1.legend(['train', 'test'])\n",
        "\n",
        "ax2.set_ylabel('Dokładność')\n",
        "ax2.set_xlabel('Epoka')\n",
        "ax2.plot(range(n_iterations), history['train_accuracy'], color='green')\n",
        "ax2.plot(range(0, n_iterations, eval_every), history['test_accuracy'], color='orange')\n",
        "ax2.legend(['train', 'test'])\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plot_iris_with_decision_boundary(x_train, y_train, model)"
      ],
      "metadata": {
        "id": "sy4gASl7BvvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zad 4. -- Pełen zbiór Iris\n",
        "\n",
        "Dokonaj klasyfikacji dla pełnego zbioru Iris, tj. 4 cechy wejściowe\n",
        "oraz 3 klasy."
      ],
      "metadata": {
        "id": "tVogZU8WsJ9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO przygotowanie danych\n",
        "# z podziałem na zbiór treningowy oraz testowy\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Załaduj zestaw danych Iris\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Podział na część treningową oraz walidacyjną w proporcji 80 / 20\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "# PyTorch operuje na tensorach, więc dokonujemy konwersji z np.array\n",
        "# na torch.Tensor:\n",
        "x_train = torch.FloatTensor(x_train)\n",
        "y_train = torch.LongTensor(y_train)\n",
        "\n",
        "x_test = torch.FloatTensor(x_test)\n",
        "y_test = torch.LongTensor(y_test)\n",
        "\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "uLUxk1U6qxz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nasz model będzie miał 5 neuronów w warstwie ukrytej. Każdy z 4 wejściami\n",
        "na wartości cech. Trzy wyjścia (po jednym na klasę), których wartości są **\"logitami\"** (ang. logits), tj. $$z_i = Wx + b$$ dla i-tego wyjścia.\n",
        "\n",
        "Wyjście dla którego wartość jest największa wskazuje klasę predykowaną przez sieć. Alternatywnie, na podstawie logitów możemy obliczyć\n",
        "prawdopodobieństwa przynależności do konkretnej klasy $i$ za pomocą\n",
        "funkcji **softmax**:\n",
        "\n",
        "$$P(y = i) = \\frac{e^{z_i}}{\\sum_{j=1}^{C} e^{z_j}}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "aVSCTzvhIqkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hiperparametry modelu:\n",
        "n_inputs = 4\n",
        "n_outputs = 3  # bo 3 klasy\n",
        "n_hidden = 5\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(n_inputs, n_hidden),\n",
        "    # nn.ReLU(),\n",
        "    nn.Linear(n_hidden, n_outputs)\n",
        ")\n",
        "\n",
        "# F. straty -- dla zadania klasyfikacji z l. klas > 2:\n",
        "# CrossEntropyLoss pracuje z logitami\n",
        "compute_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# TODO\n",
        "# Hiperparametry dla treningu:\n",
        "n_iterations = 500\n",
        "eval_every = 1\n",
        "learning_rate = 0.05\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "history = defaultdict(list)\n",
        "\n",
        "# Trening modelu\n",
        "for i in range(n_iterations):\n",
        "    model.train()  # Przełącz w tryb treningu\n",
        "\n",
        "    out = model(x_train)  # Inferencja (faza w przód)\n",
        "    loss = compute_loss(out, y_train)\n",
        "\n",
        "    history['train_loss'].append(loss.item())\n",
        "    # CrossEntropyLoss pracuje z logitami, więc bierzemy argmax do compute_accuracy\n",
        "    with torch.no_grad():\n",
        "        predicted = torch.argmax(out, -1)\n",
        "        accuracy = (predicted == y_train).float().mean().item()\n",
        "        history['train_accuracy'].append(accuracy)\n",
        "\n",
        "\n",
        "    if i % eval_every == 0:  # Policz stratę i dokł. na zb. testowym\n",
        "        model.eval()   # Przełącz w tryb ewaluacji\n",
        "        with torch.no_grad():  # Nie licz gradientów\n",
        "            out = model(x_test)\n",
        "\n",
        "            history['test_loss'].append(compute_loss(out, y_test).item())\n",
        "            # CrossEntropyLoss pracuje z logitami, więc bierzemy argmax do compute_accuracy\n",
        "            predicted = torch.argmax(out, -1)\n",
        "            accuracy = (predicted == y_test).float().mean().item()\n",
        "            history['test_accuracy'].append(accuracy)\n",
        "\n",
        "\n",
        "    loss.backward()   # Policz gradienty (faza wstecz)\n",
        "    optimizer.step()  # Aktualizuj parametry modelu\n",
        "    optimizer.zero_grad()  # Zerowanie gradientów przed kolejną iteracją\n",
        "\n",
        "\n",
        "# Wizualizacja historii treningu:\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))  # 1 wiersz, 2 kolumny\n",
        "\n",
        "# Strata\n",
        "ax1.set_ylabel('Strata')\n",
        "ax1.set_xlabel('Epoka')\n",
        "ax1.plot(range(n_iterations), history['train_loss'], color='green')\n",
        "ax1.plot(range(0, n_iterations, eval_every), history['test_loss'], color='orange')\n",
        "ax1.legend(['train', 'test'])\n",
        "\n",
        "ax2.set_ylabel('Dokładność')\n",
        "ax2.set_xlabel('Epoka')\n",
        "ax2.plot(range(n_iterations), history['train_accuracy'], color='green')\n",
        "ax2.plot(range(0, n_iterations, eval_every), history['test_accuracy'], color='orange')\n",
        "ax2.legend(['train', 'test'])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hTscijiktfLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Porcjowanie danych\n",
        "\n",
        "Jak dotąd stosowaliśmy trening na całym zbiorze treningowym, jednak algorytm SGD (i inne) zakłada, że trening wykonywany jest na **grupach** lub porcjach (ang. batch) przykładów.\n",
        "\n",
        "`PyTorch` udostępnia klasę `DataLoader`, która upraszcza proces iterowania po kolejnych grupach przykładów."
      ],
      "metadata": {
        "id": "fg5wH4TjK67r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dummy_data = torch.tensor(np.arange(10)).unsqueeze(-1)\n",
        "\n",
        "\n",
        "print('Cały zbiór:', dummy_data)\n",
        "\n",
        "print('Iteruję po grupach: ')\n",
        "for batch in DataLoader(dataset=dummy_data, batch_size=2):\n",
        "    print(batch)\n",
        "\n",
        "\n",
        "print('\\nIteruję po grupach (losowa kolejność): ')\n",
        "for batch in DataLoader(dataset=dummy_data, batch_size=2, shuffle=True):\n",
        "    print(batch)\n",
        "\n",
        "print('\\nPonownie iteruję po grupach (losowa kolejność): ')\n",
        "for batch in DataLoader(dataset=dummy_data, batch_size=2, shuffle=True):\n",
        "    print(batch)"
      ],
      "metadata": {
        "id": "_SFlIPmxNqPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nasz kod po uwzględnieniu grupowania będzie wyglądał następująco."
      ],
      "metadata": {
        "id": "-JHEFaUUPeln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Podział na część treningową oraz walidacyjną w proporcji 80 / 20\n",
        "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "\n",
        "x_train = torch.Tensor(x_train)\n",
        "y_train = torch.LongTensor(y_train)  # Tensor z 64-bitowymi intami\n",
        "x_test = torch.Tensor(x_test)\n",
        "y_test = torch.LongTensor(y_test)\n",
        "\n",
        "# PyTorch operuje na tensorach, więc dokonujemy konwersji z np.array\n",
        "# na torch.Tensor:\n",
        "train_loader = DataLoader(list(zip(x_train, y_train)), batch_size=30, shuffle=True)\n",
        "test_loader = DataLoader(list(zip(x_test, y_test)), batch_size=30, shuffle=False)"
      ],
      "metadata": {
        "id": "1QXFVxSrRO2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Możemy zdefiniować pomocnicze funkcje do treningu i rysowania wykresów."
      ],
      "metadata": {
        "id": "x_FkApS7oY8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, test_loader, optimizer, n_epochs=500, eval_every=10):\n",
        "    history = defaultdict(list)\n",
        "    # F. straty -- dla zadania klasyfikacji z l. klas > 2:\n",
        "    compute_loss = nn.CrossEntropyLoss()\n",
        "    # Trening modelu\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()  # Przełącz w tryb treningu\n",
        "\n",
        "        total_loss = 0\n",
        "        total_samples = 0\n",
        "        total_correct = 0\n",
        "        # Epoka to przejście po całym zbiorze treningowym:\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()  # Zerowanie gradientów przed kolejną iteracją\n",
        "            out = model(x_batch)  # Inferencja (faza w przód)\n",
        "\n",
        "            loss = compute_loss(out, y_batch)\n",
        "\n",
        "            loss.backward()   # Policz gradienty (faza wstecz)\n",
        "            optimizer.step()  # Aktualizuj parametry modelu\n",
        "\n",
        "            total_samples += x_batch.shape[0]\n",
        "            total_loss += loss.item() * x_batch.shape[0]\n",
        "\n",
        "            predicted = torch.argmax(out, -1)\n",
        "            total_correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "        history['train_loss'].append(total_loss / total_samples)\n",
        "        history['train_accuracy'].append(total_correct / total_samples)\n",
        "\n",
        "        if epoch % eval_every == 0:  # Policz stratę i dokł. na zb. testowym\n",
        "            model.eval()   # Przełącz w tryb ewaluacji\n",
        "            total_loss = 0\n",
        "            total_samples = 0\n",
        "            total_correct = 0\n",
        "            with torch.no_grad():  # Nie licz gradientów\n",
        "                for x_batch, y_batch in test_loader:\n",
        "                    out = model(x_batch)  # Inferencja (faza w przód)\n",
        "\n",
        "                    total_samples += x_batch.shape[0]\n",
        "                    total_loss += loss.item() * x_batch.shape[0]\n",
        "\n",
        "                    predicted = torch.argmax(out, -1)\n",
        "                    total_correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "            history['test_loss'].append(total_loss / total_samples)\n",
        "            history['test_accuracy'].append(np.mean(total_correct / total_samples))\n",
        "            print(f'Epoch: {epoch}\\tTrain loss: {history[\"train_loss\"][-1]:.3f}'\\\n",
        "                  f'\\tAcc: {history[\"train_accuracy\"][-1]:.2f}'\\\n",
        "                  f'\\tTest loss: {history[\"test_loss\"][-1]:.3f}'\\\n",
        "                  f'\\tTest acc: {history[\"test_accuracy\"][-1]:.2f}')\n",
        "    return history\n",
        "\n",
        "\n",
        "def plot_train_history(history):\n",
        "    # Wizualizacja historii treningu:\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))  # 1 wiersz, 2 kolumny\n",
        "\n",
        "    n_epochs = len(history['train_loss'])\n",
        "    eval_every = n_epochs // len(history['test_loss'])\n",
        "    xs = range(0, n_epochs, eval_every)\n",
        "    # Strata\n",
        "    ax1.set_ylabel('Strata')\n",
        "    ax1.set_xlabel('Epoka')\n",
        "    ax1.plot(history['train_loss'], color='green')\n",
        "    ax1.plot(xs, history['test_loss'], color='orange')\n",
        "    ax1.legend(['train', 'test'])\n",
        "\n",
        "    ax2.set_ylabel('Dokładność')\n",
        "    ax2.set_xlabel('Epoka')\n",
        "    ax2.plot(history['train_accuracy'], color='green')\n",
        "    ax2.plot(xs, history['test_accuracy'], color='orange')\n",
        "    ax2.legend(['train', 'test'])\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "RZ-DOdZnXbbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_inputs = x_train.shape[1]\n",
        "n_outputs = 3  # bo 3 klasy\n",
        "n_hidden = 5\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(n_inputs, n_hidden),\n",
        "    # <- Nasz model jest liniowy -- brak nieliniowej f. aktywacji\n",
        "    nn.Linear(n_hidden, n_outputs)\n",
        ")\n",
        "\n",
        "# Hiperparametry dla treningu:\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "history = train_model(model, train_loader, test_loader, optimizer)\n",
        "plot_train_history(history)"
      ],
      "metadata": {
        "id": "_Wo0uzunYa51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zadanie -- klasyfikacja dla zbioru MNIST-1D\n",
        "\n",
        "Zbiór MNIST-1D jest alternatywą dla zbioru MNIST-2D cyfr pisanych odręcznie. Jest od niego znacznie mniejszy, dane są wektorami jednowymiarowymi, które można interpretować jako pkt. definiujące kształt."
      ],
      "metadata": {
        "id": "t1cckYpXNn3K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjISSgksGvjp"
      },
      "outputs": [],
      "source": [
        "# Instalacja pakietu ze zbiorem\n",
        "!uv pip install mnist1d > /dev/null\n",
        "\n",
        "import mnist1d"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wizualizacja\n",
        "\n",
        "Przykłady w zbiorze MNIST-1D powstały na podstawie jednowymiarowych **szablonów** kształtów cyfr od 0 do 9\n",
        "złożonych z 12 punktów.\n",
        "\n",
        "Szablony można zwizualizować, jak widać poniżej."
      ],
      "metadata": {
        "id": "xg7RwX_sLFfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "templates = mnist1d.data.get_templates()\n",
        "print(\"Templates for the MNIST-1D dataset:\")\n",
        "x = templates['x']\n",
        "t = templates['t']\n",
        "y = templates['y']\n",
        "fig = mnist1d.utils.plot_signals(x, t, labels=y, ratio=1.33, dark_mode=True)"
      ],
      "metadata": {
        "id": "FQAEOIk3JJqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generowanie zbioru danych\n",
        "\n",
        "Dzięki funkcji `make_dataset()` można wygenerować zbiór danych  o różnej liczności. Domyślnie jest to 4000 przykładów dla zb. treningowego oraz 1000 dla testowego."
      ],
      "metadata": {
        "id": "dScXejoqdfXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = mnist1d.data.make_dataset(mnist1d.data.get_dataset_args())\n",
        "x_train, y_train = dataset['x'], dataset['y']\n",
        "x_test, y_test = dataset['x_test'], dataset['y_test']\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "2WdvttLVHaGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist1d.utils.plot_signals(x_train[:10], dataset['t'], y_train[:10],\n",
        "                           zoom=max(dataset['t'])) ;"
      ],
      "metadata": {
        "id": "IZMqIh3xJ38Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Polecenie\n",
        "\n",
        "Na podstawie podanego przykładu wytrenuj sieć typu MLP dla klasyfikacji zbioru MNIST-1D.\n",
        "\n",
        "* Uwzględnij 2 architektury sieci: z **jedną** oraz **dwoma** warstwami ukrytymi\n",
        "  (nie zapomnij o funkcji aktywacji dla warstw ukrytych)\n",
        "* Łączna liczba neuronów ukrytych nie powinna przekraczać 100\n",
        "* Trening powinien trwać 500 epok\n",
        "\n",
        "Odpowiedz na pytania:\n",
        "* Czy uczenie grupami poprawia jakość klasyfikacji w stosunku do przypadku, gdy mamy tylko jedną grupę (cały zbiór)?\n",
        "  * Wypróbuj grupy o rozmiarach 32, 64 oraz 128\n",
        "* Czy wyniki poprawią się, jeżeli zamiast `SGD` użyjemy algorytmu `Adam`?"
      ],
      "metadata": {
        "id": "7RXKBsA8qepr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test = torch.Tensor(dataset['x']), torch.Tensor(dataset['x_test'])\n",
        "y_train, y_test = torch.LongTensor(dataset['y']), torch.LongTensor(dataset['y_test'])"
      ],
      "metadata": {
        "id": "N2g4hV7ra1Gu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}