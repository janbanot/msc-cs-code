{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMI5Mrt5VQ/Gl42z3CrC7NP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janbanot/msc-cs-code/blob/main/sem3/DL/DL_2025_Task3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sprawozdanie 3 - Jan Banot\n",
        "!uv pip install torchinfo kagglehub"
      ],
      "metadata": {
        "id": "g6WzC8o0pKXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQibb9cDIrwZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# 1. Konfiguracja urządzenia i parametrów\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 64\n",
        "IMG_SIZE = 28\n",
        "\n",
        "print(f\"Praca na urządzeniu: {DEVICE}\")\n",
        "\n",
        "# 2. Pobieranie danych z Kaggle (CelebA-HQ Resized)\n",
        "try:\n",
        "    import kagglehub\n",
        "    path = kagglehub.dataset_download(\"badasstechie/celebahq-resized-256x256\")\n",
        "    # Zbiór zazwyczaj znajduje się w podfolderze 'celeba_hq_256'\n",
        "    data_dir = os.path.join(path, \"celeba_hq_256\")\n",
        "    if not os.path.exists(data_dir):\n",
        "        data_dir = path\n",
        "    print(f\"Dane pobrane do: {data_dir}\")\n",
        "except Exception as e:\n",
        "    print(f\"Błąd pobierania: {e}. Upewnij się, że masz zainstalowane kagglehub: !pip install kagglehub\")\n",
        "\n",
        "# 3. Definicja transformacji (Kluczowe dla zadania)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1), # Konwersja na skalę szarości\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),      # Zmniejszenie rozdzielczości do 28x28\n",
        "    transforms.ToTensor(),                       # Zakres [0, 1]\n",
        "])\n",
        "\n",
        "class CelebADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        # Tworzymy listę wszystkich plików .jpg w folderze\n",
        "        self.image_names = [f for f in os.listdir(root_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "        if len(self.image_names) == 0:\n",
        "            raise RuntimeError(f\"Nie znaleziono zdjęć w: {root_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.image_names[idx])\n",
        "        # Otwieramy obraz i konwertujemy na RGB\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, 0\n",
        "\n",
        "# 4. Ładowanie zbioru danych\n",
        "dataset = CelebADataset(root_dir=data_dir, transform=transform)\n",
        "\n",
        "# Podział na trening i test\n",
        "train_size = int(0.9 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# 5. Wizualizacja przygotowanych danych\n",
        "def imshow(img):\n",
        "    img = img.numpy().transpose((1, 2, 0))\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Podgląd przetworzonych danych (CelebA 28x28 Grayscale)\")\n",
        "    plt.show()\n",
        "\n",
        "dataiter = iter(train_loader)\n",
        "images, _ = next(dataiter)\n",
        "imshow(torchvision.utils.make_grid(images[:16]))\n",
        "\n",
        "print(f\"Liczba obrazów treningowych: {len(train_dataset)}\")\n",
        "print(f\"Kształt pojedynczego obrazu: {images[0].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchinfo import summary\n",
        "\n",
        "# --- 1. Model VAE ---\n",
        "class CelebVAE(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super(CelebVAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Koder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 4, stride=2, padding=1),  # -> (64, 14, 14)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1), # -> (128, 7, 7)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.fc_mu = nn.Linear(128 * 7 * 7, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(128 * 7 * 7, latent_dim)\n",
        "\n",
        "        # Dekoder\n",
        "        self.decoder_input = nn.Linear(latent_dim, 128 * 7 * 7)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1), # -> (64, 14, 14)\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.ConvTranspose2d(64, 1, 4, stride=2, padding=1),  # -> (1, 28, 28)\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        return self.fc_mu(h), self.fc_logvar(h)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = self.decoder_input(z).view(-1, 128, 7, 7)\n",
        "        return self.decoder(h)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "# --- 2. Dyskryminator (dla VAE-GAN) ---\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 4, 2, 1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(32, 64, 4, 2, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 7 * 7, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "# --- 3. Wspólna architektura U-Net dla FM i DDPM ---\n",
        "class TimeEmbedding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(1, dim),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(dim, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, t):\n",
        "        return self.mlp(t)\n",
        "\n",
        "class UNetBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_dim, up=False):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Linear(time_dim, out_ch)\n",
        "        if up:\n",
        "            self.conv = nn.ConvTranspose2d(in_ch, out_ch, 4, 2, 1)\n",
        "        else:\n",
        "            self.conv = nn.Conv2d(in_ch, out_ch, 4, 2, 1)\n",
        "        self.bn = nn.BatchNorm2d(out_ch)\n",
        "        self.relu = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        h = self.conv(x)\n",
        "        h = h + self.time_mlp(t).unsqueeze(-1).unsqueeze(-1)\n",
        "        return self.relu(self.bn(h))\n",
        "\n",
        "class SimpleUNet(nn.Module):\n",
        "    def __init__(self, time_dim=64):\n",
        "        super().__init__()\n",
        "        self.time_embed = TimeEmbedding(time_dim)\n",
        "\n",
        "        # Downsampling\n",
        "        self.down1 = UNetBlock(1, 64, time_dim)    # 28 -> 14\n",
        "        self.down2 = UNetBlock(64, 128, time_dim) # 14 -> 7\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Upsampling\n",
        "        self.up1 = UNetBlock(128, 64, time_dim, up=True) # 7 -> 14\n",
        "        self.up2 = UNetBlock(128, 1, time_dim, up=True)  # 14 -> 28\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        t = self.time_embed(t)\n",
        "        d1 = self.down1(x, t)\n",
        "        d2 = self.down2(d1, t)\n",
        "        b = self.bottleneck(d2)\n",
        "        u1 = self.up1(b, t)\n",
        "        u2 = self.up2(torch.cat([u1, d1], dim=1), t)\n",
        "        return u2\n",
        "\n",
        "# --- 4. Inicjalizacja modeli i sprawdzenie poprawności ---\n",
        "vae_model = CelebVAE().to(DEVICE)\n",
        "unet_model = SimpleUNet().to(DEVICE)\n",
        "disc_model = Discriminator().to(DEVICE)\n",
        "\n",
        "print(\"Podsumowanie VAE:\")\n",
        "summary(vae_model, input_size=(1, 1, 28, 28))\n",
        "print(\"\\nPodsumowanie U-Net (FM/DDPM):\")\n",
        "summary(unet_model, input_data=[torch.randn(1, 1, 28, 28).to(DEVICE), torch.randn(1, 1).to(DEVICE)])"
      ],
      "metadata": {
        "id": "HM9W0AABLw-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parametry wspólne\n",
        "EPOCHS = 10\n",
        "LR = 1e-3\n",
        "\n",
        "# Konfiguracja DDPM\n",
        "T = 500 # Liczba kroków czasowych\n",
        "betas = torch.linspace(1e-4, 0.02, T).to(DEVICE)\n",
        "alphas = 1. - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "def get_loss_vae(model, x, beta=1.0):\n",
        "    recon_x, mu, logvar = model(x)\n",
        "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return (BCE + beta * KLD) / x.size(0)\n",
        "\n",
        "def get_loss_fm(model, x1):\n",
        "    x0 = torch.randn_like(x1)\n",
        "    t = torch.rand(x1.size(0), 1).to(DEVICE)\n",
        "    # Ścieżka liniowa: x_t = (1-t)*x0 + t*x1\n",
        "    xt = (1 - t.view(-1, 1, 1, 1)) * x0 + t.view(-1, 1, 1, 1) * x1\n",
        "    v_pred = model(xt, t)\n",
        "    target_v = x1 - x0\n",
        "    return F.mse_loss(v_pred, target_v)\n",
        "\n",
        "def get_loss_ddpm(model, x0):\n",
        "    t = torch.randint(0, T, (x0.size(0),)).to(DEVICE)\n",
        "    noise = torch.randn_like(x0)\n",
        "    alpha_t = alphas_cumprod[t].view(-1, 1, 1, 1)\n",
        "    # Zaszumianie: x_t = sqrt(alpha_bar)*x0 + sqrt(1-alpha_bar)*noise\n",
        "    xt = torch.sqrt(alpha_t) * x0 + torch.sqrt(1 - alpha_t) * noise\n",
        "    epsilon_pred = model(xt, t.float().view(-1, 1) / T)\n",
        "    return F.mse_loss(epsilon_pred, noise)"
      ],
      "metadata": {
        "id": "KIW-AQ1Eq7W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "history = {'vae': [], 'fm': [], 'ddpm': []}\n",
        "\n",
        "# Optymalizatory\n",
        "opt_vae = torch.optim.AdamW(vae_model.parameters(), lr=LR)\n",
        "opt_fm = torch.optim.AdamW(unet_model.parameters(), lr=LR) # Wspólny UNet\n",
        "# Dla czystego porównania dwa osobne UNety, trenujemy je jeden po drugim.\n",
        "unet_fm = SimpleUNet().to(DEVICE)\n",
        "unet_ddpm = SimpleUNet().to(DEVICE)\n",
        "opt_fm = torch.optim.AdamW(unet_fm.parameters(), lr=LR)\n",
        "opt_ddpm = torch.optim.AdamW(unet_ddpm.parameters(), lr=LR)\n",
        "\n",
        "print(\"Rozpoczynam trening modeli (10 epok każdy)...\")\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    l_vae, l_fm, l_ddpm = 0, 0, 0\n",
        "\n",
        "    for batch, _ in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
        "        batch = batch.to(DEVICE)\n",
        "\n",
        "        # --- Trening VAE ---\n",
        "        opt_vae.zero_grad()\n",
        "        loss_v = get_loss_vae(vae_model, batch)\n",
        "        loss_v.backward()\n",
        "        opt_vae.step()\n",
        "        l_vae += loss_v.item()\n",
        "\n",
        "        # --- Trening FM ---\n",
        "        opt_fm.zero_grad()\n",
        "        loss_f = get_loss_fm(unet_fm, batch)\n",
        "        loss_f.backward()\n",
        "        opt_fm.step()\n",
        "        l_fm += loss_f.item()\n",
        "\n",
        "        # --- Trening DDPM ---\n",
        "        opt_ddpm.zero_grad()\n",
        "        loss_d = get_loss_ddpm(unet_ddpm, batch)\n",
        "        loss_d.backward()\n",
        "        opt_ddpm.step()\n",
        "        l_ddpm += loss_d.item()\n",
        "\n",
        "    # Logowanie średniej straty\n",
        "    history['vae'].append(l_vae / len(train_loader))\n",
        "    history['fm'].append(l_fm / len(train_loader))\n",
        "    history['ddpm'].append(l_ddpm / len(train_loader))\n",
        "\n",
        "    print(f\"[{epoch}/{EPOCHS}] VAE: {history['vae'][-1]:.4f} | FM: {history['fm'][-1]:.4f} | DDPM: {history['ddpm'][-1]:.4f}\")"
      ],
      "metadata": {
        "id": "qPnlFNO1q9C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def sample_vae(model, n=16):\n",
        "    z = torch.randn(n, model.latent_dim).to(DEVICE)\n",
        "    return model.decode(z).cpu()\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_fm(model, n=16, steps=50):\n",
        "    x = torch.randn(n, 1, 28, 28).to(DEVICE)\n",
        "    dt = 1.0 / steps\n",
        "    for i in range(steps):\n",
        "        t = torch.ones(n, 1).to(DEVICE) * (i / steps)\n",
        "        v = model(x, t)\n",
        "        x = x + v * dt\n",
        "    return x.cpu()\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_ddpm(model, n=16):\n",
        "    x = torch.randn(n, 1, 28, 28).to(DEVICE)\n",
        "    for i in reversed(range(T)):\n",
        "        t = (torch.ones(n, 1) * i / T).to(DEVICE)\n",
        "        epsilon_pred = model(x, t)\n",
        "\n",
        "        alpha = alphas[i]\n",
        "        alpha_bar = alphas_cumprod[i]\n",
        "        beta = betas[i]\n",
        "\n",
        "        if i > 0:\n",
        "            noise = torch.randn_like(x)\n",
        "        else:\n",
        "            noise = 0\n",
        "\n",
        "        # Formuła odszumiania DDPM\n",
        "        x = (1 / torch.sqrt(alpha)) * (x - ((1 - alpha) / torch.sqrt(1 - alpha_bar)) * epsilon_pred) + torch.sqrt(beta) * noise\n",
        "    return x.cpu()"
      ],
      "metadata": {
        "id": "knDsk1lDwVYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_results(history, vae_imgs, fm_imgs, ddpm_imgs):\n",
        "    # --- Wykresy Funkcji Straty ---\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    epochs_range = range(1, len(history['vae']) + 1)\n",
        "\n",
        "    ax[0].plot(epochs_range, history['vae'], color='blue', marker='o')\n",
        "    ax[0].set_title(\"VAE Loss (BCE + KLD)\")\n",
        "    ax[0].set_xlabel(\"Epoch\")\n",
        "    ax[0].grid(True)\n",
        "\n",
        "    ax[1].plot(epochs_range, history['fm'], color='green', marker='o')\n",
        "    ax[1].set_title(\"Flow Matching Loss (MSE)\")\n",
        "    ax[1].set_xlabel(\"Epoch\")\n",
        "    ax[1].grid(True)\n",
        "\n",
        "    ax[2].plot(epochs_range, history['ddpm'], color='red', marker='o')\n",
        "    ax[2].set_title(\"DDPM Loss (MSE)\")\n",
        "    ax[2].set_xlabel(\"Epoch\")\n",
        "    ax[2].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- Porównanie Generowanych Obrazów ---\n",
        "    def show_row(imgs, title, row_idx, n=8):\n",
        "        for i in range(n):\n",
        "            plt.subplot(3, n, row_idx * n + i + 1)\n",
        "            plt.imshow(imgs[i].squeeze(), cmap='gray')\n",
        "            plt.axis('off')\n",
        "            if i == 0: plt.title(title, loc='left', pad=10)\n",
        "\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    show_row(vae_imgs, \"VAE (Blurry)\", 0)\n",
        "    show_row(fm_imgs, \"Flow Matching\", 1)\n",
        "    show_row(ddpm_imgs, \"DDPM (Detailed)\", 2)\n",
        "    plt.suptitle(\"Porównanie Modelu: VAE vs FM vs DDPM (CelebA 28x28)\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Próbkowanie modeli\n",
        "print(\"Generuję próbki...\")\n",
        "vae_samples = sample_vae(vae_model, n=16)\n",
        "fm_samples = sample_fm(unet_fm, n=16)\n",
        "ddpm_samples = sample_ddpm(unet_ddpm, n=16)\n",
        "\n",
        "# Wyświetlanie\n",
        "plot_results(history, vae_samples, fm_samples, ddpm_samples)"
      ],
      "metadata": {
        "id": "lYV4tw2ywsrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- KROK: Dłuższy trening (50 epok) i weryfikacja ---\n",
        "\n",
        "NEW_EPOCHS = 50\n",
        "vae_model = CelebVAE().to(DEVICE); unet_fm = SimpleUNet().to(DEVICE); unet_ddpm = SimpleUNet().to(DEVICE)\n",
        "opt_vae = torch.optim.AdamW(vae_model.parameters(), lr=LR); opt_fm = torch.optim.AdamW(unet_fm.parameters(), lr=LR); opt_ddpm = torch.optim.AdamW(unet_ddpm.parameters(), lr=LR)\n",
        "history = {'vae': [], 'fm': [], 'ddpm': []}\n",
        "\n",
        "print(f\"Rozpoczynam trening do {NEW_EPOCHS} epok...\")\n",
        "\n",
        "# Kontynuujemy od aktualnego stanu history\n",
        "current_epoch = len(history['vae'])\n",
        "\n",
        "for epoch in range(current_epoch + 1, NEW_EPOCHS + 1):\n",
        "    l_vae, l_fm, l_ddpm = 0, 0, 0\n",
        "\n",
        "    for batch, _ in tqdm(train_loader, desc=f\"Epoch {epoch}/{NEW_EPOCHS}\"):\n",
        "        batch = batch.to(DEVICE)\n",
        "\n",
        "        # 1. VAE\n",
        "        opt_vae.zero_grad()\n",
        "        loss_v = get_loss_vae(vae_model, batch)\n",
        "        loss_v.backward()\n",
        "        opt_vae.step()\n",
        "        l_vae += loss_v.item()\n",
        "\n",
        "        # 2. Flow Matching\n",
        "        opt_fm.zero_grad()\n",
        "        loss_f = get_loss_fm(unet_fm, batch)\n",
        "        loss_f.backward()\n",
        "        opt_fm.step()\n",
        "        l_fm += loss_f.item()\n",
        "\n",
        "        # 3. DDPM\n",
        "        opt_ddpm.zero_grad()\n",
        "        loss_d = get_loss_ddpm(unet_ddpm, batch)\n",
        "        loss_d.backward()\n",
        "        opt_ddpm.step()\n",
        "        l_ddpm += loss_d.item()\n",
        "\n",
        "    # Zapisywanie historii\n",
        "    history['vae'].append(l_vae / len(train_loader))\n",
        "    history['fm'].append(l_fm / len(train_loader))\n",
        "    history['ddpm'].append(l_ddpm / len(train_loader))\n",
        "\n",
        "    # Co 5 epok generujemy podgląd, żeby widzieć postęp\n",
        "    if epoch % 10 == 0 or epoch == NEW_EPOCHS:\n",
        "        print(f\"Podgląd po {epoch} epokach:\")\n",
        "        with torch.no_grad():\n",
        "            v_s = sample_vae(vae_model, n=4)\n",
        "            f_s = sample_fm(unet_fm, n=4)\n",
        "            d_s = sample_ddpm(unet_ddpm, n=4)\n",
        "\n",
        "            # Szybki podgląd w konsoli\n",
        "            combined = torch.cat([v_s, f_s, d_s], dim=0)\n",
        "            grid = torchvision.utils.make_grid(combined, nrow=4)\n",
        "            plt.imshow(grid.permute(1, 2, 0).cpu(), cmap='gray')\n",
        "            plt.title(f\"Progress at epoch {epoch}\")\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "# --- FINALNA WERYFIKACJA ---\n",
        "print(\"Trening zakończony. Generuję ostateczne zestawienie...\")\n",
        "final_vae = sample_vae(vae_model, n=16)\n",
        "final_fm = sample_fm(unet_fm, n=16, steps=100) # Zwiększamy kroki dla lepszej jakości\n",
        "final_ddpm = sample_ddpm(unet_ddpm, n=16)\n",
        "\n",
        "plot_results(history, final_vae, final_fm, final_ddpm)"
      ],
      "metadata": {
        "id": "RIkeT8Fbx2_F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}