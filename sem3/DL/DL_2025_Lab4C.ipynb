{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janbanot/msc-cs-code/blob/main/sem3/DL/DL_2025_Lab4C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8h0zX8CHIuh"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo\n",
        "\n",
        "# Import potrzebnych modułów i funkcji\n",
        "\n",
        "# Importy z biblioteki standardowej\n",
        "from collections import defaultdict\n",
        "from random import random\n",
        "\n",
        "# Importy z bibliotek zewnętrznych\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.functional as F\n",
        "from torch import nn, tensor\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset\n",
        "import random\n",
        "from torchvision import datasets, transforms, models\n",
        "from PIL import Image\n",
        "\n",
        "# Importy do wizualizacji\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Dodatkowe importy\n",
        "from torchinfo import summary\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,\n",
        "    ConfusionMatrixDisplay, classification_report, roc_auc_score\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "html"
        },
        "id": "s1o3gwrJHIui"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<!-- Potrzebne dla poprawnego wyświetlania tqdm w VSCode https://stackoverflow.com/a/77566731 -->\n",
        "<style>\n",
        ".cell-output-ipywidget-background {\n",
        "    background-color: transparent !important;\n",
        "}\n",
        ":root {\n",
        "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
        "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
        "}\n",
        "</style>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyY5f76eHIuj"
      },
      "source": [
        "# Uczenie transferowe na przykładzie sieci konwolucyjnych"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWxLaXhtHIuj"
      },
      "outputs": [],
      "source": [
        "# Obliczenia wykonamy na GPU, jeżeli jest dostępne, a na CPU w przeciwny razie\n",
        "def get_device():\n",
        "  return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "get_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pg3ha3ktHIuk"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, test_loader,\n",
        "                optimizer,\n",
        "                n_epochs=20, eval_every=1,\n",
        "                device=None,\n",
        "                history=None):\n",
        "    device = device or get_device()\n",
        "\n",
        "    history = history or defaultdict(list)\n",
        "    # Przenieś model na określone urządzenie\n",
        "    model.to(device)\n",
        "\n",
        "    # Definiowanie funkcji straty\n",
        "    compute_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    test_loss = history['test_loss'][-1] if history['test_loss'] else 0\n",
        "    test_accuracy = history['test_accuracy'][-1] if history['test_accuracy'] else 0\n",
        "\n",
        "    # Pętla treningowa\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()  # Ustaw model w tryb treningowy\n",
        "\n",
        "        total_loss = 0\n",
        "        total_samples = 0\n",
        "        total_correct = 0\n",
        "\n",
        "        # Iteracja po danych treningowych\n",
        "        for x_batch, y_batch in tqdm(train_loader):\n",
        "            # Przenieś dane na określone urządzenie\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Wyzerowanie gradientów przed kolejną iteracją\n",
        "            out = model(x_batch)  # Faza w przód\n",
        "\n",
        "            loss = compute_loss(out, y_batch)\n",
        "\n",
        "            loss.backward()   # Faza wsteczna do obliczenia gradientów\n",
        "            optimizer.step()  # Aktualizacja parametrów modelu\n",
        "\n",
        "            total_samples += x_batch.shape[0]\n",
        "            total_loss += loss.item() * x_batch.shape[0]\n",
        "\n",
        "            predicted = torch.argmax(out, -1)\n",
        "            total_correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "        train_loss = total_loss / total_samples\n",
        "        train_accuracy = total_correct / total_samples\n",
        "\n",
        "        if (epoch + 1) % eval_every == 0:  # Ewaluacja na zbiorze testowym\n",
        "            model.eval()   # Ustaw model w tryb ewaluacji\n",
        "            total_loss = 0\n",
        "            total_samples = 0\n",
        "            total_correct = 0\n",
        "            with torch.no_grad():  # Wyłączenie obliczania gradientów\n",
        "                for x_batch, y_batch in tqdm(test_loader):\n",
        "                    # Przenieś dane na określone urządzenie\n",
        "                    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "                    out = model(x_batch)  # Faza w przód\n",
        "\n",
        "                    loss = compute_loss(out, y_batch)\n",
        "\n",
        "                    total_samples += x_batch.shape[0]\n",
        "                    total_loss += loss.item() * x_batch.shape[0]\n",
        "\n",
        "                    predicted = torch.argmax(out, -1)\n",
        "                    total_correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "            test_loss = total_loss / total_samples\n",
        "            test_accuracy = total_correct / total_samples\n",
        "\n",
        "            print(f'Epoch: {epoch}\\tTrain loss: {train_loss:.3f}'\\\n",
        "                  f'\\tTrain acc: {train_accuracy:.3f}'\\\n",
        "                  f'\\tTest loss: {test_loss:.3f}'\\\n",
        "                  f'\\tTest acc: {test_accuracy:.3f}')\n",
        "        else:\n",
        "            print(f'Epoch: {epoch}\\tTrain loss: {train_loss:.3f}'\\\n",
        "                  f'\\tTrain acc: {train_accuracy:.3f}')\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_accuracy'].append(train_accuracy)\n",
        "\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['test_accuracy'].append(test_accuracy)\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "def plot_train_history(history):\n",
        "    # Wizualizacja historii treningu:\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))  # 1 wiersz, 2 kolumny\n",
        "\n",
        "    n_epochs = len(history['train_loss'])\n",
        "    eval_every = n_epochs // len(history['test_loss'])\n",
        "    xs = range(0, n_epochs, eval_every)\n",
        "    # Strata\n",
        "    ax1.set_ylabel('Strata')\n",
        "    ax1.set_xlabel('Epoka')\n",
        "    ax1.plot(history['train_loss'], color='green')\n",
        "    ax1.plot(history['test_loss'], color='orange')\n",
        "    ax1.legend(['train', 'test'])\n",
        "\n",
        "    ax2.set_ylabel('Dokładność')\n",
        "    ax2.set_xlabel('Epoka')\n",
        "    ax2.plot(history['train_accuracy'], color='green')\n",
        "    ax2.plot(history['test_accuracy'], color='orange')\n",
        "    ax2.legend(['train', 'test'])\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpwn0TXmHIuk"
      },
      "source": [
        "## Wybór modelu\n",
        "\n",
        "`TorchVision` oferuje bogatą [listę modeli](https://pytorch.org/vision/main/models.html).\n",
        "\n",
        "Wybieramy [ResNet18](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html), który jest względnie prosty, ale powinien być szybki."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9oIC1qTHIul"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "torch.manual_seed(42)  # Reset ziarna generatora, dla powtarzalności wyników\n",
        "\n",
        "# Ładujemy model ResNet-18 wraz z wytrenowanymi na ImageNet wagami\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# \"Zamrażamy\" parametry sieci\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Podmieniamy warstwę wyjściową\n",
        "num_features = model.fc.in_features\n",
        "# model.fc = nn.Linear(num_features, 10)  # 10 bo liczba klas dla CIFAR-10\n",
        "\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 50),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.1),\n",
        "    nn.Linear(50, 10)\n",
        ")\n",
        "\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2A4SkKIHIul"
      },
      "outputs": [],
      "source": [
        "summary(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdScxsZfHIul"
      },
      "source": [
        "Każdy model wymaga danych o określonym wymiarze.\n",
        "Gotowe *transformacje* można znaleźć w zestawie z wagami.\n",
        "Na ich podstawie będzie można dopasować obrazy 32x32 ze zbioru CIFAR-10\n",
        "do obrazów 224x224 wymaganych przez ResNet-18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FspMYy92HIul"
      },
      "outputs": [],
      "source": [
        "transform = models.ResNet18_Weights.IMAGENET1K_V1.transforms()\n",
        "transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF_6YFi7HIul"
      },
      "source": [
        "Alternatywnie, możemy samodzielnie zdefniować transformacje:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GPvzl-YHIul"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    # Resize the shorter side of the image to 256 pixels using bilinear interpolation\n",
        "    transforms.Resize(256, interpolation=InterpolationMode.BILINEAR),\n",
        "    # Center crop the image to 224x224 pixels\n",
        "    transforms.CenterCrop(224),\n",
        "    # Convert the image to a PyTorch tensor\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # transforms.RandomHorizontalFlip(),  # <- dodatkowo\n",
        "    # Normalize the tensor with the specified mean and standard deviation\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mhxQiKvHIum"
      },
      "outputs": [],
      "source": [
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True,\n",
        "                                 transform=transform)\n",
        "\n",
        "# W celu skrócenia czasu obliczeń ograniczymy się do 5000 losowo wybranych\n",
        "# obrazów (z 50 000)\n",
        "train_subset = Subset(train_dataset,\n",
        "                      indices=random.sample(range(len(train_dataset)), 5_000))\n",
        "\n",
        "# Cały zbiór testowy, tj. 10 000 obrazów\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True,\n",
        "                                transform=transform)\n",
        "test_subset = Subset(test_dataset,\n",
        "                     indices=random.sample(range(len(test_dataset)), 5_000))\n",
        "\n",
        "train_loader = DataLoader(dataset=train_subset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEWjiWAeHIum"
      },
      "outputs": [],
      "source": [
        "history = train_model(model, train_loader, test_loader,\n",
        "            optimizer=optim.AdamW(model.parameters(), lr=learning_rate),\n",
        "            n_epochs=5,\n",
        "            eval_every=5)\n",
        "\n",
        "plot_train_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H28P3NJYHIum"
      },
      "source": [
        "## Faza II -- dostrajanie\n",
        "\n",
        "Po zakończeniu podstawowego treningu (nowej) warstwy wyjściowej, możemy\n",
        "odmrozić wszystkie warstwy w celu ,,delikatnego'' dostrojenia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfItm4gHHIum"
      },
      "outputs": [],
      "source": [
        "# \"Odmrażamy\" parametry sieci\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True  # Parametr będzie ulegał korekcji w trakcie treningu\n",
        "\n",
        "history = train_model(model, train_loader, test_loader,\n",
        "            optimizer=optim.AdamW(model.parameters(), lr=learning_rate / 10),\n",
        "            n_epochs=2,\n",
        "            history=history,\n",
        "            eval_every=1)\n",
        "\n",
        "plot_train_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDDPdAgSHIum"
      },
      "source": [
        "## A teraz, czy to zadziała dla zdjęcia spoza zbioru testowego?\n",
        "\n",
        "Możemy sprawdzić, czy model poprawnie rozpozna zdjęcie, np. pobrane z serwisu [unsplash](https://images.unsplash.com/photo-1511194872177-813cf1cd32bb?q=80&w=2670&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgmTn1F6HIum"
      },
      "outputs": [],
      "source": [
        "!wget -O horse.jpeg  \"https://images.unsplash.com/photo-1511194872177-813cf1cd32bb?q=80&w=2670&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBakjasZHIum"
      },
      "outputs": [],
      "source": [
        "image_path = 'horse.jpeg'\n",
        "raw_image = big_image = Image.open(image_path)\n",
        "display(raw_image);\n",
        "\n",
        "# Uwaga, zbiór CIFAR-10 ma zdjęcia w rozdzielczości 32x32 -- dopasowanie do tego wymiaru\n",
        "# może poprawić(!) dokładność klasyfikacji, pomimo odrzucenia większości informacji\n",
        "raw_image = raw_image.resize((32, 32))\n",
        "display(raw_image);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baiUvmR1HIun"
      },
      "outputs": [],
      "source": [
        "# Obraz należy dopasować do formy wymaganej przez sieć:\n",
        "transform = models.ResNet18_Weights.IMAGENET1K_V1.transforms()\n",
        "image = transform(raw_image)\n",
        "\n",
        "# Jak wygląda obraz na wejściu sieci?\n",
        "image_to_show = image.permute(1, 2, 0).clamp(0, 1)\n",
        "\n",
        "plt.imshow(image_to_show)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXn4gt7BHIun"
      },
      "outputs": [],
      "source": [
        "# Dodajemy wymiar, bo sieć potrzbuje [Batch, Channel, Width, Height],\n",
        "# a nie tylko [Channel, Width, Height]\n",
        "image_to_input = image.unsqueeze(0).to(get_device())\n",
        "\n",
        "model.eval()  # Tryb ewaluacji\n",
        "with torch.no_grad():\n",
        "    outputs = model(image_to_input)\n",
        "    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "    predicted_class = probabilities.argmax(dim=1).item()\n",
        "\n",
        "print(f\"Predicted class: {test_dataset.classes[predicted_class]} with prob. {probabilities[0][predicted_class].item():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hda2EpmyHIun"
      },
      "source": [
        "# Przygotowanie zdjęć do treningu\n",
        "\n",
        "Korzystanie z **gotowych** zestawów zdjęć z `torchvision.datasets` jest wygodne,\n",
        "ale niewiele trudniejesze jest przetwarzanie własnych zestawów zdjęć.\n",
        "\n",
        "Ułatwia to, np. klasa `torchvision.datasets.ImageFolder`\n",
        "\n",
        "Możemy spróbować, np. ze zbiorem zdjęć sinic z dostępnego na github [repozytorium](https://github.com/iman2693/CTCB)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/iman2693/CTCB/archive/refs/heads/main.zip -O CTCB-main.zip\n",
        "!unzip CTCB-main.zip"
      ],
      "metadata": {
        "id": "_oeeVX0nJDM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHYVWFqJHIun"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224, antialias=True),  # Resize to the input size expected by ResNet\n",
        "    transforms.CenterCrop(222),  # Crop the center to create a square\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization as per ImageNet stats\n",
        "])\n",
        "\n",
        "train_imgs = ImageFolder(root=pathlib.Path('CTCB-main/dataset/Train'), transform=transform)\n",
        "test_imgs = ImageFolder(root=pathlib.Path('CTCB-main/dataset/Test'), transform=transform)\n",
        "\n",
        "f'{train_imgs.classes=} {len(train_imgs)=}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7QpiXW6HIun"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_imgs, batch_size=16, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WARuoqKHIun"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)  # Reset ziarna generatora, dla powtarzalności wyników\n",
        "\n",
        "# Ładujemy model ResNet-18 wraz z wytrenowanymi na ImageNet wagami\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# \"Zamrażamy\" parametry sieci\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Podmieniamy warstwę wyjściową\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, len(train_imgs.classes))\n",
        "\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVfAUAxUHIun"
      },
      "outputs": [],
      "source": [
        "history = train_model(model, train_loader, test_loader,\n",
        "            optimizer=optim.AdamW(model.parameters(), lr=learning_rate),\n",
        "            n_epochs=5,\n",
        "            eval_every=5)\n",
        "\n",
        "plot_train_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAfkaVkrHIun"
      },
      "outputs": [],
      "source": [
        "# \"Odmrażamy\" parametry sieci\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True  # Parametr będzie ulegał korekcji w trakcie treningu\n",
        "\n",
        "history = train_model(model, train_loader, test_loader,\n",
        "            optimizer=optim.AdamW(model.parameters(), lr=learning_rate / 10),\n",
        "            n_epochs=2,\n",
        "            history=history,\n",
        "            eval_every=1)\n",
        "\n",
        "plot_train_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVa-wLWEHIuo"
      },
      "outputs": [],
      "source": [
        "def show_classification_metrics(model, data_loader, device=None, class_names=None):\n",
        "    if device is None:\n",
        "        device = get_device()\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            # Move data to the specified device\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            # Prediction\n",
        "            out = model(x_batch)\n",
        "            preds = torch.argmax(out, -1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_preds = np.array(all_preds)\n",
        "\n",
        "    # Determine the unique labels\n",
        "    if class_names is not None:\n",
        "        labels = np.arange(len(class_names))\n",
        "    else:\n",
        "        labels = np.unique(np.concatenate((all_labels, all_preds)))\n",
        "\n",
        "    # Compute the confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds, labels=labels)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    num_errors = len(all_labels) - np.sum(all_preds == all_labels)\n",
        "\n",
        "    # Display the confusion matrix\n",
        "    if class_names is not None:\n",
        "        if len(labels) != len(class_names):\n",
        "            raise ValueError(\"Number of class names must match number of labels.\")\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "    else:\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    # Rotate the x-axis labels by 90 degrees\n",
        "    plt.setp(disp.ax_.get_xticklabels(), rotation=90)\n",
        "    plt.show()\n",
        "\n",
        "    # Display additional information\n",
        "    print(f'Number of errors: {num_errors}')\n",
        "    print(f'Accuracy: {accuracy:.3f}')\n",
        "    print(f'Precision: {precision:.3f}')\n",
        "    print(f'Recall: {recall:.3f}')\n",
        "    print(f'F1-score: {f1:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFKy5PAiHIuo"
      },
      "outputs": [],
      "source": [
        "show_classification_metrics(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pcl0Ls7JHIuo"
      },
      "source": [
        "# Zadanie 3 (z sieci konwolucyjnych)\n",
        "\n",
        "Proszę zastosować *uczenie transferowe* do rozwiązania problemu klasyfikacji zdjęć przedstawiających\n",
        "cyfry w angielskim języku migowym https://github.com/ardamavi/Sign-Language-Digits-Dataset\n",
        "\n",
        "* Proszę wybrać i porównać ze sobą *dwa* modele z [listy PyTorch](https://pytorch.org/vision/main/models.html) nie większe niż 20 mln parametrów\n",
        "* Czy użycie dodatkowych transformacji na wejściu sieci, np. odbicie w poziomie, wymazywanie, jest w stanie podnieść dokładność modelu?\n",
        "* Czy dodanie 2 warstw na wyjściu zamiast 1 poprawi jakość klasyfikacji?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDMtC_q3HIuo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}