{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janbanot/msc-cs-code/blob/main/sem3/DL/DL_2025_Lab6-b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivc2pp_NTQ5b"
      },
      "source": [
        "# Uczenie nienadzorowane -- wybrane metody"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ1tEIKWTQ5e"
      },
      "source": [
        "## Sieci typu koder--dekoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIEKIoArTQ5e"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<!-- Potrzebne dla poprawnego wyświetlania paska postępu tqdm w VSCode https://stackoverflow.com/a/77566731 -->\n",
        "<style>\n",
        ".cell-output-ipywidget-background {\n",
        "    background-color: transparent !important;\n",
        "}\n",
        ":root {\n",
        "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
        "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
        "}\n",
        "</style>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXZJ8eZdTQ5g"
      },
      "outputs": [],
      "source": [
        "!pip -q install torchinfo\n",
        "\n",
        "# Import potrzebnych modułów i funkcji\n",
        "\n",
        "from collections import defaultdict\n",
        "from random import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.functional as F\n",
        "from torch import nn, tensor\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import Subset\n",
        "import random\n",
        "from torchvision import datasets, transforms, models\n",
        "from PIL import Image\n",
        "\n",
        "# Importy do wizualizacji\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY6u98DxTQ5h"
      },
      "outputs": [],
      "source": [
        "def get_device():   # Obliczenia wykonamy na GPU, jeżeli jest dostępne, a na CPU w przeciwny razie\n",
        "  return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def train_unsupervised(model, train_loader, test_loader,\n",
        "                optimizer,\n",
        "                n_epochs=20, eval_every=1,\n",
        "                loss_fn=None,\n",
        "                scheduler=None,\n",
        "                device=None,\n",
        "                history=None):\n",
        "    device = device or get_device()\n",
        "\n",
        "    history = history or defaultdict(list)\n",
        "    # Przenieś model na określone urządzenie\n",
        "    model.to(device)\n",
        "\n",
        "    # Definiowanie funkcji straty\n",
        "    compute_loss = loss_fn or nn.CrossEntropyLoss()\n",
        "\n",
        "    test_loss = history['test_loss'][-1] if history['test_loss'] else 0\n",
        "\n",
        "    # Pętla treningowa\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()  # Ustaw model w tryb treningowy\n",
        "\n",
        "        total_loss = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        # Iteracja po danych treningowych\n",
        "        for x_batch, _ in tqdm(train_loader):\n",
        "            # Przenieś dane na określone urządzenie\n",
        "            x_batch = x_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Wyzerowanie gradientów przed kolejną iteracją\n",
        "            out = model(x_batch)  # Faza w przód\n",
        "\n",
        "            loss = compute_loss(out, x_batch)\n",
        "\n",
        "            loss.backward()   # Faza wsteczna do obliczenia gradientów\n",
        "            optimizer.step()  # Aktualizacja parametrów modelu\n",
        "\n",
        "            total_samples += x_batch.shape[0]\n",
        "            total_loss += loss.item() * x_batch.shape[0]\n",
        "\n",
        "        train_loss = total_loss / total_samples\n",
        "\n",
        "        if (epoch + 1) % eval_every == 0:  # Ewaluacja na zbiorze testowym\n",
        "            model.eval()   # Ustaw model w tryb ewaluacji\n",
        "            total_loss = 0\n",
        "            total_samples = 0\n",
        "            with torch.no_grad():  # Wyłączenie obliczania gradientów\n",
        "                for x_batch, _ in tqdm(test_loader):\n",
        "                    # Przenieś dane na określone urządzenie\n",
        "                    x_batch = x_batch.to(device)\n",
        "\n",
        "                    out = model(x_batch)  # Faza w przód\n",
        "\n",
        "                    loss = compute_loss(out, x_batch)\n",
        "\n",
        "                    total_samples += x_batch.shape[0]\n",
        "                    total_loss += loss.item() * x_batch.shape[0]\n",
        "\n",
        "            test_loss = total_loss / total_samples\n",
        "\n",
        "            print(f'Epoch: {epoch}\\tTrain loss: {train_loss:.3f}'\\\n",
        "                  f'\\tTest loss: {test_loss:.3f}')\n",
        "        else:\n",
        "            print(f'Epoch: {epoch}\\tTrain loss: {train_loss:.3f}')\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['test_loss'].append(test_loss)\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqKd2oeaTQ5i"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    \"\"\" Prosty model typu koder-dekoder \"\"\"\n",
        "    def __init__(self, input_dim, encoding_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Koder (ang. encoder)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, encoding_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Dekoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, input_dim),\n",
        "            nn.Sigmoid()  # Wyj. z zakresu [0, 1]\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, x):\n",
        "        return self.decoder(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decode(self.encode(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU69b3XmTQ5j"
      },
      "outputs": [],
      "source": [
        "# Model przetwarza spłaszczone dane, czyli wektory, stąd konieczna jest transformacja\n",
        "flatten_transform = transforms.Lambda(lambda x: x.view(-1))\n",
        "\n",
        "# Transformacje, można również dodać odbicia itp\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Converts images to PyTorch tensors (values between 0 and 1)\n",
        "    flatten_transform\n",
        "])\n",
        "\n",
        "train_dataset = mnist_train = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = mnist_test = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNXmw4ZxTQ5j"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "device = get_device()\n",
        "\n",
        "# Hiperparametry\n",
        "input_dim = 28 * 28   # 784 for 28x28 images\n",
        "latent_dim = 16       # Wymiar przestrzeni ukrytej\n",
        "learning_rate = 1e-3\n",
        "\n",
        "model = Autoencoder(input_dim, latent_dim).to(device)\n",
        "print(summary(model))\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = StepLR(optimizer, step_size=5)\n",
        "\n",
        "train_unsupervised(model,\n",
        "                   train_loader=train_loader, test_loader=test_loader,\n",
        "                   optimizer=optimizer, scheduler=scheduler,\n",
        "                   n_epochs=7, loss_fn=nn.BCELoss())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up8AjNIPTQ5j"
      },
      "outputs": [],
      "source": [
        "# Ewaluacja na zb. testowym\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    all_encoded = []\n",
        "    all_decoded = []\n",
        "    all_original = []\n",
        "    for data, _ in test_loader:\n",
        "        data = data.view(-1, input_dim).to(device)\n",
        "        encoded = model.encode(data)\n",
        "        decoded = model.decode(encoded)\n",
        "        all_encoded.append(encoded.cpu())\n",
        "        all_decoded.append(decoded.cpu())\n",
        "        all_original.append(data.cpu())\n",
        "\n",
        "    decoded_imgs = torch.cat(all_decoded)\n",
        "    original_imgs = torch.cat(all_original)\n",
        "\n",
        "decoded_imgs = decoded_imgs.numpy()\n",
        "original_imgs = original_imgs.numpy()\n",
        "\n",
        "# Wizualizacja\n",
        "n_samples = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n_samples):\n",
        "    # Oryginalne obrazy\n",
        "    ax = plt.subplot(2, n_samples, i + 1)\n",
        "    plt.imshow(original_imgs[i].reshape(28, 28), cmap='gray')\n",
        "    plt.title(\"Oryginalne\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Rekonstrukcje\n",
        "    ax = plt.subplot(2, n_samples, i + 1 + n_samples)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28), cmap='gray')\n",
        "    plt.title(\"Rekonstrukcje\")\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj4H4hwqTQ5k"
      },
      "source": [
        "# Zadanie 1\n",
        "\n",
        "Proszę sprawdzić jak zmienia się jakość rekonstrukcji w zależności od wymiarowości reprezentacji ukrytej w autokoderze\n",
        "dla zbioru MNIST oraz FashionMNIST (mogą być wybrane klasy).\n",
        "\n",
        "Można porównać zarówno \"surowe\" wartości f. straty, jak i ocenić różnice wizualnie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSbma4otTQ5l"
      },
      "source": [
        "# Przestrzeń ukryta - wizualizacja\n",
        "\n",
        "Mając dostęp do wersji w przestrzeni ukrytej możemy, np. sprawdzić jak wygląda \"średnia\" wszystkich obrazów\n",
        "ze zbioru testowego"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SUpNRPpTQ5l"
      },
      "outputs": [],
      "source": [
        "all_enc = torch.vstack(all_encoded[:-1])  # Łącz listę N wyników dla grup obrazów [64, 32] w [N x 64, 32]\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    mean_image = model.decode( all_enc.mean(dim=0).to(device) )\n",
        "\n",
        "plt.imshow(mean_image.cpu().reshape(28, 28), cmap='gray')\n",
        "plt.title(\"Obraz 'średniej'\")\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VNf-b2-TQ5l"
      },
      "outputs": [],
      "source": [
        "all_org = torch.vstack(all_original[:-1])  # Łącz listę N wyników dla grup obrazów [64, 32] w [N x 64, 32]\n",
        "mean_image = all_org.mean(dim=0)\n",
        "\n",
        "plt.imshow(mean_image.reshape(28, 28), cmap='gray')\n",
        "plt.title(\"Obraz 'średniej' dla oryg. obr. ze zb. testowego\")\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlz3arnbTQ5m"
      },
      "source": [
        "Przestrzeń ukryta (zazwyczaj) ma znacznie mniejszą liczbę wymiarów.\n",
        "Możemy pokusić się o jej wizualizację na płaszczyźnie.\n",
        "\n",
        "Jedną z prostszych metod będzie analiza składowych głównych\n",
        "(ang. Principal Component Analysis, PCA)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TruPi7SUTQ5m"
      },
      "outputs": [],
      "source": [
        "# Use PCA to reduce dimensions to 2 for visualization\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def vizualize_space(samples_stacked, labels,\n",
        "                    viz_method=None,\n",
        "                    title='Visualization of the Latent Space'):\n",
        "\n",
        "    if viz_method is None:\n",
        "        viz_method = PCA(n_components=2)\n",
        "\n",
        "    points_2d = viz_method.fit_transform(samples_stacked)\n",
        "\n",
        "    # Plot the latent space\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    scatter = plt.scatter(points_2d[:, 0], points_2d[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
        "    plt.colorbar(scatter, label='Klasa')\n",
        "    plt.xlabel('Wymiar ukryty 1')\n",
        "    plt.ylabel('Wymiar ukryty 2')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "all_labels = [labels for _, labels in test_loader]\n",
        "labels = torch.vstack(all_labels[:-1])  # Ostatnia grupa ma rozm. < 64 dlatego ją pomijamy\n",
        "\n",
        "vizualize_space(all_org, labels, title='Wizualizacja przestrzeni przykładów')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ThNIHh_TQ5n"
      },
      "outputs": [],
      "source": [
        "vizualize_space(all_enc, labels, title='Wizualizacja przestrzeni ukrytej')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-L82R5veTQ5n"
      },
      "source": [
        "## t-SNE\n",
        "\n",
        "Skrót [t-SNE](https://miroslawmamczur.pl/jak-dziala-metoda-redukcji-wymiarow-t-sne/) oznacza stochastyczną metodę porządkowania sąsiadów w oparciu o rozkład t (t-Distributed Stochastic Neighbor Embedding). Jest to nieliniowa i nienadzorowana technika stosowana przede wszystkim do eksploracji i wizualizacji danych wielowymiarowych.\n",
        "\n",
        "Może dawać lepsze wyniki niż PCA, ale jest stochastyczna oraz wolniejsza.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyDcpY5VTQ5n"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "\n",
        "vizualize_space(all_enc, labels,\n",
        "                viz_method=TSNE(n_components=2, learning_rate='auto', init='random'),\n",
        "                title='Wiz. przestrzeni ukrytej (t-SNE)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r6EgKGaYTQ5o"
      },
      "outputs": [],
      "source": [
        "vizualize_space(all_org, labels,\n",
        "                viz_method=TSNE(n_components=2, learning_rate='auto', init='random'),\n",
        "                title='Wiz. przestrzeni przykładów (t-SNE)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrvW2feZTQ5o"
      },
      "source": [
        "*Wniosek* Autokodery dokonują kompresji wejścia do reprezentacji ukrytej.\n",
        "Powoduje to \"ściśnięcie* klas w przestrzeni na skutek utraty części informacji."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87VoVSgsTQ5p"
      },
      "source": [
        "## Autokodery wariacyjne"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voPSjYg2TQ5p"
      },
      "source": [
        "### Rola Rozbieżności KL w VAE\n",
        "\n",
        "Wariacyjny Autoenkoder (VAE) nie tylko stara się rekonstruować dane wejściowe na podstawie zakodowanej reprezentacji (jak standardowy koder-dekoder), ale także nakłada *aprioryczny* rozkład na przestrzeń ukrytą, tj. wymusza żeby rozkład ten przypominał zadany rozkład -- zazwyczaj standardowy rozkład normalny $\\mathcal{N}(0, I)$.\n",
        "\n",
        "Podczas treningu VAE dąży do tego, aby rozkład przestrzeni ukrytej (charakteryzowany przez średnią $\\mu$ i wariancję $\\sigma^2$ dla każdego wejścia) był zbliżony do zadanego rozkładu apriorycznego.\n",
        "\n",
        "W funkcji straty VAE rozbieżność Kullbacka-Leiblera (KL) mierzy, na ile rozkład kodera odbiega od wybranego rozkładu apriorycznego. Minimalizowanie rozbieżności KL wymusza uporządkowaną strukturę reprezentacji ukrytych i zapobiega zapadaniu się modelu w trywialne rozwiązania. Dzięki temu przestrzeń ukryta będzie miała strukturę **ułatwiającą generowanie nowych**, realistycznych próbek.\n",
        "\n",
        "### Sztuczka Reparametryzacji\n",
        "\n",
        "W VAE nie próbkujemy bezpośrednio z rozkładu opisanego przez $\\mu$ i $\\sigma$, ponieważ próbkowanie **uniemożliwiłoby przepływ gradientów** przez koder. Sztuczka reparametryzacji rozwiązuje ten problem poprzez wyrażenie zmiennej losowej $z$ jako:\n",
        "$$\n",
        "z = \\mu + \\sigma \\epsilon\n",
        "$$\n",
        "gzie $\\epsilon \\sim \\mathcal{N}(0, I)$. Dzięki temu losowość jest izolowana w $\\epsilon$, które nie zależy od parametrów kodera.\n",
        "Koder **dostarcza $\\mu$ i $\\sigma$, umożliwiając** obliczanie gradientów."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJpYTmE5TQ5p"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim=784, hidden_dim=256, latent_dim=32):\n",
        "        super(VAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        # Warstwy kodera\n",
        "        self.encode_l1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.mean_layer = nn.Linear(hidden_dim, latent_dim)  # Średnia\n",
        "        self.log_var_layer = nn.Linear(hidden_dim, latent_dim)  # Log z wariancji\n",
        "        # Warstwy dekodera\n",
        "        self.decode_l1 = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.decode_l2 = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        recon_x = self.decode(z)\n",
        "        return recon_x, mu, logvar\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = torch.relu(self.encode_l1(x))\n",
        "        mu = self.mean_layer(h)\n",
        "        logvar = self.log_var_layer(h)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        # Trik reparametryzacji\n",
        "        # Wariancja musi być dodatnia, e^x to zapewnia:\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)  # Szum\n",
        "        z = mu + eps * std\n",
        "        return z\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = torch.relu(self.decode_l1(z))\n",
        "        recon_x = torch.sigmoid(self.decode_l2(h))\n",
        "        return recon_x\n",
        "\n",
        "\n",
        "def loss_function(recon_x, x, mu, logvar, beta=1.0):\n",
        "    # Strata rekonstrukcji (entropia krzyżowa)\n",
        "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    # Dywergencja KL(N(μ, σ²) || N(0, I))\n",
        "    variance = logvar.exp()\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - variance)\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "\n",
        "model = VAE(latent_dim=64).to(get_device())\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for data, _ in tqdm(train_loader):\n",
        "        data = data.view(-1, 28 * 28).to(get_device())\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    avg_loss = train_loss / len(train_loader.dataset)\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {avg_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrXu9dzOTQ5q"
      },
      "outputs": [],
      "source": [
        "def show_vae_output(model, n_samples=10):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        all_decoded = []\n",
        "        all_original = []\n",
        "        for data, _ in test_loader:\n",
        "            data = data.view(-1, input_dim).to(get_device())\n",
        "            decoded, _, _ = model(data)\n",
        "            all_decoded.append(decoded.cpu())\n",
        "            all_original.append(data.cpu())\n",
        "\n",
        "        decoded_imgs = torch.cat(all_decoded)\n",
        "        original_imgs = torch.cat(all_original)\n",
        "\n",
        "    # Wizualizacja\n",
        "\n",
        "    decoded_imgs = decoded_imgs.numpy()\n",
        "    original_imgs = original_imgs.numpy()\n",
        "\n",
        "    n_samples = 10\n",
        "    plt.figure(figsize=(10, 2.5))\n",
        "    for i in range(n_samples):\n",
        "        ax = plt.subplot(2, n_samples, i + 1)\n",
        "        plt.imshow(original_imgs[i].reshape(28, 28), cmap='gray')\n",
        "        plt.title(\"Original\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        ax = plt.subplot(2, n_samples, i + 1 + n_samples)\n",
        "        plt.imshow(decoded_imgs[i].reshape(28, 28), cmap='gray')\n",
        "        plt.title(\"Reconst.\")\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "show_vae_output(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJgW6YoHTQ5q"
      },
      "source": [
        "Nieco lepsze rezultaty możemy osiągnąć dodając do VAE warsty konwolucyjne."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54epudskTQ5q"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim=32):\n",
        "        super(VAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Koder -- prosta sieć konwolucyjna\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),  # (batch, 32, 14, 14)\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1), # (batch, 64, 7, 7)\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=0),# (batch, 128, 3, 3)\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        # Wstrzykujemy informację o klasie przykładu, stąd \" + num_classes\"\n",
        "        self.fc_mu = nn.Linear(128 * 3 * 3, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(128 * 3 * 3, latent_dim)\n",
        "\n",
        "        # Dekoder  -- również sieć konwolucyjna\n",
        "        self.decoder_input = nn.Linear(latent_dim, 128 * 3 * 3)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=0),  # (batch, 64, 7, 7)\n",
        "            nn.LeakyReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),   # (batch, 32, 14, 14)\n",
        "            nn.LeakyReLU(),\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),    # (batch, 1, 28, 28)\n",
        "            nn.Flatten(),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = x.view(x.size(0), 1, 28, 28)\n",
        "        enc = self.encoder(x)\n",
        "        mu = self.fc_mu(enc)\n",
        "        logvar = self.fc_logvar(enc)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        epsilon = torch.randn_like(std)\n",
        "        return mu + epsilon * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        x = self.decoder_input(z)\n",
        "        x = x.view(-1, 128, 3, 3)\n",
        "        reconstructed = self.decoder(x)\n",
        "        return reconstructed\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        reconstructed_x = self.decode(z)\n",
        "        return reconstructed_x, mu, logvar\n",
        "\n",
        "\n",
        "model = VAE().to(get_device())\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for data, _ in tqdm(train_loader):\n",
        "        data = data.view(-1, 28 * 28).to(get_device())\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "                             #beta=0.5 + 0.5 * (1. + epoch) / epochs)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = train_loss / len(train_loader.dataset)\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {avg_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ffUIi0xTQ5r"
      },
      "outputs": [],
      "source": [
        "show_vae_output(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ0vZfbCTQ5r"
      },
      "source": [
        "### Próbkowanie przestrzeni ukrytej\n",
        "\n",
        "Próbkowanie przestrzeni ukrytej polega na wybraniu losowej próbki z wielowymiarowego rozkładu normalnego\n",
        "$\\mathcal{N}(0, 1)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmcjM7ZjTQ5r"
      },
      "outputs": [],
      "source": [
        "import torchvision.utils as vutils\n",
        "\n",
        "def show_images(images):\n",
        "    nrow = ncol = int(len(images) ** 0.5 + 0.5)\n",
        "    grid = vutils.make_grid(images, nrow=nrow, padding=2, normalize=True)\n",
        "    np_grid = grid.numpy().transpose((1, 2, 0))\n",
        "    plt.figure(figsize=(ncol, nrow))\n",
        "    plt.imshow(np_grid, cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "def sample_model(model, num_samples=16):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(num_samples, model.latent_dim)\n",
        "        z = z.to(get_device())\n",
        "        generated = model.decode(z).cpu()\n",
        "        generated = generated.view(num_samples, 1, 28, 28)\n",
        "    return generated\n",
        "\n",
        "\n",
        "show_images(sample_model(model, num_samples=5*5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmgmUdlvTQ5r"
      },
      "source": [
        "Możemy wygenerować próbki skupione w sąsiedztwie podanego przykładu.\n",
        "W tym celu korzystamy z kodera w celu uzyskania parametrów, tj. średniej\n",
        "oraz odchylenia, dla podanego przykładu i stosujemy je przy losowaniu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9SKzVpSTQ5s"
      },
      "outputs": [],
      "source": [
        "def sample_neighborhood(vae, x, num_samples=5):\n",
        "    \"\"\" Zwraca próbki w sąsiedztwie podanego przykładu 'x' \"\"\"\n",
        "    vae.eval()\n",
        "    with torch.no_grad():\n",
        "        mu, logvar = vae.encode(x)\n",
        "        # Repeat mu and logvar for num_samples times\n",
        "        mu = mu.repeat(num_samples, 1)\n",
        "        logvar = logvar.repeat(num_samples, 1)\n",
        "        # Sample latent vectors using the reparameterization trick\n",
        "        z = vae.reparameterize(mu, logvar)\n",
        "        reconstructions = vae.decode(z).view(num_samples, 1, 28, 28).cpu()\n",
        "    return reconstructions\n",
        "\n",
        "\n",
        "img_batch, _ = next(iter(test_loader))\n",
        "\n",
        "for example_id in range(2):\n",
        "    input_image = img_batch[example_id].view(-1, 28*28).to(get_device())\n",
        "    reconstructions = sample_neighborhood(model, input_image, num_samples=4*4)\n",
        "    show_images(reconstructions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjlxpeF0TQ5s"
      },
      "source": [
        "### Interpolacja pomiędzy przykładami\n",
        "\n",
        "\n",
        "Koder dokonuje \"kompresji\" obrazu do wektora w przestrzeni ukrytej.\n",
        "Mając wektory dla pary przykładów, możemy dokonać prostej interpolacji pomiędzy nimi\n",
        "otrzymując przykłady łączące cechy obu obrazów."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpW5lVj1TQ5t"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "digit_label1, digit_label2 = 1, 9  # Klasy obrazów\n",
        "img1, img2 = None, None  # Wybrane obrazy\n",
        "\n",
        "# Znajdź obrazy z obu klas\n",
        "for img_batch, label_batch in test_loader:\n",
        "    for img, label in zip(img_batch, label_batch):\n",
        "        if label.item() == digit_label1 and img1 is None:\n",
        "            img1 = img\n",
        "        if label.item() == digit_label2 and img2 is None:\n",
        "            img2 = img\n",
        "        if img1 is not None and img2 is not None:\n",
        "            break\n",
        "    if img1 is not None and img2 is not None:\n",
        "        break\n",
        "\n",
        "device = get_device()\n",
        "img1 = img1.unsqueeze(0).to(device)  # Shape: [1, 1, 28, 28]\n",
        "img2 = img2.unsqueeze(0).to(device)\n",
        "\n",
        "steps = 9\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Zakoduj obrazy do postaci ukrytej\n",
        "    mu1, logvar1 = model.encode(img1)\n",
        "    z1 = model.reparameterize(mu1, logvar1)\n",
        "\n",
        "    mu2, logvar2 = model.encode(img2)\n",
        "    z2 = model.reparameterize(mu2, logvar2)\n",
        "\n",
        "    # Współczynniki interpolacji\n",
        "    alphas = torch.linspace(0, 1, steps).to(device).unsqueeze(1)\n",
        "\n",
        "    # Właściwa interpolacja\n",
        "    z_interp = z1 * (1 - alphas) + z2 * alphas  # Shape: [steps, latent_dim]\n",
        "\n",
        "    # Dekoduj obrazy\n",
        "    generated = model.decode(z_interp).cpu()\n",
        "    generated = generated.view(steps, 1, 28, 28)\n",
        "\n",
        "# Wyświetl\n",
        "grid = vutils.make_grid(generated, nrow=len(generated), padding=2, normalize=True)\n",
        "np_grid = grid.numpy().transpose((1, 2, 0))\n",
        "plt.figure(figsize=(len(generated), 1))\n",
        "plt.imshow(np_grid, cmap='gray')\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Psu4GwaTQ5t"
      },
      "source": [
        "## Warunkowanie\n",
        "\n",
        "Bazowa wersja VAE nie daje nam kontroli nad tym jakiej klasy przykład uzyskamy\n",
        "losując wektor z przestrzeni ukrytej. Jeżeli na wejście sieci będziemy podawać\n",
        "również docelowe etykiety przykładów, to będziemy mogli **warunkować**\n",
        "pracę dekodera, tj. uzależnić wynik od pożądanej klasy docelowej."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l93AybMFTQ5u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CondVAE(nn.Module):\n",
        "    def __init__(self, num_classes, latent_dim=32):\n",
        "        super(CondVAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Koder -- prosta sieć konwolucyjna\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),  # (batch, 32, 14, 14)\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1), # (batch, 64, 7, 7)\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=0),# (batch, 128, 3, 3)\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        # Wstrzykujemy informację o klasie przykładu, stąd \" + num_classes\"\n",
        "        self.fc_mu = nn.Linear(128 * 3 * 3 + num_classes, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(128 * 3 * 3 + num_classes, latent_dim)\n",
        "\n",
        "        # Dekoder  -- również sieć konwolucyjna\n",
        "        self.decoder_input = nn.Linear(latent_dim + num_classes, 128 * 3 * 3)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=0),  # (batch, 64, 7, 7)\n",
        "            nn.LeakyReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),   # (batch, 32, 14, 14)\n",
        "            nn.LeakyReLU(),\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),    # (batch, 1, 28, 28)\n",
        "            nn.Flatten(),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def encode(self, x, labels):\n",
        "        x = x.view(x.size(0), 1, 28, 28)\n",
        "        features = self.encoder(x)\n",
        "        labels_onehot = nn.functional.one_hot(labels, num_classes=self.num_classes).float()\n",
        "        combined = torch.cat([features, labels_onehot], dim=1)\n",
        "        mu = self.fc_mu(combined)\n",
        "        logvar = self.fc_logvar(combined)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        epsilon = torch.randn_like(std)\n",
        "        return mu + epsilon * std\n",
        "\n",
        "    def decode(self, z, labels):\n",
        "        labels_onehot = nn.functional.one_hot(labels, num_classes=self.num_classes).float()\n",
        "        z = torch.cat([z, labels_onehot], dim=1)\n",
        "        x = self.decoder_input(z)\n",
        "        x = x.view(-1, 128, 3, 3)\n",
        "        reconstructed = self.decoder(x)\n",
        "        return reconstructed\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        mu, logvar = self.encode(x, labels)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        reconstructed_x = self.decode(z, labels)\n",
        "        return reconstructed_x, mu, logvar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dnwhoYTTQ5u"
      },
      "outputs": [],
      "source": [
        "# Trening\n",
        "\n",
        "model = CondVAE(latent_dim=64, num_classes=10).to(get_device())\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for data, labels in tqdm(train_loader):\n",
        "        data = data.view(-1, 28 * 28).to(get_device())\n",
        "        labels = labels.to(get_device())\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data, labels)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar, beta=4)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    avg_loss = train_loss / len(train_loader.dataset)\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {avg_loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXakceExTQ5u"
      },
      "source": [
        "### Warunkowane próbkowanie\n",
        "\n",
        "Do losowego wektora w przestrzeni ukrytej na wejście dekodera podajemy również\n",
        "pożądaną klasę (etykietę) wyniku."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TBoqEmkTQ5v"
      },
      "outputs": [],
      "source": [
        "def sample_and_display_cond(model, label, num_samples=9):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(num_samples, model.latent_dim)\n",
        "        labels = torch.full((num_samples, ), label).to(get_device())\n",
        "        z = z.to(get_device())\n",
        "        generated = model.decode(z, labels).cpu()\n",
        "        generated = generated.view(num_samples, 1, 28, 28)\n",
        "    show_images(generated)\n",
        "\n",
        "for i in range(0, 10):\n",
        "    sample_and_display_cond(model, label=i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANSvAGfXTQ5v"
      },
      "source": [
        "# Zadanie 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHi_ffUKTQ5w"
      },
      "source": [
        "Proszę sprawdzić działanie autokodera wariacyjnego na zbiorze FashionMNIST.\n",
        "Zbiór ten jest bardziej wymagający niż MNIST, stąd proszę rozważyć:\n",
        "\n",
        "- większą liczbę epok treningu\n",
        "- większy wymiar reprezentacji ukrytej zamiast, tj. > 64\n",
        "- większy rozmiar modelu, np. liczbę kanałów w modułach konwolucyjnych"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}