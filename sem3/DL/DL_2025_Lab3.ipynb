{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janbanot/msc-cs-code/blob/main/sem3/DL/DL_2025_Lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca6859f6",
      "metadata": {
        "id": "ca6859f6"
      },
      "source": [
        "# Metody regularyzacji w treningu sieci neuronowych"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7ab8342",
      "metadata": {
        "id": "a7ab8342"
      },
      "source": [
        "## Zad. 1a. Zjawisko niedopasowania oraz przeuczenia na przykładzie regresji 1D\n",
        "\n",
        "W tym zadaniu poznasz koncepcje *underfittingu* (niedopasowania modelu) i *overfittingu* (nadmiernego dopasowania modelu) na prostym przykładzie regresji jednowymiarowej. Użyjemy małej sieci neuronowej MLP (Multi-Layer Perceptron) do aproksymacji funkcji z dodanym szumem.\n",
        "\n",
        "Dostarczony kod demonstruje dopasowanie różnych konfiguracji sieci neuronowej do danych treningowych pochodzących z funkcji:  \n",
        "Celem jest trening modelu dobrze odwzorowującego funkcję (trend), z której pochodzą dane:\n",
        "\n",
        "$$ y = \\sin(2\\pi x) + 0.5 \\sin(4\\pi x) + 0.3 \\cos(6\\pi x) + \\epsilon $$\n",
        "\n",
        "gdzie $ x \\in [0, 1] $, a $ \\epsilon $ to szum gaussowski o odchyleniu standardowym 0.2.\n",
        "Dane treningowe składają się z 25 punktów, a testowe z 200 punktów (bez szumu).\n",
        "\n",
        "### Sprawdź następujące konfiguracje:\n",
        "* 1 warstwa ukryta z 2 neuronami\n",
        "* 2 warstwy ukryte po 10 neuronów\n",
        "* 3 warstwy ukryte po 50 neuronów\n",
        "\n",
        "**Dla każdej konfiguracji określ czy mamy do czynienia z przeuczeniem lub niedopasowaniem.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87c09f76",
      "metadata": {
        "id": "87c09f76"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ustalamy ziarno generatora dla powtarzalności wyników:\n",
        "torch.manual_seed(283)\n",
        "np.random.seed(283)\n",
        "\n",
        "def fn(x):\n",
        "    \" Funkcja, którą chcemy nauczyć się odwzorowywać \"\n",
        "    return np.sin(2 * np.pi * x) + 0.5 * np.sin(4 * np.pi * x) + 0.3 * np.cos(6 * np.pi * x)\n",
        "\n",
        "def make_data(n=50):\n",
        "    x = np.linspace(0, 1, n)\n",
        "    y_true = fn(x)\n",
        "    noise = np.random.randn(n) * 0.2\n",
        "    # Zazwyczaj dane treningowe obarczone są pewnym szumem, tu również go dodajemy:\n",
        "    y = y_true + noise\n",
        "    return x.reshape(-1, 1).astype(np.float32), y.reshape(-1, 1).astype(np.float32), y_true.reshape(-1, 1).astype(np.float32)\n",
        "\n",
        "x_train, y_train, y_true = make_data(25)\n",
        "x_test = np.linspace(0, 1, 200).reshape(-1, 1).astype(np.float32)\n",
        "y_test_true = fn(x_test)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hidden_layers=[10]):\n",
        "        \" Elementy hidden_layers określają liczbę neuronów w warstwach ukrytych \"\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        in_dim = 1  # Na wej. mamy dane jednowymiarowe\n",
        "        for h in hidden_layers:\n",
        "            layers += [nn.Linear(in_dim, h), nn.Tanh()]\n",
        "            in_dim = h  # Kolejne wej. są wyjściami z poprzedniej warstwy\n",
        "        layers += [nn.Linear(in_dim, 1)]\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def train_model(hidden_layers, weight_decay=0.0, epochs=1000):\n",
        "    model = MLP(hidden_layers)\n",
        "    opt = optim.Adam(model.parameters(), lr=0.01, weight_decay=weight_decay)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    x_t = torch.tensor(x_train)\n",
        "    y_t = torch.tensor(y_train)\n",
        "    losses = []\n",
        "    for _ in range(epochs):\n",
        "        opt.zero_grad()\n",
        "        y_pred = model(x_t)\n",
        "        loss = loss_fn(y_pred, y_t)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "    return model, losses\n",
        "\n",
        "\n",
        "def solve(hidden_layers=[2], weight_decay=0.0):\n",
        "    model, _ = train_model(hidden_layers, weight_decay)\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    with torch.no_grad():\n",
        "        y_pred = model(torch.tensor(x_test)).numpy()\n",
        "\n",
        "    plt.plot(x_test, y_test_true, 'g--', label='Prawdziwa funkcja')\n",
        "    plt.scatter(x_train, y_train, color='red', label='Dane treningowe', alpha=0.3)\n",
        "    plt.plot(x_test, y_pred, 'b', label='Predykcja')\n",
        "    plt.ylim(-2, 2)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "solve(hidden_layers=[2], weight_decay=0)\n",
        "solve(hidden_layers=[10, 10], weight_decay=0)\n",
        "solve(hidden_layers=[50, 50, 50], weight_decay=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb6688a3",
      "metadata": {
        "id": "eb6688a3"
      },
      "source": [
        "## Zad. 1b. Wpływ regularyzacji L2\n",
        "\n",
        "Sprawdź jak regularyzacja L2 (`weight_decay`) wpływa na efekty treningu.\n",
        "\n",
        "Jaka wartość współczynnika `weight_decay` jest odpowiednia, rozważ: `1e-2`, `1e-3`, `1e-4`?\n",
        "\n",
        "**Czy regularyzacja L2 pomaga w uogólnianiu?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65f93f4f",
      "metadata": {
        "id": "65f93f4f"
      },
      "outputs": [],
      "source": [
        "# Rozwiązanie\n",
        "for e in (1e-2, 1e-3, 1e-4):\n",
        "  print(f\"weight decay = {e}\")\n",
        "  solve(hidden_layers=[2], weight_decay=e)\n",
        "  solve(hidden_layers=[10, 10], weight_decay=e)\n",
        "  solve(hidden_layers=[50, 50, 50], weight_decay=e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fcdabde",
      "metadata": {
        "id": "6fcdabde"
      },
      "source": [
        "## Zad. 1c. Liczność zbioru danych\n",
        "\n",
        "Sprawdź jak zmieni się sytuacja, gdy zbiór treningowy będzie liczył 250 punktów zamiast 25.\n",
        "Rozważ przypadki **z oraz bez regularyzacji L2**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61ad7295",
      "metadata": {
        "id": "61ad7295"
      },
      "outputs": [],
      "source": [
        "x_train, y_train, y_true = make_data(250)\n",
        "\n",
        "solve(hidden_layers=[2], weight_decay=0)\n",
        "solve(hidden_layers=[10, 10], weight_decay=0)\n",
        "solve(hidden_layers=[50, 50, 50], weight_decay=0)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eb6688a3",
        "6fcdabde"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}