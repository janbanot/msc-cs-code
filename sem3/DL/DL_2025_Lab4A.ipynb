{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janbanot/msc-cs-code/blob/main/sem3/DL/DL_2025_Lab4A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpWx_-qeVtYQ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Import potrzebnych modułów i funkcji\n",
        "\n",
        "!uv pip install torchinfo\n",
        "\n",
        "# Importy z biblioteki standardowej\n",
        "from collections import defaultdict\n",
        "from random import random\n",
        "import math\n",
        "\n",
        "# Importy z bibliotek zewnętrznych\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, tensor\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Importy do wizualizacji\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Importy z biblioteki scikit-learn\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,\n",
        "    ConfusionMatrixDisplay, classification_report, roc_auc_score\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dodatkowe importy\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipgZzSuPPDlD"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<!-- Potrzebne dla poprawnego wyświetlania tqdm w VSCode https://stackoverflow.com/a/77566731 -->\n",
        "<style>\n",
        ".cell-output-ipywidget-background {\n",
        "    background-color: transparent !important;\n",
        "}\n",
        ":root {\n",
        "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
        "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
        "}\n",
        "</style>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiYuSVL115-c"
      },
      "source": [
        "# Sieci konwolucyjne\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJHMcUjUfX3K"
      },
      "source": [
        "## Operacja konwolucji\n",
        "\n",
        "**Konwolucja lub inaczej splot** to działanie określone dla pary funkcji, które daje w wyniku nową funkcję. Splot znajduje liczne zastosowania w przetwarzaniu sygnałów, w tym obrazów. Przykładowo, konwolucja z odpowiednio dobranym filtrem pozwala na rozmywanie lub wyostrzanie obrazu, a także detekcję krawędzi.\n",
        "\n",
        "W sztucznych sieciach neuronowych przetwarzających dodanie warstwy neuronów realizujących konwolucje pikseli obrazu daje zazwyczaj dobre rezultaty poprawiając zdolność uogólniania,\n",
        "zmniejszając liczbę parametrów oraz skracając czas treningu.\n",
        "\n",
        "Poniżej widzimy przykład konwolucji dwóch dyskretnych ciągów\n",
        "$a = (1, 2, 3)$ oraz $b=(4, 5, 6)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGirOkJ-afG4"
      },
      "outputs": [],
      "source": [
        "a1, a2, a3 = 1, 2, -1\n",
        "b1, b2, b3 = 2, 1, 0.5\n",
        "\n",
        "# Operacja konwolucji polega na przenożeniu (a1, a2, a3) przez\n",
        "# kolejno przesunięty ciąg (b3, b2, b1)\n",
        "#\n",
        "#       a1 a2 a3     =>   a1 * b1\n",
        "# b3 b2 b1\n",
        "#\n",
        "#       a1 a2 a3     =>   a1 * b2 + a2 * b1\n",
        "#    b3 b2 b1\n",
        "#\n",
        "#       a1 a2 a3     =>   a1 * b3 + a2 * b2 + a3 * b3\n",
        "#       b3 b2 b1\n",
        "#\n",
        "#       a1 a2 a3     =>   a2 * b3 + a3 * b2\n",
        "#          b3 b2 b1\n",
        "#\n",
        "#       a1 a2 a3     =>   a2 * b3 + a3 * b2\n",
        "#          b3 b2 b1\n",
        "#\n",
        "#       a1 a2 a3     =>   a3 * b3\n",
        "#             b3 b2 b1\n",
        "\n",
        "print(np.convolve([a1, a2, a3], [b1, b2, b3]))\n",
        "\n",
        "[a1 * b1,\n",
        " a1*b2 + a2*b1,\n",
        " a1*b3 + a2*b2 + a3*b1,\n",
        " a2*b3 + a3*b2,\n",
        " a3*b3 ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV1mMAFzflok"
      },
      "source": [
        "Analogiczny efekt uzyskamy korzystając z funkcji `numpy.convolve`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q14aJDliJFd"
      },
      "source": [
        "Odpowiednio dobierając wagi jednego z ciągów możemy w wyniku konwolucji otrzymać, np. ciąg będący uśrednieniem sąsiednich wyrazów."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKIJj4lMf7KD"
      },
      "outputs": [],
      "source": [
        "a = [1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2]\n",
        "b = [0.5, 0.5]  # (1/2, 1/2) -> uzyskamy efekt uśrednienia sąsiednich elementów a\n",
        "c = np.convolve(a, b)\n",
        "c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-xV6zTWga40"
      },
      "outputs": [],
      "source": [
        "def plot_conv1d(inp, out):\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(6, 3))\n",
        "\n",
        "    axes[0].bar(range(len(inp)+(1 if len(inp) < len(out) else 0)), height=(inp+[0.0] if len(inp) < len(out) else inp))\n",
        "    axes[0].set_title('Ciąg a')\n",
        "    axes[1].bar(range(len(out)), height=out)\n",
        "    axes[1].set_xlabel('Konwolucja a z b');\n",
        "\n",
        "\n",
        "plot_conv1d(a, c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0756crMnhFq"
      },
      "source": [
        "### Konwolucje 1d w PyTorch\n",
        "\n",
        "Analogiczny efekt możelmy uzyskać w `PyTorch` za pomocą klasy `Conv1d`.\n",
        "\n",
        "**UWAGA**: Konwolucje w `PyTorch` zakładają, że dane przetwarzamy w **grupach** (ang. batch) przykładów, a każdy złożony jest z określonej liczby **kanałów** (ang. channel).\n",
        "\n",
        "Stąd nasze dane umieszczamy w tensorach z 3 wymiarami, tj.\n",
        "\n",
        "    x[ indeks przykładu ][ indeks kanału ][ indeks piksela ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Przypomnienie -- manipulacja wymiarami w PyTorch\n",
        "m1 = torch.tensor( [[1, 2, 3],\n",
        "                    [4, 5, 6]])\n",
        "\n",
        "print(m1.shape, m1[1][2])\n",
        "\n",
        "m2 = m1.unsqueeze(0)  # Dodajemy nowy wymiar na pozycji 0\n",
        "print(m2.shape, m2[0][1][2]) # Dodatkowy indeks na poz. 0\n",
        "\n",
        "m3 = m2.unsqueeze(0)  # I kolejny\n",
        "print(m3.shape, m3[0][0][1][2])\n",
        "\n",
        "m4 = m1.view(1, 1, 2, 3)  # Ten sam efekt co 2 powyższe\n",
        "print(m4.shape, m4[0][0][1][2])\n",
        "\n",
        "m4 = m1.view(1, 1, 2, -1)  # Możemy podać -1 dla ostatniego wym.\n",
        "print(m4.shape, m4[0][0][1][2])\n",
        "\n",
        "m5 = m4.squeeze()  # Usuń wymiary o rozmiarze 1\n",
        "print(m5.shape, m5[1][2])\n",
        "\n",
        "m6 = m1.unsqueeze(2) # Dodajemy nowy wymiar na pozycji 2\n",
        "print(m6.shape, m6[1][2][0])\n",
        "\n",
        "m7 = m1.permute(1, 0)  # Zamień kolejność wymiarów wiersze <-> kolumny\n",
        "print(m7.shape, m7[2][1])\n"
      ],
      "metadata": {
        "id": "KVd7IrBNdkSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tworzymy 3-wymiarowe tensory z naszych 1-wym. ciągów a i b\n",
        "at = torch.tensor(a, dtype=torch.float32).view(1, 1, -1)\n",
        "bt = torch.tensor(b, dtype=torch.float32).view(1, 1, -1)\n",
        "\n",
        "at.shape, bt.shape, at[0, 0]"
      ],
      "metadata": {
        "id": "c7dmd2oe8X5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wynik jest \"hurtowy\", tj. tensor 3d odpowiadający przykładom, kanałom oraz obliczonym wartościom\n"
      ],
      "metadata": {
        "id": "tXZvwEJbBMrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out = F.conv1d(at, bt, padding=1)\n",
        "\n",
        "print(f'{out.shape = }')\n",
        "print(out[0][0])"
      ],
      "metadata": {
        "id": "o9r-aDbFBKyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_conv1d(a, out.numpy()[0][0])\n",
        "plot_conv1d(a, out.squeeze().numpy())"
      ],
      "metadata": {
        "id": "tzceEeGiBrTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wersja obiektowa z `Conv1d`"
      ],
      "metadata": {
        "id": "RgqfxOOCCHnx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dv3x-Hc-m7ER"
      },
      "outputs": [],
      "source": [
        "# Warstwa Conv1d przyjmuje parametry:\n",
        "# - in_channels=1 jeden kanał wejściowy\n",
        "# - out_channels=1 jeden kanał wynikowy\n",
        "# - kernel_size=2 rozmiar kernela\n",
        "# Podanie parametru `padding=1` spowoduje dodanie jednej dodatkowej wartości (zero) z każdej strony.\n",
        "conv = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2, bias=False, padding=1)\n",
        "\n",
        "# Ustaw parametry konwolucji\n",
        "with torch.no_grad():\n",
        "    conv.weight = nn.Parameter(bt)\n",
        "\n",
        "c_tensor = conv(at)   # Wykonaj konwolucję\n",
        "\n",
        "# Odłączenie od grafu obliczeń i konwersja do NumPy\n",
        "c_pytorch = c_tensor.detach().numpy()[0][0]\n",
        "# lub\n",
        "c_pytorch = c_tensor.detach().numpy().squeeze()\n",
        "\n",
        "print(f'Wynik: {c_pytorch}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvimA8obp4gV"
      },
      "outputs": [],
      "source": [
        "plot_conv1d(a, c_pytorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxQA86bBqY12"
      },
      "source": [
        "### Uzupełnianie (padding) i rozmiar kernela\n",
        "\n",
        "Domyślnie `padding=0`, co powoduje, że wynik jest \"krótszy\" z każdej strony."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyUlMwcgqSHI"
      },
      "outputs": [],
      "source": [
        "conv = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2, bias=False,\n",
        "                 padding=0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    conv.weight = nn.Parameter(bt)\n",
        "\n",
        "c_tensor = conv(at)   # Wykonaj konwolucję\n",
        "\n",
        "c_pytorch = c_tensor.detach().squeeze().numpy()\n",
        "print(f'Wynik: {c_pytorch}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c938oJf3qmrz"
      },
      "outputs": [],
      "source": [
        "plot_conv1d(a, c_pytorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpklMlQT0BKa"
      },
      "source": [
        "Dla kernela o rozmiarze 3 możemy mieć średnią 3 sąsiednich elementów."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrGG_kE6z_Xu"
      },
      "outputs": [],
      "source": [
        "conv = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, bias=False,\n",
        "                 padding=1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    conv.weight = nn.Parameter(torch.tensor([[[1./3, 1./3, 1./3,]]], dtype=torch.float32))\n",
        "\n",
        "c_tensor = conv(at)   # Wykonaj konwolucję\n",
        "\n",
        "# Odłączenie od grafu obliczeń, usunięcie zbędnego wymiaru i konwersja do NumPy\n",
        "c_pytorch = c_tensor.detach().squeeze().numpy()\n",
        "print(f'Wynik: {c_pytorch}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAqA8qzs0X_f"
      },
      "outputs": [],
      "source": [
        "plot_conv1d(a, c_pytorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud4Wo_o5vDsQ"
      },
      "source": [
        "## Konwolucje 2d\n",
        "\n",
        "W analogiczny sposób można dokonywać konwolucji dla macierzy,\n",
        "co jest przydatne przy przetwarzaniu obrazów.\n",
        "Poniżej widzimy kilka przykładów konwolucji przy pomocy\n",
        "`torch.nn.Conv2d`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex1 = torch.zeros(1, 1, 10, 10)\n",
        "ex1[0, 0, 3:7, 3:7] = 1  # Kwadrat\n",
        "\n",
        "ex2 = torch.randn(1, 1, 20, 20)  # Losowo\n",
        "\n",
        "input = ex2\n",
        "\n",
        "kernels = [\n",
        "    torch.tensor([[1, 1, 1],\n",
        "                  [0, 0, 0],\n",
        "                  [-1, -1, -1]], dtype=torch.float32).view(1, 1, 3, 3),  # Poziomy\n",
        "\n",
        "    torch.tensor([[-1, 0, 1],\n",
        "                  [-1, 0, 1],\n",
        "                  [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3),  # Pionowy\n",
        "\n",
        "    torch.tensor([[1, 1, 1],\n",
        "                  [1, 1, 1],\n",
        "                  [1, 1, 1]], dtype=torch.float32).view(1, 1, 3, 3) / 9  # Rozmycie (blur)\n",
        "]\n",
        "\n",
        "for i, kernel in enumerate(kernels):\n",
        "    output = F.conv2d(input, kernel, padding=0)\n",
        "    plt.subplot(1, 4, i+2)\n",
        "    o = output[0, 0].detach()\n",
        "    plt.imshow(o, cmap='gray')\n",
        "\n",
        "print(f'Rozmiar wyjścia: {o.shape}')\n",
        "\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.imshow(input[0, 0], cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y5Bu6YFmGq9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zad. 0\n",
        "\n",
        "Sprawdź działanie konwolucji dla użytych wcześniej filtrów (kerneli) na obrazie \"Lena\"."
      ],
      "metadata": {
        "id": "6nNmbeJALc6X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1zphlJXja_v"
      },
      "outputs": [],
      "source": [
        "# Pobierz przykładowy obraz:\n",
        "!wget --output-document Lenna.png https://upload.wikimedia.org/wikipedia/en/7/7d/Lenna_%28test_image%29.png &> /dev/null\n",
        "\n",
        "# Podgląd\n",
        "im = Image.open('Lenna.png')\n",
        "im"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2fRiQqflgxW"
      },
      "outputs": [],
      "source": [
        "im = im.convert('L')  # Konwersja na skalę szarości\n",
        "# Dodatkowo normalizujemy wartości ( / 255), tak aby jasność pikseli była z zakresu [0, 1]\n",
        "pixels = tensor( np.asarray(im) / 255, dtype=torch.float32 )\n",
        "pixels.shape, pixels.dtype, pixels[0::100, 0::100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pe2qHVDIsDRQ"
      },
      "outputs": [],
      "source": [
        "# Pomocnicza funkcja do wizualizacji\n",
        "\n",
        "def nn_output_to_image(out):\n",
        "    pix = out.detach().numpy().squeeze()  # odłącz (nie chcemy obliczać gradientów) i konwertuj do numpy\n",
        "    # Normalizuj wartości pikseli do przedziału [0, 255]\n",
        "    pix = np.interp(pix, (pix.min(), pix.max()), (0, 255))\n",
        "    return Image.fromarray(np.uint8(pix))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSeIZjJZm77o"
      },
      "outputs": [],
      "source": [
        "nn_output_to_image(pixels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Rozwiązanie...\n",
        "# Obraz 'Lena' musi mieć odpowiedni kształt dla F.conv2d:\n",
        "# [Batch_size, Channels_in, Height, Width]\n",
        "# Nasz tensor 'pixels' ma kształt [H, W], więc dodajemy dwa wymiary.\n",
        "input_image = pixels.view(1, 1, pixels.shape[0], pixels.shape[1])\n",
        "\n",
        "# Tytuły dla wykresów\n",
        "titles = [\"Oryginał (Lena)\", \"Filtr Poziomy\", \"Filtr Pionowy\", \"Filtr Rozmycie\"]\n",
        "\n",
        "# Przygotowanie figury do wizualizacji (1 wiersz, 4 kolumny)\n",
        "plt.figure(figsize=(20, 6))\n",
        "\n",
        "# Wyświetlenie obrazu oryginalnego\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.imshow(pixels, cmap='gray') # Oryginalny tensor 2D\n",
        "plt.title(titles[0])\n",
        "plt.axis('off')\n",
        "\n",
        "# Przetwarzanie i wyświetlanie wyników dla każdego kernela\n",
        "for i, kernel in enumerate(kernels):\n",
        "    # Zastosowanie konwolucji 2D\n",
        "    # padding=0 oznacza, że obraz wyjściowy będzie nieco mniejszy\n",
        "    output = F.conv2d(input_image, kernel, padding=0)\n",
        "\n",
        "    # Pobranie wyniku (tensor [H-2, W-2]) i odłączenie go od grafu obliczeń\n",
        "    output_tensor = output[0, 0].detach()\n",
        "\n",
        "    # Wyświetlenie przetworzonego obrazu\n",
        "    plt.subplot(1, 4, i + 2) # Miejsca 2, 3, 4\n",
        "    plt.imshow(output_tensor, cmap='gray')\n",
        "    plt.title(titles[i + 1])\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Sprawdzenie rozmiarów\n",
        "print(f\"Oryginalny rozmiar obrazu: {pixels.shape}\")\n",
        "print(f\"Rozmiar obrazu po konwolucji 3x3 (padding=0): {output_tensor.shape}\")"
      ],
      "metadata": {
        "id": "MS9_iiDIX4P3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trPnC34XUdI-"
      },
      "source": [
        "## Liczba kanałów wyjściowych\n",
        "\n",
        "Parametr `out_channels` określa ile różnych konwolucji jest obliczanych -- każda\n",
        "z odrębnymi parametrami (maskami) generuje jeden wektor (macierz) wyjściową.\n",
        "W ten sposób jeden obraz wejściowy może być przekształcony na wiele sposobów,\n",
        "**skupiając** się na różnych cechach obrazu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE9Xk8qX2zPi"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1234567)\n",
        "\n",
        "conv = nn.Conv2d(in_channels=1,  # 1 bo obraz w skali szarości\n",
        "                 out_channels=2, # 2 kanały wyjściowe\n",
        "                 kernel_size=3,  # rozmiar \"maski\"\n",
        "                 bias=False)     # bez wyrazu wolnego\n",
        "\n",
        "# Jak widać, mamy dwie odrębne maski (filtry)\n",
        "list(conv.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ra3UOaRYTeMQ"
      },
      "outputs": [],
      "source": [
        "input = pixels.unsqueeze(0)\n",
        "out = conv(input)\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU_lPQRLVbWe"
      },
      "source": [
        "Każdy kanał wyjścia to nowy obraz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k01y-YMuTi21"
      },
      "outputs": [],
      "source": [
        "nn_output_to_image(out[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNKN5AF4UYDt"
      },
      "outputs": [],
      "source": [
        "nn_output_to_image(out[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoylylePPDlL"
      },
      "source": [
        "### Obraz kolorowy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whu6JQrKPDlL"
      },
      "outputs": [],
      "source": [
        "conv = nn.Conv2d(in_channels=3,  # 3 bo RGB\n",
        "                 out_channels=1, # 1 kanał wyjściowy\n",
        "                 kernel_size=3,  # rozmiar \"maski\"\n",
        "                 bias=False)     # bez wyrazu wolnego\n",
        "\n",
        "list(conv.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMfrj6roPDlL"
      },
      "outputs": [],
      "source": [
        "im_rgb = Image.open('Lenna.png').resize(size=(256, 256))\n",
        "\n",
        "# display(im_rgb)\n",
        "\n",
        "pixels = tensor( np.asarray(im_rgb) / 255, dtype=torch.float32 )\n",
        "print(f'{pixels.shape = }\\nR: {pixels[10, 20, 0] = }\\nG: {pixels[10, 20, 1] = }')\n",
        "\n",
        "# Kolejność wymiarów jest nieodpowiednia, obecnie\n",
        "# dla każdego koordynatu mamy [x, y, channel] = value\n",
        "#\n",
        "# Potrzebujemy [channel, x, y] = value\n",
        "\n",
        "pixels = pixels.moveaxis(2, 0)\n",
        "print(f'{pixels.shape = }\\nR: {pixels[0, 10, 20] = }\\nG: {pixels[1, 10, 20] = }')\n",
        "\n",
        "out = conv(pixels)  # Obliczamy konwolucję\n",
        "print(f'{out.shape = }')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7YdidSzPDlL"
      },
      "outputs": [],
      "source": [
        "nn_output_to_image(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPaZGhag1Pb-"
      },
      "source": [
        "## Próbkowanie obrazu -- MaxPool2d\n",
        "\n",
        "Oprócz konwolucji, bardzo przydatną operacją jest **próbkowanie w dół**. W przypadku `MaxPool2d` polega ono na wzięciu maksymalnej wartości w oknie wejściowym przesuwanym po obrazie wzdłuż każdego wymiaru. Wielkość okna określa parametr `kernel_size`. Przykładowo, dla `kernel_size=2` otrzymamy na wyjściu obraz dwa razy mniejszy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAGST6C7z6rj"
      },
      "outputs": [],
      "source": [
        "maxpool = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "list(maxpool.parameters())  # MaxPool2d nie ma żadnych parametrów"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9DprhP90O8F"
      },
      "outputs": [],
      "source": [
        "input = tensor([\n",
        "    [1, 0, 2, 0],\n",
        "    [1, 2, 2, 3],\n",
        "    [1, 0, 2, 3],\n",
        "    [1, 4, 2, 5]\n",
        "], dtype=torch.float32)\n",
        "\n",
        "maxpool(input[None, :, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SG4LNoQ7gV-"
      },
      "outputs": [],
      "source": [
        "im = im.convert('L')  # Konwersja na skalę szarości\n",
        "# Dodatkowo normalizujemy wartości ( / 255), tak aby jasność pikseli była z zakresu [0, 1]\n",
        "pixels = tensor( np.asarray(im) / 255, dtype=torch.float32 )\n",
        "\n",
        "maxpool = nn.MaxPool2d(kernel_size=4)\n",
        "out = maxpool(pixels.unsqueeze(0).unsqueeze(0))\n",
        "nn_output_to_image(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8yHdhroOBX5"
      },
      "source": [
        "# Przykład -- klasyfikacja MNIST za pomocą sieci konwolucyjnych"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOFs_dprVtYd"
      },
      "outputs": [],
      "source": [
        "# Pobieramy zbiór MNIST\n",
        "# Zbiór treningowy...\n",
        "train_ds = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
        "# ...i testowy\n",
        "test_ds = torchvision.datasets.MNIST(root='./data', train=False, download=True)\n",
        "\n",
        "f'{len(train_ds) = }, {len(test_ds) = }'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sS8w3D-WuhqN"
      },
      "outputs": [],
      "source": [
        "train_ds.data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIVVhC5Jvawg"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "  img, label = train_ds[i]\n",
        "  display(img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PlJ53Z8V4AK"
      },
      "source": [
        "## Przygotowanie danych\n",
        "\n",
        "**Uwaga**, pojedynczy obraz na wejściu sieci będzie miał wymiary\n",
        "(nr kanału, wysokość, szerokość). W przypadku obrazów w skali szarości\n",
        "liczba-kanałów jest, oczywiście, równa 1, ale dla obrazów RGB będzie 3, a dla\n",
        "RGBA 4.\n",
        "\n",
        "Sieci podajemy albo pojedynczy przykład (obraz) albo zbiór przykładów.\n",
        "W drugim przypadku, wejście będzie 4-wymiarową tablicą (tensorem).\n",
        "\n",
        "Zbiór danych ma oryginalnie wymiary (nr obrazu, wys, szer),\n",
        "więc przekształcamy go na (nr obrazu, nr kanału, wys, szer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VktZBN-nUpNQ"
      },
      "outputs": [],
      "source": [
        "# Dodatkowy wymiar dla kanału wstawiamy za pomocą\n",
        "#      |\n",
        "#      V\n",
        "# [:, np.newaxis, :, :]\n",
        "\n",
        "X_train = train_ds.data[:, np.newaxis, :, :] / 255\n",
        "y_train = train_ds.targets.data\n",
        "\n",
        "X_test = test_ds.data[:, np.newaxis, :, :] / 255\n",
        "y_test = test_ds.targets.data\n",
        "\n",
        "# int [][][][] tab = new int [Liczba obrazow][Liczba kanalow][Wysokosc][Szerokosc]\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtzGU--4lBso"
      },
      "outputs": [],
      "source": [
        "X_train, _ , y_train, _ = train_test_split(X_train, y_train, train_size=10000, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhMuFVn_OLKQ"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(list(zip(X_train, y_train)), shuffle=True, batch_size=64)\n",
        "test_loader = DataLoader(list(zip(X_test, y_test)), shuffle=False, batch_size=256)\n",
        "\n",
        "# DataLoader pozwala nam iterować po kolejnych partiach w postaci par (wej, wyj).\n",
        "# Jak widzimy, każda porcja danych składa się ze 64 obrazów wejściowych\n",
        "# oraz odpowiadających im odpowiedzi.\n",
        "for inp, ans in train_loader:\n",
        "  print(inp.shape, ans.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBTXpt3ur2z-"
      },
      "source": [
        "Nasza sieć zawiera teraz warstwy konwolucyjne oraz warstwy w pełni połączone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyvn2AjbVwI4"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)  # Reset ziarna generatora, dla powtarzalności wyników\n",
        "\n",
        "model = nn.Sequential(\n",
        "  # wej: (1, 28, 28) => wyj: (4, 28, 28):\n",
        "  # z 1 obrazu 28x28 generujemy 4 obrazy 28x28\n",
        "  nn.Conv2d(in_channels=1, out_channels=4,\n",
        "            kernel_size=5,  padding=2),\n",
        "  nn.ReLU(),\n",
        "\n",
        "  # wej: (4, 28, 28) => wyj: (4, 14, 14):\n",
        "  nn.MaxPool2d(kernel_size=2),  # Zmniejszamy rozmiar każdego obrazu (kanału) o połowę\n",
        "\n",
        "  # Podwajamy liczbę kanałów:\n",
        "  # wej: (4, 14, 14) => wyj: (8, 14, 14):\n",
        "  nn.Conv2d(in_channels=4, out_channels=8,\n",
        "            kernel_size=3,  padding=1),\n",
        "  nn.ReLU(),\n",
        "\n",
        "  # wej: (8, 14, 14) => wyj: (8, 7, 7):\n",
        "  nn.MaxPool2d(kernel_size=2),\n",
        "\n",
        "  # wej. (8, 7, 7) => wyj (8*7*7, )  <- wektor 1d\n",
        "  nn.Flatten(),  # Łączymy wszystko w jeden wektor [8 x 7 x 7]\n",
        "\n",
        "  nn.Linear(8 * 7 * 7, 20),  # 20 neuronów w warstwie ukrytej\n",
        "  nn.ReLU(),\n",
        "\n",
        "  nn.Linear(20, 10),  # 10 neuronów w warstwie wyjściowej\n",
        ")\n",
        "\n",
        "summary(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ruBAcr-qYgD"
      },
      "source": [
        "## Trening sieci\n",
        "\n",
        "W każdej epoce trenujemy sieć na całym zbiorze treningowym, jednak kolejność\n",
        "w której pokazywane są obrazy jest za każdym razem losowa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8aBI0vh5ypN"
      },
      "source": [
        "## GPU or not GPU -- this is (not) a question\n",
        "\n",
        "Obliczenia będą *domyślnie* wykonywane na procesorze (CPU), ale jeżeli mamy\n",
        "dostępny procesor graficzny (GPU) wraz z zainstalowanymi bibliotekami, to możemy\n",
        "zazwyczaj znacznie **przyspieszyć** obliczenia.\n",
        "\n",
        "W notatnikach `Colab` można wybrać w menu `Środowisko wykonawcze / Zmień typ środowiska wykonawczego` **akcelerator sprzętowy**.\n",
        "\n",
        "Narzędzie `nvidia-smi` pozwala nam pobrać informacje o dostępnym GPU firmy Nvidia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKC6xhtVcz4U"
      },
      "outputs": [],
      "source": [
        "# Obliczenia wykonamy na GPU, jeżeli jest dostępne, a na CPU w przeciwny razie\n",
        "def get_device():\n",
        "  return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "get_device()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5FFjjAk6Cql"
      },
      "source": [
        "**GPU** oraz **CPU** mają zazwyczaj odrębną przestrzeń adresową (i fizycznie różne banki pamięci), dlatego konieczne jest **kopiowanie** danych z pamięci głównej do pamięci GPU i z powrotem.\n",
        "\n",
        "Dzieje się to za pomocą metod:\n",
        "* `.to(device)` gdzie `device` można uzyskać za pomocą `torch.device()` z parametrem `cuda` lub `cpu`\n",
        "* `.cpu()` -- kopiowanie do pamięci głównej z pamięci GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DK7AeumJ65sA"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, test_loader,\n",
        "                optimizer,\n",
        "                n_epochs=20, eval_every=1,\n",
        "                device=None,\n",
        "                history=None):\n",
        "    device = device or get_device()\n",
        "\n",
        "    history = history or defaultdict(list)\n",
        "    # Przenieś model na określone urządzenie\n",
        "    model.to(device)\n",
        "\n",
        "    # Definiowanie funkcji straty\n",
        "    compute_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Pętla treningowa\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()  # Ustaw model w tryb treningowy\n",
        "\n",
        "        total_loss = 0\n",
        "        total_samples = 0\n",
        "        total_correct = 0\n",
        "\n",
        "        # Iteracja po danych treningowych\n",
        "        for x_batch, y_batch in tqdm(train_loader):\n",
        "            # Przenieś dane na określone urządzenie\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Wyzerowanie gradientów przed kolejną iteracją\n",
        "            out = model(x_batch)  # Faza w przód\n",
        "\n",
        "            loss = compute_loss(out, y_batch)\n",
        "\n",
        "            loss.backward()   # Faza wsteczna do obliczenia gradientów\n",
        "            optimizer.step()  # Aktualizacja parametrów modelu\n",
        "\n",
        "            total_samples += x_batch.shape[0]\n",
        "            total_loss += loss.item() * x_batch.shape[0]\n",
        "\n",
        "            predicted = torch.argmax(out, -1)\n",
        "            total_correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "        history['train_loss'].append(total_loss / total_samples)\n",
        "        history['train_accuracy'].append(total_correct / total_samples)\n",
        "\n",
        "        if epoch % eval_every == 0:  # Ewaluacja na zbiorze testowym\n",
        "            model.eval()   # Ustaw model w tryb ewaluacji\n",
        "            total_loss = 0\n",
        "            total_samples = 0\n",
        "            total_correct = 0\n",
        "            with torch.no_grad():  # Wyłączenie obliczania gradientów\n",
        "                for x_batch, y_batch in test_loader:\n",
        "                    # Przenieś dane na określone urządzenie\n",
        "                    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "                    out = model(x_batch)  # Faza w przód\n",
        "\n",
        "                    loss = compute_loss(out, y_batch)\n",
        "\n",
        "                    total_samples += x_batch.shape[0]\n",
        "                    total_loss += loss.item() * x_batch.shape[0]\n",
        "\n",
        "                    predicted = torch.argmax(out, -1)\n",
        "                    total_correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "            history['test_loss'].append(total_loss / total_samples)\n",
        "            history['test_accuracy'].append(total_correct / total_samples)\n",
        "            print(f'Epoch: {epoch}\\tTrain loss: {history[\"train_loss\"][-1]:.3f}'\\\n",
        "                  f'\\tAcc: {history[\"train_accuracy\"][-1]:.3f}'\\\n",
        "                  f'\\tTest loss: {history[\"test_loss\"][-1]:.3f}'\\\n",
        "                  f'\\tTest acc: {history[\"test_accuracy\"][-1]:.3f}')\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "def plot_train_history(history):\n",
        "    # Wizualizacja historii treningu:\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))  # 1 wiersz, 2 kolumny\n",
        "\n",
        "    n_epochs = len(history['train_loss'])\n",
        "    eval_every = n_epochs // len(history['test_loss'])\n",
        "    xs = range(0, n_epochs, eval_every)\n",
        "    # Strata\n",
        "    ax1.set_ylabel('Strata')\n",
        "    ax1.set_xlabel('Epoka')\n",
        "    ax1.plot(history['train_loss'], color='green')\n",
        "    ax1.plot(xs, history['test_loss'], color='orange')\n",
        "    ax1.legend(['train', 'test'])\n",
        "\n",
        "    ax2.set_ylabel('Dokładność')\n",
        "    ax2.set_xlabel('Epoka')\n",
        "    ax2.plot(history['train_accuracy'], color='green')\n",
        "    ax2.plot(xs, history['test_accuracy'], color='orange')\n",
        "    ax2.legend(['train', 'test'])\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m665Kl_b-L3J"
      },
      "outputs": [],
      "source": [
        "%%timeit -n1 -r1\n",
        "\n",
        "torch.manual_seed(42)  # Reset ziarna generatora, dla powtarzalności wyników\n",
        "history = train_model(model, train_loader, test_loader,\n",
        "            optimizer=torch.optim.SGD(model.parameters(), lr=0.1),\n",
        "            n_epochs=10, device=get_device())\n",
        "\n",
        "plot_train_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2i64JkLFAlYM"
      },
      "outputs": [],
      "source": [
        "def show_classification_metrics(model, data_loader, device=None, class_names=None):\n",
        "    if device is None:\n",
        "        device = get_device()\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            # Move data to the specified device\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            # Prediction\n",
        "            out = model(x_batch)\n",
        "            preds = torch.argmax(out, -1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_preds = np.array(all_preds)\n",
        "\n",
        "    # Determine the unique labels\n",
        "    if class_names is not None:\n",
        "        labels = np.arange(len(class_names))\n",
        "    else:\n",
        "        labels = np.unique(np.concatenate((all_labels, all_preds)))\n",
        "\n",
        "    # Compute the confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds, labels=labels)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    num_errors = len(all_labels) - np.sum(all_preds == all_labels)\n",
        "\n",
        "    # Display the confusion matrix\n",
        "    if class_names is not None:\n",
        "        if len(labels) != len(class_names):\n",
        "            raise ValueError(\"Number of class names must match number of labels.\")\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "    else:\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    # Rotate the x-axis labels by 90 degrees\n",
        "    plt.setp(disp.ax_.get_xticklabels(), rotation=90)\n",
        "    plt.show()\n",
        "\n",
        "    # Display additional information\n",
        "    print(f'Number of errors: {num_errors}')\n",
        "    print(f'Accuracy: {accuracy:.3f}')\n",
        "    print(f'Precision: {precision:.3f}')\n",
        "    print(f'Recall: {recall:.3f}')\n",
        "    print(f'F1-score: {f1:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8CngvtCA3cp"
      },
      "outputs": [],
      "source": [
        "show_classification_metrics(model, test_loader);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgc8_WqKGVUd"
      },
      "outputs": [],
      "source": [
        "def show_errors( model, data_loader, classes = None, device = None, max_errors_per_class = 10):\n",
        "    \"\"\" Display misclassified examples from a data loader. \"\"\"\n",
        "\n",
        "    device = device or get_device()\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    errors_per_class = defaultdict(list)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(x_batch)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            # Identify misclassified indices\n",
        "            misclassified = preds != y_batch\n",
        "            misclassified_indices = misclassified.nonzero(as_tuple=False).squeeze()\n",
        "\n",
        "            # Handle case when there's only one misclassification in the batch\n",
        "            if misclassified_indices.ndim == 0:\n",
        "                misclassified_indices = misclassified_indices.unsqueeze(0)\n",
        "\n",
        "            for idx in misclassified_indices:\n",
        "                true_label = y_batch[idx].item()\n",
        "                pred_label = preds[idx].item()\n",
        "\n",
        "                # Append error if under the max limit for the true class\n",
        "                if len(errors_per_class[true_label]) < max_errors_per_class:\n",
        "                    errors_per_class[true_label].append((x_batch[idx].cpu(), pred_label, true_label))\n",
        "\n",
        "    # Plotting the errors\n",
        "    for true_class, errors in errors_per_class.items():\n",
        "        if not errors:\n",
        "            continue  # Skip if there are no errors for this class\n",
        "\n",
        "        num_errors = len(errors)\n",
        "        fig, axs = plt.subplots(1, num_errors, figsize=(1 * num_errors, 1.6))\n",
        "        fig.suptitle(f'True Class: {classes[true_class] if classes else f\"Class {true_class}\"}', fontsize=10)\n",
        "\n",
        "        # Ensure axs is iterable\n",
        "        if num_errors == 1:\n",
        "            axs = [axs]\n",
        "\n",
        "        for ax, (image, pred_label, _) in zip(axs, errors):\n",
        "            # Handle grayscale and RGB images\n",
        "            if image.shape[0] == 1:\n",
        "                ax.imshow(image.squeeze(0), cmap='gray')\n",
        "            else:\n",
        "                ax.imshow(image.permute(1, 2, 0))\n",
        "\n",
        "            pred_class_name = classes[pred_label] if classes else f'{pred_label}'\n",
        "            ax.set_title(f'Pred: {pred_class_name}', fontsize=8)\n",
        "            ax.axis('off')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "show_errors(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ9Yv-UXS0Yd"
      },
      "source": [
        "## Podgląd warstw konwolucyjnych"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6AFDkyJUD-c"
      },
      "outputs": [],
      "source": [
        "img_idx = 1\n",
        "img, _ = test_ds[img_idx]\n",
        "img = img.resize(size=(28*2, 28*2))\n",
        "\n",
        "display('Obraz wejściowy:')\n",
        "display(img)\n",
        "\n",
        "out = X_test[img_idx]\n",
        "\n",
        "# Iterujemy po kolejnych \"modułach\" i podglądamy jak wygląda wynik na ich wyjściu\n",
        "#\n",
        "# Sequential\n",
        "# ├─Conv2d: 1-1          0.\n",
        "# ├─ReLU: 1-2            1.\n",
        "# ├─MaxPool2d: 1-3       2.\n",
        "# ├─Conv2d: 1-4          3.\n",
        "# ├─ReLU: 1-5            4.\n",
        "# ├─MaxPool2d: 1-6       5. <- kończymy na ostatniej warstwie konwolucyjnej\n",
        "# ├─Flatten: 1-7         <- spłaszczanie\n",
        "# ├─Linear: 1-8          <- ukryta warstwa w pełni połączona\n",
        "# ├─ReLU: 1-9\n",
        "# ├─Linear: 1-10         <- warstwa wyj.\n",
        "\n",
        "for i in range(6):  # przejdź po kolejnych \"warstwach\"\n",
        "  module_idx = str(i)\n",
        "  module = model._modules[module_idx]\n",
        "  out = module(out.to(get_device())).cpu()  # przepuść \"out\" przez moduł\n",
        "  num_channels, w, h = out.shape\n",
        "\n",
        "  fig, axes = plt.subplots(1, num_channels)  # każdy kanał to obraz\n",
        "  fig.set_figwidth(num_channels * 1.2)\n",
        "\n",
        "  for i in range(num_channels):\n",
        "    im = nn_output_to_image(out[i])\n",
        "    axes[i].imshow(im, cmap='gray')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "  print(f'\\nPo przejściu przez: {str(module)}:')\n",
        "  display(fig)\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf4u2D2T4gcO"
      },
      "source": [
        "# Zadanie 1\n",
        "\n",
        "Wytrenuj prostą **sieć konwolucyjną** dla zadania klasyfikacji obrazów ze zbioru [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist).\n",
        "\n",
        "Rozważ dwie architektury:\n",
        "- jak w przykładzie z cyframi\n",
        "- 2x większa liczba kanałów w warstwach konwolucyjnych"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVI7IrH04hmt"
      },
      "outputs": [],
      "source": [
        "# Pobieramy zbiór FashionMNIST\n",
        "# Zbiór treningowy...\n",
        "fashion_train_ds = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True)\n",
        "fashion_test_ds = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCJneVgd5Dan"
      },
      "outputs": [],
      "source": [
        "f'{len(train_ds) = }, {len(test_ds) = }'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_f = fashion_train_ds.data.float()[:, np.newaxis, :, :] / 255.0\n",
        "y_train_f = fashion_train_ds.targets\n",
        "\n",
        "X_test_f = fashion_test_ds.data.float()[:, np.newaxis, :, :] / 255.0\n",
        "y_test_f = fashion_test_ds.targets\n",
        "\n",
        "print(f\"Kształt X_train_f przed podziałem: {X_train_f.shape}\")\n",
        "print(f\"Kształt X_test_f: {X_test_f.shape}\")\n",
        "\n",
        "X_train_f_sample, _, y_train_f_sample, _ = train_test_split(\n",
        "    X_train_f, y_train_f, train_size=10000, random_state=42, stratify=y_train_f\n",
        ")\n",
        "\n",
        "train_loader_f = DataLoader(list(zip(X_train_f_sample, y_train_f_sample)), shuffle=True, batch_size=64)\n",
        "test_loader_f = DataLoader(list(zip(X_test_f, y_test_f)), shuffle=False, batch_size=256)\n",
        "\n",
        "fashion_class_names = [\n",
        "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
        "]\n",
        "\n",
        "# Sprawdzenie jednej partii danych\n",
        "for inp, ans in train_loader_f:\n",
        "  print(f\"Kształt partii wejściowej (batch): {inp.shape}\")\n",
        "  print(f\"Kształt partii wyjściowej (batch): {ans.shape}\")\n",
        "  break"
      ],
      "metadata": {
        "id": "pUFFplB9XaDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 4 i 8 kanałów\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model_1 = nn.Sequential(\n",
        "  # wej: (1, 28, 28) => wyj: (4, 28, 28)\n",
        "  nn.Conv2d(in_channels=1, out_channels=4, kernel_size=5, padding=2),\n",
        "  nn.ReLU(),\n",
        "  # wej: (4, 28, 28) => wyj: (4, 14, 14):\n",
        "  nn.MaxPool2d(kernel_size=2),\n",
        "  # wej: (4, 14, 14) => wyj: (8, 14, 14):\n",
        "  nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, padding=1),\n",
        "  nn.ReLU(),\n",
        "  # wej: (8, 14, 14) => wyj: (8, 7, 7):\n",
        "  nn.MaxPool2d(kernel_size=2),\n",
        "  # wej. (8, 7, 7) => wyj (8*7*7 = 392)\n",
        "  nn.Flatten(),\n",
        "  nn.Linear(8 * 7 * 7, 20),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(20, 10),\n",
        ")\n",
        "\n",
        "print(summary(model_1, input_size=(64, 1, 28, 28), col_names=[\"input_size\", \"output_size\", \"num_params\"], verbose=0))"
      ],
      "metadata": {
        "id": "LiaLKN6K6XAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_1 = train_model(\n",
        "    model_1,\n",
        "    train_loader_f,\n",
        "    test_loader_f,\n",
        "    optimizer=torch.optim.SGD(model_1.parameters(), lr=0.1),\n",
        "    n_epochs=10,\n",
        "    device=get_device()\n",
        ")"
      ],
      "metadata": {
        "id": "-y5x-Mpn6hdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_train_history(history_1)\n",
        "show_classification_metrics(model_1, test_loader_f, device=get_device(), class_names=fashion_class_names)"
      ],
      "metadata": {
        "id": "M7NLH5zz6gRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 2x więcej kanałów (8 i 16 kanałów)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "model_2 = nn.Sequential(\n",
        "  # wej: (1, 28, 28) => wyj: (8, 28, 28)\n",
        "  nn.Conv2d(in_channels=1, out_channels=8, kernel_size=5, padding=2), # 2x\n",
        "  nn.ReLU(),\n",
        "  # wej: (8, 28, 28) => wyj: (8, 14, 14):\n",
        "  nn.MaxPool2d(kernel_size=2),\n",
        "  # wej: (8, 14, 14) => wyj: (16, 14, 14):\n",
        "  nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1), # 2x\n",
        "  nn.ReLU(),\n",
        "  # wej: (16, 14, 14) => wyj: (16, 7, 7):\n",
        "  nn.MaxPool2d(kernel_size=2),\n",
        "  # wej. (16, 7, 7) => wyj (16*7*7 = 784)\n",
        "  nn.Flatten(),\n",
        "  nn.Linear(16 * 7 * 7, 20), # Dopasowany rozmiar wejściowy\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(20, 10),\n",
        ")\n",
        "\n",
        "print(summary(model_2, input_size=(64, 1, 28, 28), col_names=[\"input_size\", \"output_size\", \"num_params\"], verbose=0))"
      ],
      "metadata": {
        "id": "Wpo7SAyJ6sYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_2 = train_model(\n",
        "    model_2,\n",
        "    train_loader_f,\n",
        "    test_loader_f,\n",
        "    optimizer=torch.optim.SGD(model_2.parameters(), lr=0.1),\n",
        "    n_epochs=10,\n",
        "    device=get_device()\n",
        ")"
      ],
      "metadata": {
        "id": "j6NR7-Tn61os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_train_history(history_2)\n",
        "show_classification_metrics(model_2, test_loader_f, device=get_device(), class_names=fashion_class_names)"
      ],
      "metadata": {
        "id": "X-EKZ1Jw65uB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mJ9Yv-UXS0Yd"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}