{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janbanot/msc-cs-code/blob/main/DL_2025_Lab1b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zad 1\n",
        "\n",
        "Zadanie skupia się na tematyce regresji liniowej prezentowanej w ramach pierwszego wykładu."
      ],
      "metadata": {
        "id": "Gvsfxpb46nq8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpAhlhJlsOwT"
      },
      "outputs": [],
      "source": [
        "# Import potrzebnych bibliotek\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTKLAyECsVOG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98fb8705-e286-47b4-81bd-715cbe1f54c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.03 0.19 0.34 0.46 0.78 0.81 1.08 1.18 1.39 1.6  1.65 1.9 ]\n",
            "[0.67 0.85 1.05 1.   1.4  1.5  1.3  1.54 1.55 1.68 1.73 1.6 ]\n"
          ]
        }
      ],
      "source": [
        "# Przykładowe dane:\n",
        "\n",
        "x = np.array([0.03, 0.19, 0.34, 0.46, 0.78, 0.81, 1.08, 1.18, 1.39, 1.60, 1.65, 1.90])\n",
        "y = np.array([0.67, 0.85, 1.05, 1.0, 1.40, 1.5, 1.3, 1.54, 1.55, 1.68, 1.73, 1.6 ])\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOp0XN21suKU"
      },
      "outputs": [],
      "source": [
        "def predict(X, w, b):\n",
        "    \"\"\"\n",
        "    Oblicza przewidywane wartości na podstawie wagi (w) i biasu (b).\n",
        "\n",
        "    Args:\n",
        "        X (numpy.ndarray): Dane wejściowe.\n",
        "        w (float): Współczynnik kierunkowy.\n",
        "        b (float): Wyraz wolny.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Predykowane wartości.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gs7BVWFstPP_"
      },
      "outputs": [],
      "source": [
        "def plot(x, y, w, b):\n",
        "    \" Wykres danych (x, y) oraz modelu liniowego \"\n",
        "    fig,ax = plt.subplots()\n",
        "    ax.scatter(x,y)\n",
        "\n",
        "    min_x = np.min(x)\n",
        "    max_x = np.max(x)\n",
        "\n",
        "    plt.xlim([min_x, max_x])\n",
        "    plt.ylim([0, np.max(y)])\n",
        "    ax.set_xlabel('Input, $x$')\n",
        "    ax.set_ylabel('Output, $y$')\n",
        "    # Prosta\n",
        "    x_line = np.arange(min_x, max_x, 0.01)\n",
        "    y_line = predict(x_line, w, b)\n",
        "    plt.plot(x_line, y_line,'b-',lw=2)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH2i0VUdt0cz"
      },
      "outputs": [],
      "source": [
        "# Parametry\n",
        "w = -0.8\n",
        "b = 1.6\n",
        "# Wykres danych i modelu liniowego:\n",
        "plot(x,y,w,b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nooWSZRBvOTz"
      },
      "outputs": [],
      "source": [
        "def compute_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Oblicza sumę kwadratów błędów.\n",
        "\n",
        "    Args:\n",
        "        y_true (numpy.ndarray): Rzeczywiste wartości.\n",
        "        y_pred (numpy.ndarray): Przewidywane wartości.\n",
        "\n",
        "    Returns:\n",
        "        float: Wartość funkcji straty.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2z_UR48avqk2"
      },
      "outputs": [],
      "source": [
        "# TODO: Spróbuj ręcznie dopasować wartości modelu, tak aby uzyskać\n",
        "# jak najniższą stratę\n",
        "w = -0.5\n",
        "b = 0.5\n",
        "plot(x,y,w,b)\n",
        "print(f'Strata: {compute_loss(y, predict(x, w, b))}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sprawdź jak daleko znalezione parametry znajdują się od optimum na poniższym wykresie."
      ],
      "metadata": {
        "id": "wsvtzNSp86Zt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJOD0oo-w4Az"
      },
      "outputs": [],
      "source": [
        "# Siatka (grid) 2d dla par wartości atrybutów modelu\n",
        "b_mesh, w_mesh = np.meshgrid(np.arange(0.0, 2.0, 0.02),\n",
        "                             np.arange(-1.0, 1.0, 0.02))\n",
        "\n",
        "all_losses = np.zeros_like(w_mesh)\n",
        "\n",
        "# Wyznacz stratę dla kombinacji (bias, waga)\n",
        "for row_col, bias in np.ndenumerate(b_mesh):\n",
        "    weight = w_mesh[row_col]\n",
        "    all_losses[row_col] = compute_loss(y, predict(x, weight, bias))\n",
        "\n",
        "# Wykres mapy ciepła dla funkcji straty\n",
        "fig = plt.figure()\n",
        "ax = plt.axes()\n",
        "fig.set_size_inches(7, 7)\n",
        "levels = 256\n",
        "ax.contourf(b_mesh, w_mesh, all_losses ,levels)\n",
        "levels = 40\n",
        "ax.contour(b_mesh, w_mesh, all_losses ,levels, colors=['#80808080'])\n",
        "ax.set_ylim([1,-1])\n",
        "ax.set_xlabel(r'Wyr wolny, $b$')\n",
        "ax.set_ylabel(r'Wsp kierunkowy, $w$')\n",
        "\n",
        "# Parametry modelu jako czerwony pkt. -- powinien być blisko minimum\n",
        "ax.plot(b, w,'ro')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zad 2\n",
        "\n",
        "Uzupełnij poniższy kod, tak aby dokonał treningu naszego modelu liniowego\n",
        "automatycznie za pomocą __metody gradientu prostego__.\n",
        "\n",
        "- Proszę sprawdzić, jak zachowuje się trening dla różnych wartości **współczynnika szybkości uczenia**."
      ],
      "metadata": {
        "id": "YopT_vr87FUf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljqZuKNNfRqg"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, y, w_init, b_init, learning_rate, n_iterations):\n",
        "    \"\"\"\n",
        "    Metoda gradientu prostego\n",
        "\n",
        "    Args:\n",
        "        X (numpy.ndarray): Dane wejściowe.\n",
        "        y (numpy.ndarray): Rzeczywiste wartości.\n",
        "        w_init (float): Początkowa wartość wagi.\n",
        "        b_init (float): Początkowa wartość biasu.\n",
        "        learning_rate (float): Szybkość nauki.\n",
        "        n_iterations (int): Liczba iteracji.\n",
        "\n",
        "    Returns:\n",
        "        w (float): Wytrenowana waga.\n",
        "        b (float): Wytrenowany bias.\n",
        "        history (list): Historia wartości funkcji straty.\n",
        "    \"\"\"\n",
        "    w = w_init\n",
        "    b = b_init\n",
        "    history = []\n",
        "\n",
        "    m = len(X)\n",
        "\n",
        "    for i in range(n_iterations):\n",
        "        y_pred = predict(X, w, b)\n",
        "        loss = compute_loss(y, y_pred)\n",
        "        history.append(loss)\n",
        "\n",
        "        # Obliczanie gradientów i aktualizacja parametrów\n",
        "        # TODO\n",
        "\n",
        "        print(f\"Iteracja {i+1}: Strata = {loss:.3f}, w = {w:.3f}, b = {b:.3f}\")\n",
        "\n",
        "    return w, b, history\n",
        "\n",
        "# Trenowanie modelu\n",
        "# Inicjalizacja parametrów\n",
        "w_initial = -0.5\n",
        "b_initial = 0.5\n",
        "\n",
        "# Hiperparametry:\n",
        "learning_rate = 0.01\n",
        "n_iterations = 20\n",
        "\n",
        "# Trenowanie\n",
        "w_trained, b_trained, loss_history = gradient_descent(x, y, w_initial, b_initial, learning_rate, n_iterations)\n",
        "\n",
        "print(f\"\\nWytrenowane parametry: w = {w_trained:.3f}, b = {b_trained:.3f}\")\n",
        "\n",
        "plot(x, y, w_trained, b_trained)\n",
        "\n",
        "# Wizualizacja historii funkcji straty\n",
        "plt.plot(range(n_iterations), loss_history, color='green')\n",
        "plt.xlabel('Iteracje')\n",
        "plt.ylabel('Funkcja straty')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLXSs6oY8ZXq"
      },
      "source": [
        "# Zad. 3\n",
        "\n",
        " Przeprowadź trening **modelu liniowego** dla poniższych (symulowanych) danych ceny domu w zależności od metrażu, tj. `(surface, price)`.\n",
        "\n",
        "_Zwróć uwagę na wartość współczynnika uczenia._\n",
        "\n",
        "Odpowiedz na pytania:\n",
        "- Dlaczego wart. współczynnika szybkości uczenia różni się istotnie od tej w poprzednim zadaniu?\n",
        "- Czy transformacja danych wejściowych pomoże?\n",
        "- Jak działa `sklearn.preprocessing.StandardScaler`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlLUDBmAjPxK"
      },
      "outputs": [],
      "source": [
        "# Generowanie przykładowych danych\n",
        "np.random.seed(42)  # Dla powtarzalności wyników\n",
        "\n",
        "# Załóżmy, że cena domu zależy liniowo od jego powierzchni\n",
        "# Cena = 50 * powierzchnia + 10 + losowy szum\n",
        "houses = 30\n",
        "surface = 100 + np.random.rand(houses, 1) * houses  # Powierzchnia w metrach kwadratowych (od 100 do 200)\n",
        "noise = np.random.randn(houses, 1)\n",
        "price = 5 * surface + 10 * noise  # Cena w tysiącach złotych\n",
        "\n",
        "# Wizualizacja danych\n",
        "plt.scatter(surface, price, color='blue', label='Dane treningowe')\n",
        "plt.xlabel('Powierzchnia (m²)')\n",
        "plt.ylabel('Cena (tys. zł)')\n",
        "plt.title('Cena domu vs. Powierzchnia')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Twoje rozwiązanie ...\n",
        "# waga = ...\n",
        "# bias = ..."
      ],
      "metadata": {
        "id": "HAK9ZOi279u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK5KOMI9NkEH"
      },
      "source": [
        "# Zad 4\n",
        "\n",
        "Uzupełnij poniższy kod, tak aby poprawnie obliczał gradient funkcji\n",
        "straty dla problemu *klasyfikacji* binarnej na przykładzie zbioru Iris. Zbiór ten zawiera 3 klasy, ale traktujemy klasy versicolor oraz virginica jako jedną.\n",
        "\n",
        "Nasz model liniowy będzie miał 3 parametry `w1`, `w2` i `b` odpowiednio dla 2 z 4 cech dostępnych w zbiorze."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nasze rozwiązanie"
      ],
      "metadata": {
        "id": "YwEzdk1s06pU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Odczyt danych i funkcja do rys. wykresu"
      ],
      "metadata": {
        "id": "58camhvu0y2s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwJo_iYHNw6Q"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D  # Niezbędne do tworzenia wykresów 3D\n",
        "\n",
        "# Załaduj zestaw danych Iris\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Wybierz tylko pierwsze dwie cechy: długość płatka i szerokość płatka\n",
        "X = X[:, 0:2]\n",
        "\n",
        "# Połącz klasy 1 (versicolor) i 2 (virginica) w jedną klasę (1)\n",
        "# Klasa 0 pozostaje bez zmian (setosa)\n",
        "y = np.where(y == 2, 1, y)  # Wszystkie \"2\" zastąp \"1\"\n",
        "X[::10], y[::10]  # Pokaż co 10-ty zestaw danych"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dJsLbMfHcoy"
      },
      "outputs": [],
      "source": [
        "def plot2(X, y, w1, w2, b, predictor):\n",
        "    \"\"\" Rysuje dwa wykresy dla podanego zbioru danych i modelu liniowego.\n",
        "    predictor uruchamia model liniowy na podanych danych.\n",
        "    \"\"\"\n",
        "    target_names = ['Setosa', 'Versicolor/Virginica']\n",
        "    colors = ['red', 'green']\n",
        "\n",
        "    # Utwórz wykres rozrzutu punktów danych\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    for class_idx, target_name in enumerate(target_names):\n",
        "        plt.scatter(\n",
        "            X[y == class_idx, 0],       # Długość działki kielicha\n",
        "            X[y == class_idx, 1],       # Szerokość działki kielicha\n",
        "            color=colors[class_idx],\n",
        "            label=target_name,\n",
        "            edgecolor='k',      # Czarny brzeg dla lepszej widoczności\n",
        "            s=100,              # Rozmiar punktów\n",
        "            alpha=0.7           # Przezroczystość dla nakładających się punktów\n",
        "        )\n",
        "\n",
        "    # Zdefiniuj zakres dla wykresu\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "\n",
        "    # Utwórz siatkę do wykreślenia granicy decyzyjnej\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
        "                         np.linspace(y_min, y_max, 200))\n",
        "\n",
        "    # Przewiduj etykiety klas dla każdego punktu w siatce\n",
        "    Z = predictor(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Wykreśl granicę decyzyjną, gdzie prawdopodobieństwo wynosi 0.5\n",
        "    plt.contour(xx, yy, Z, levels=[0.5], cmap=\"Greys\", vmin=0, vmax=1)\n",
        "\n",
        "    # Ustaw etykiety osi i tytuł wykresu\n",
        "    plt.xlabel(iris.feature_names[0].capitalize(), fontsize=12)\n",
        "    plt.ylabel(iris.feature_names[1].capitalize(), fontsize=12)\n",
        "    plt.title('Wykres Rozrzutu Zestawu Danych Iris z Granicą Decyzyjną', fontsize=15)\n",
        "    plt.legend(title='Gatunki')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Wykres 3D z Powierzchnią Decyzyjną\n",
        "\n",
        "    # Zainicjalizuj nową figurę dla wykresu 3D\n",
        "    fig = plt.figure(figsize=(6, 7))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # Wykreśl punkty danych w przestrzeni 3D\n",
        "    for i, target_name in enumerate(target_names):\n",
        "        ax.scatter(\n",
        "            X[y == i, 0],                # Długość działki kielicha\n",
        "            X[y == i, 1],                # Szerokość działki kielicha\n",
        "            y[y == i],                   # Etykieta klasy jako oś z\n",
        "            color=colors[i],\n",
        "            label=target_name,\n",
        "            edgecolor='k',\n",
        "            s=100,\n",
        "            alpha=0.7\n",
        "        )\n",
        "\n",
        "    # Utwórz siatkę dla powierzchni decyzyjnej\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 50),\n",
        "                         np.linspace(y_min, y_max, 50))\n",
        "\n",
        "    # Oblicz funkcję decyzyjną (kombinacja liniowa) dla każdego punktu w siatce\n",
        "    Z_plane = w1 * xx + w2 * yy + b\n",
        "\n",
        "    # Wykreśl powierzchnię decyzyjną\n",
        "    ax.plot_surface(xx, yy, Z_plane, alpha=0.3, color='blue', rstride=1, cstride=1, linewidth=0, antialiased=True)\n",
        "\n",
        "    ax.set_xlabel(iris.feature_names[0].capitalize(), fontsize=12)\n",
        "    ax.set_ylabel(iris.feature_names[1].capitalize(), fontsize=12)\n",
        "    ax.set_zlabel('Funkcja Decyzyjna', fontsize=12)\n",
        "    ax.set_title('Wykres 3D z Powierzchnią Decyzyjną Modelu Liniowego', fontsize=15)\n",
        "\n",
        "    ax.legend()\n",
        "    ax.view_init(elev=20, azim=30) # Dostosuj kąt widzenia dla lepszej wizualizacji\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Właściwa część"
      ],
      "metadata": {
        "id": "Gz3wdTsiA0W9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThYbR67kOIv0"
      },
      "outputs": [],
      "source": [
        "def predict(X, w1, w2, b):\n",
        "    # Uwaga -- X jest macierzą, tj.\n",
        "    # [ x1_1 x1_2 ]  <- pierwszy przykład\n",
        "    # [ x2_1 x2_2 ]  <- drugi\n",
        "    # ...\n",
        "    # [ xn_1 xn_2 ]  <- ostatni\n",
        "    pass   # TODO uzupełnij:\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def compute_loss(X, y_true, w1, w2, b):\n",
        "    p = sigmoid(predict(X, w1, w2, b))\n",
        "    # Dodanie małej wartości epsilon dla stabilności numerycznej\n",
        "    epsilon = 1e-15\n",
        "    p = np.clip(p, epsilon, 1 - epsilon)\n",
        "    return np.sum(-y_true * np.log(p) - (1 - y_true) * np.log(1 - p))\n",
        "\n",
        "def compute_gradient(X, y_true, w1, w2, b):\n",
        "    \"\"\"\n",
        "    Oblicza gradient funkcji straty względem parametrów w1, w2, b.\n",
        "\n",
        "    Zwraca: (dw1, dw2, db)\n",
        "    \"\"\"\n",
        "    z = predict(X, w1, w2, b)\n",
        "    p = sigmoid(z)\n",
        "\n",
        "    # TODO\n",
        "\n",
        "def gradient_descent(X, y, w1, w2, b, learning_rate, n_iterations):\n",
        "    history = []\n",
        "\n",
        "    for i in range(n_iterations):\n",
        "        loss = compute_loss(X, y, w1, w2, b)\n",
        "        history.append(loss)\n",
        "\n",
        "        dw1, dw2, db = compute_gradient(X, y, w1, w2, b)\n",
        "\n",
        "        # Aktualizacja parametrów\n",
        "        # TODO\n",
        "\n",
        "        if (i % 100) == 0:\n",
        "            print(f\"Iteracja {i+1}: Strata = {loss:.3f}, {w1 =:.3f}, {w2 =:.3f} b = {b:.3f}\")\n",
        "\n",
        "    return w1, w2, b, history\n",
        "\n",
        "\n",
        "w1_initial = -0.1\n",
        "w2_initial = 0.1\n",
        "b_initial = 0.0\n",
        "\n",
        "# Hiperparametry:\n",
        "learning_rate = 0.001\n",
        "n_iterations = 1000\n",
        "# Trenowanie\n",
        "w1, w2, b, loss_history = gradient_descent(X, y, w1_initial, w2_initial,\n",
        "                                           b_initial, learning_rate, n_iterations)\n",
        "\n",
        "print(f\"\\nWytrenowane parametry: {w1 = :.3f}, {w2 = :.3f} b = {b:.3f}\")\n",
        "\n",
        "pred_probs = predict(X, w1, w2, b)\n",
        "pred_labels = (pred_probs > 0.5).astype(int)\n",
        "num_errors = np.sum(y != pred_labels)\n",
        "print(f'Liczba błędów: {num_errors}')\n",
        "\n",
        "# Wizualizacja historii funkcji straty\n",
        "plt.plot(range(n_iterations), loss_history, color='green')\n",
        "plt.xlabel('Iteracje')\n",
        "plt.ylabel('Funkcja straty')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xV_K5wQiKMJg"
      },
      "outputs": [],
      "source": [
        "plot2(X, y, w1, w2, b, lambda x : sigmoid(predict(x, w1, w2, b)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient funkcji straty\n",
        "\n",
        "Używana funkcja straty to **Binary Cross-Entropy Loss** (binarna entropia krzyżowa), która jest powszechnie stosowana w problemach klasyfikacji binarnej, takich jak regresja logistyczna.\n",
        "\n",
        "```python\n",
        "def compute_loss(X, y_true, w1, w2, b):\n",
        "    p = sigmoid(predict(X, w1, w2, b))\n",
        "    return np.sum(-y_true * np.log(p) - (1 - y_true) * np.log(1 - p))\n",
        "```\n",
        "\n",
        "Gdzie:\n",
        "- $ X $ to macierz cech (przykładowo, $ X = [x_1, x_2] $).\n",
        "- $ y_{\\text{true}} $ to wektor prawdziwych etykiet (0 lub 1).\n",
        "- $ w_1, w_2 $ to wagi modelu.\n",
        "- $ b $ to wyraz wolny (bias).\n",
        "- $ p $ to przewidywane prawdopodobieństwa uzyskane poprzez funkcję sigmoidalną.\n",
        "\n",
        "### **Kroki do bliczenia gradientu**\n",
        "\n",
        "Aby zoptymalizować model, musimy obliczyć gradient funkcji straty względem parametrów $ w_1, w_2, b $. Gradient pomoże nam zrozumieć, w którą stronę należy aktualizować parametry, aby minimalizować stratę.\n",
        "\n",
        "#### 1. **Funkcja straty:**\n",
        "  \n",
        "  $$\n",
        "  L = -\\sum_{i=1}^{N} \\left[ y_i \\cdot \\log(p_i) + (1 - y_i) \\cdot \\log(1 - p_i) \\right]\n",
        "  $$\n",
        "  \n",
        "  gdzie:\n",
        "  \n",
        "  $$\n",
        "  p_i = \\sigma(z_i) = \\frac{1}{1 + e^{-z_i}} \\quad \\textrm{(f. sigmoidalna)}\n",
        "  $$\n",
        "\n",
        "  oraz\n",
        "\n",
        "  $$\n",
        "  z_i = w_1 x_1^{(i)} + w_2 x_2^{(i)} + b  \\quad \\textrm{(akt. neuronu dla }x_i)\n",
        "  $$\n",
        "\n",
        "#### 2. **Obliczenie pochodnej f. straty względem $ p $**\n",
        "\n",
        "Zauważmy zależności $L \\rightarrow p \\rightarrow z \\rightarrow \\{ w_1, w_2, b \\} $\n",
        "\n",
        "Najpierw obliczamy pochodną funkcji straty $ L $ względem przewidywanych prawdopodobieństw $ p_i $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial p_i} = -\\left( \\frac{y_i}{p_i} - \\frac{1 - y_i}{1 - p_i} \\right)\n",
        "$$\n",
        "\n",
        "#### 3. **Obliczenie pochodnej $ p $ względem $ z $**\n",
        "\n",
        "Ponieważ $ p_i = \\sigma(z_i) $, pochodna $ p_i $ względem $ z_i $ jest:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial p_i}{\\partial z_i} = \\sigma(z_i) \\cdot (1 - \\sigma(z_i)) = p_i (1 - p_i)\n",
        "$$\n",
        "\n",
        "#### 4. **Zastosowanie reguły łańcuchowej**\n",
        "\n",
        "Teraz możemy obliczyć pochodną funkcji straty względem $ z_i $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial z_i} = \\frac{\\partial L}{\\partial p_i} \\cdot \\frac{\\partial p_i}{\\partial z_i} = -\\left( \\frac{y_i}{p_i} - \\frac{1 - y_i}{1 - p_i} \\right) \\cdot p_i (1 - p_i) = p_i - y_i\n",
        "$$\n",
        "\n",
        "#### 5. **Obliczenie pochodnych cząstkowych względem $ w_1, w_2, b $**\n",
        "\n",
        "Teraz, korzystając z pochodnej funkcji straty względem $ z_i $, możemy obliczyć pochodne cząstkowe względem $ w_1, w_2, b $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_j} = \\sum_{i=1}^{N} \\frac{\\partial L}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial w_j} = \\sum_{i=1}^{N} (p_i - y_i) \\cdot x_{ij}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b} = \\sum_{i=1}^{N} \\frac{\\partial L}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial b} = \\sum_{i=1}^{N} (p_i - y_i) \\cdot 1 = \\sum_{i=1}^{N} (p_i - y_i)\n",
        "$$\n",
        "\n",
        "Gdzie $ j = 1, 2 $, a $ x_{ij} $ to wartość cechy $ j $-tej dla przykładu $ i $-tego.\n",
        "\n",
        "#### 6. **Finalny gradient**\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_1} = \\sum_{i=1}^{N} (p_i - y_i) \\cdot x_{i1}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_2} = \\sum_{i=1}^{N} (p_i - y_i) \\cdot x_{i2}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b} = \\sum_{i=1}^{N} (p_i - y_i)\n",
        "$$"
      ],
      "metadata": {
        "id": "uaVOKksJ1-HK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Porównanie z gotową implementacją\n",
        "\n",
        "Dla porównania użyjemy gotowej implementacji."
      ],
      "metadata": {
        "id": "4f9xRopbweb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Inicjalizuj model regresji logistycznej\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Wytrenuj model na danych\n",
        "model.fit(X, y)\n",
        "\n",
        "# Pobierz parametry modelu\n",
        "coef = model.coef_[0]\n",
        "intercept = model.intercept_[0]\n",
        "print(f\"Współczynniki modelu: {coef}\")\n",
        "print(f\"Wyraz wolny modelu: {intercept}\")\n",
        "\n",
        "plot2(X, y, coef[0], coef[1], intercept, model.predict)"
      ],
      "metadata": {
        "id": "5z8w1RBotTw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wersja PyTorch\n",
        "\n",
        "PyTorch umożliwia automatyczne obliczanie gradientów, ale potrzebujemy zastąpić `ndarray` z numpy typem `tensor` oraz zmodyfikować kod obliczeń, tak aby używał odpowiedników z biblioteki PyTorch, np.\n",
        "`torch.exp` zamiast `np.exp`."
      ],
      "metadata": {
        "id": "r3OPDRG0C6rK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import tensor\n",
        "\n",
        "\n",
        "# Załaduj zestaw danych Iris\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Wybierz tylko pierwsze dwie cechy: długość płatka i szerokość płatka\n",
        "X = tensor(X[:, 0:2])  # Uwaga, torch.tensor zamiast np.array\n",
        "\n",
        "# Połącz klasy 1 (versicolor) i 2 (virginica) w jedną klasę (1)\n",
        "# Klasa 0 pozostaje bez zmian (setosa)\n",
        "y = tensor(np.where(y == 2, 1, y))\n",
        "\n",
        "X.requires_grad = True"
      ],
      "metadata": {
        "id": "m23IrGiu-m6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, w1, w2, b):\n",
        "    return w1 * X[:,0] + w2 * X[:,1] + b\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + torch.exp(-x))\n",
        "\n",
        "\n",
        "def compute_loss(X, y_true, w1, w2, b):\n",
        "    p = sigmoid(predict(X, w1, w2, b))\n",
        "    # Dodanie małej wartości epsilon dla stabilności numerycznej\n",
        "    epsilon = 1e-15\n",
        "    p = torch.clip(p, epsilon, 1 - epsilon)\n",
        "    return torch.sum(-y_true * torch.log(p) - (1 - y_true) * torch.log(1 - p))\n",
        "\n",
        "\n",
        "def gradient_descent(X, y, w1_init, w2_init, b_init, learning_rate, n_iterations):\n",
        "    loss_history = []\n",
        "\n",
        "    w1 = tensor(w1_init, requires_grad=True)\n",
        "    w2 = tensor(w2_init, requires_grad=True)\n",
        "    b = tensor(b_init, requires_grad=True)\n",
        "\n",
        "    for i in range(n_iterations):\n",
        "        loss = compute_loss(X, y, w1, w2, b)\n",
        "\n",
        "        loss_history.append(loss.detach())  # .detach() odłącza wartość od grafu obliczeń\n",
        "\n",
        "        loss.backward()  # Wart. funkcji straty jest ostatnim wierzchołkiem\n",
        "                         # grafu obliczeń, od niego zaczyna się obliczanie\n",
        "                         # gradientu aż do parametrów w1, w2 oraz b\n",
        "\n",
        "        with torch.no_grad(): # Zapobiega odłączeniu w1, w2 oraz b od grafu\n",
        "                              # obliczającego gradienty\n",
        "            # Aktualizacja parametrów\n",
        "            w1 -= learning_rate * w1.grad\n",
        "            w2 -= learning_rate * w2.grad\n",
        "            b -= learning_rate * b.grad\n",
        "\n",
        "        # Wyzerowanie gradientów\n",
        "        w1.grad = None\n",
        "        w2.grad = None\n",
        "        b.grad = None\n",
        "\n",
        "        if (i % 100) == 0:\n",
        "            print(f\"Iteracja {i+1}: Strata = {loss:.3f}, {w1 =:.3f}, {w2 =:.3f} b = {b:.3f}\")\n",
        "\n",
        "    return w1, w2, b, loss_history\n",
        "\n",
        "w1_initial = -0.1\n",
        "w2_initial = 0.1\n",
        "b_initial = 0.0\n",
        "# Hiperparametry:\n",
        "learning_rate = 0.001\n",
        "n_iterations = 1000\n",
        "# Trenowanie\n",
        "w1, w2, b, loss_history = gradient_descent(X, y, w1_initial, w2_initial,\n",
        "                                           b_initial, learning_rate, n_iterations)\n",
        "\n",
        "print(f\"\\nWytrenowane parametry: {w1 = :.3f}, {w2 = :.3f} b = {b:.3f}\")\n",
        "\n",
        "pred_probs = predict(X, w1, w2, b)\n",
        "pred_labels = (pred_probs > 0.5).numpy().astype(int)\n",
        "num_errors = np.sum(y != pred_labels)\n",
        "print(f'Liczba błędów: {num_errors}')\n",
        "\n",
        "# Wizualizacja historii funkcji straty\n",
        "plt.plot(range(n_iterations), loss_history, color='green')\n",
        "plt.xlabel('Iteracje')\n",
        "plt.ylabel('Funkcja straty')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X3GkcggZ-0iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alg. optymalizacji z PyTorch\n",
        "\n",
        "Korzystamy z gotowej implementacji metody gradientu stochastycznego (`SGD`). Nie stosujemy porcjowania danych, dlatego wynik jest taki sam jak w przypadku metody _gradientu prostego_."
      ],
      "metadata": {
        "id": "ykT6lHW_kNnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = torch.tensor(w1_initial, requires_grad=True)\n",
        "w2 = torch.tensor(w2_initial, requires_grad=True)\n",
        "b  = torch.tensor(b_initial, requires_grad=True)\n",
        "\n",
        "# Dla wygody lista param. modelu\n",
        "model_parameters = [w1, w2, b]\n",
        "\n",
        "# Hiperparametry:\n",
        "learning_rate = 0.001\n",
        "n_iterations = 1000\n",
        "\n",
        "optimizer = torch.optim.SGD(model_parameters, lr=0.001, momentum=0)\n",
        "\n",
        "loss_history = []\n",
        "for i in range(n_iterations):\n",
        "    optimizer.zero_grad()  # Zerowanie gradientów przed kolejną iteracją\n",
        "\n",
        "    loss = compute_loss(X, y, w1, w2, b)\n",
        "    loss_history.append(loss.detach())  # .detach() odłącza wartość od grafu obliczeń\n",
        "\n",
        "    loss.backward()  # Policz gradienty\n",
        "    optimizer.step()  # Aktualizuj parametry modelu\n",
        "\n",
        "print(f\"\\nWytrenowane parametry: {w1 = :.3f}, {w2 = :.3f} b = {b:.3f}\")\n",
        "\n",
        "pred_probs = predict(X, w1, w2, b)\n",
        "pred_labels = (pred_probs > 0.5).numpy().astype(int)\n",
        "num_errors = np.sum(y != pred_labels)\n",
        "print(f'Liczba błędów: {num_errors}')\n",
        "\n",
        "# Wizualizacja historii funkcji straty\n",
        "plt.plot(range(n_iterations), loss_history, color='green')\n",
        "plt.xlabel('Iteracje')\n",
        "plt.ylabel('Funkcja straty')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bck6Vp-8--FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zad 5.\n",
        "\n",
        "Na podstawie rozwiązania zadania 4. dokonaj klasyfikacji binarnej zbioru [Yeast3](https://sci2s.ugr.es/keel/dataset.php?cod=154). Oceń jakość klasyfikacji za pomocą standardowych miar typu dokładność, miara F1 oraz macierz pomyłek. Co zauważasz?"
      ],
      "metadata": {
        "id": "wfScX-f828nn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pobranie danych\n",
        "\n",
        "Poniższy kod pobiera zbiór Yeast3 i tworzy na jego podstawie ramkę `df`"
      ],
      "metadata": {
        "id": "HT1CNKr_3uA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "url = \"https://sci2s.ugr.es/keel/dataset/data/imbalanced/yeast3.zip\"\n",
        "extract_path = \"yeast3_data\"\n",
        "\n",
        "response = requests.get(url)  # Download the ZIP file\n",
        "if response.status_code == 200:\n",
        "    with zipfile.ZipFile(BytesIO(response.content), \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"Dataset downloaded and extracted successfully.\")\n",
        "else:\n",
        "    print(\"Failed to download the dataset.\")\n",
        "\n",
        "dat_files = [f for f in os.listdir(extract_path) if f.endswith(\".dat\")]\n",
        "dat_file_path = os.path.join(extract_path, dat_files[0])\n",
        "\n",
        "def read_keel_file(filepath):\n",
        "    with open(filepath, \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Ignore header lines starting with \"@\"\n",
        "    data_lines = [line.strip() for line in lines if not line.startswith(\"@\") and line.strip()]\n",
        "    attributes = [line.split()[1] for line in lines if line.startswith(\"@attribute\")]\n",
        "    df = pd.DataFrame([line.split(\",\") for line in data_lines], columns=attributes)\n",
        "    return df\n",
        "\n",
        "df = read_keel_file(dat_file_path)"
      ],
      "metadata": {
        "id": "XJyb95yb3l09"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "58camhvu0y2s"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}